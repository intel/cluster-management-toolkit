#! /usr/bin/env python3
# Requires: ansible
# Requires: python3 (>= 3.6)
# Requires: python3-natsort
# Requires: python3-openssl
# Recommends: python3-ujson

from collections import OrderedDict
import base64
import csv
import curses
from curses import wrapper
import curses.textpad
from datetime import datetime, timezone
from functools import reduce
from getpass import getuser
import http.client
# ujson is much faster than json,
# but it might not be available
try:
	import ujson as json
except ModuleNotFoundError:
	import json
from operator import itemgetter
import os
from pathlib import Path
import re
import socket
import subprocess
from subprocess import PIPE, STDOUT
import sys
import tempfile
import time
import yaml

try:
	from natsort import natsorted
except ModuleNotFoundError:
	sys.exit("ModuleNotFoundError: you probably need to install python3-natsort")

try:
	import urllib3
except ModuleNotFoundError:
	sys.exit("ModuleNotFoundError: You probably need to install python3-urllib3; did you forget to run ikt-install?")

from logparser import logparser, loglevel, split_msg, lvl_to_4letter_severity, loglevel_to_name, get_loglevel_names, name_to_loglevel, get_parser_list
import curses_helper
import iktlib
from iktlib import deep_get, deep_get_with_fallback, stgroup, versiontuple, timestamp_to_datetime, get_since
from curses_helper import curses_configuration, color_log_severity, get_mousemask, color_ram_usage, color_status_group, color, UIProps, widgetlineattrs, get_theme_ref, read_theme, strarray_extract_string, strarray_wrap_line, themearray_to_strarray, themearray_extract_string, attr_to_curses_merged

from ansible_helper import ansible_clean_results, ansible_configuration, ansible_support, ansible_get_inventory_dict, ansible_get_groups, ansible_get_groups_by_host, ansible_add_hosts, ansible_remove_hosts, ansible_set_vars, ansible_ping, ansible_run_playbook_on_selection, ansible_get_logs, ansible_delete_log
from ansible_helper import ANSIBLE_PLAYBOOK_DIR, ANSIBLE_INVENTORY
import helptexts

from kubernetes_helper import KubernetesHelper

HOMEDIR = str(Path.home())
IKTDIR = os.path.join(HOMEDIR, ".ikt")
DEPLOYMENTDIR = f"{IKTDIR}/deployments"
IKT_CONFIG_FILENAME = "ikt.yaml"
IKT_CONFIG_FILE = os.path.join(IKTDIR, IKT_CONFIG_FILENAME)
IKT_CONFIG_FILE = os.path.join(IKTDIR, f"{IKT_CONFIG_FILENAME}.d")
THEME_DIRNAME = "themes"
THEMEDIR = os.path.join(IKTDIR, THEME_DIRNAME)
DEFAULT_THEME = "default.yaml"

from iktprint import iktprint, init_iktprint

import about

PROGRAMDESCRIPTION = "UI for managing Kubernetes clusters"
PROGRAMAUTHORS = "Written by David Weinehall."

iktconfig = None

# If the user passes an object (and optionally namespace for that object)
# on the command line, they're stored here
# For pods a container can be appended too
initial_name = None
initial_namespace = None
initial_container = None

# Is iku running in read only-mode?
read_only_mode = False

# Namespace
namespace = ""

# defaults
defaultview = ""

kh = None

# Behaves roughly as which(1)
def which(commandname):
	cpath = None

	# Did we get a full path, or just a command name?
	fpath, fname = os.path.split(commandname)

	# If we got a path we just verify whether commandname
	# exists and is executable
	if fpath:
		if os.path.isfile(commandname) and os.access(commandname, os.X_OK):
			cpath = commandname
		return cpath

	# If we just got a command name we check if it's in the path
	for path in os.environ["PATH"].split(os.pathsep):
		tmp = os.path.join(path, commandname)
		if os.path.isfile(tmp) and os.access(tmp, os.X_OK):
			cpath = tmp
			break

	return cpath

def init_kubernetes_client():
	global kh
	kh = KubernetesHelper(about.program_suite_name, about.program_suite_version, None)

override_tail_lines = None
default_tail_lines = 4000

# Pass in the name of a playbook that exists in {ANSIBLE_PLAYBOOK_DIR};
# returns the path to the drop-in playbook with the highest priority
# (or the same playbook in case there is no override)
def get_playbook_path(playbook):
	path = ""

	# Check if there's a local playbook overriding this one
	local_playbooks = deep_get(iktconfig, "Ansible#local_playbooks", [])
	for playbook_path in local_playbooks:
		# Substitute {HOME}/ for {HOMEDIR}
		if playbook_path.startswith("{HOME}/"):
			playbook_path = f"{HOMEDIR}/{playbook_path[len('{HOME}/'):]}"
		# Skip non-existing playbook paths
		if not os.path.isdir(playbook_path):
			continue
		# We can have multiple directories with local playbooks;
		# the first match wins
		if os.path.isfile(f"{playbook_path}/{playbook}") == True:
			path = f"{playbook_path}/{playbook}"
			break
	if len(path) == 0:
		path = f"{ANSIBLE_PLAYBOOK_DIR}/{playbook}"
	return path

def get_package_versions(host_name):
	if ansible_support == False or not os.path.isdir(ANSIBLE_PLAYBOOK_DIR):
		return []

	control_plane_k8s_version = ""

	get_versions_path = get_playbook_path("get_versions.yaml")
	retval, ansible_results = ansible_run_playbook_on_selection(get_versions_path, selection = [host_name])

	if len(ansible_results) == 0:
		raise Exception(f"Error: Failed to get package versions from {host_name} (retval: {retval}); aborting.")

	for host in ansible_results:
		data = ansible_results[host]

		for task in data:
			if "package versions" in task.replace("_", " "):
				tmp = str(data[task].get("msg", "")).split("\n")

	if len(tmp) == 0:
		raise Exception(f"Error: Failed to get package versions from {host_name} (retval: {retval}); aborting.")

	package_versions = []

	for line in tmp:
		tmp = re.match(r"^(.*?): (.*)", line)
		if tmp is None:
			continue
		package = tmp[1]
		version = tmp[2]
		package_versions.append((package, version))

	return package_versions

def gather_cluster_info():
	iktprint([("[Gathering cluster information]\n", "phase")])

	# Set global variables that need to be available when executing playbooks
	join_token = kh.get_join_token()
	ca_cert_hash = kh.get_ca_cert_hash()
	control_plane_ip, control_plane_port = kh.get_control_plane_address()
	control_plane_node, control_plane_name = get_control_plane()

	# This is tricky: we get this from the Debian package;
	# since we cannot assume that we're running iku on the [main] control plane
	# we have to ask the [main] control plane, via ansible, what version of kubeadm it's running
	package_versions = get_package_versions(control_plane_name)
	control_plane_k8s_version = ""
	for package, version in package_versions:
		if package == "kubeadm":
			control_plane_k8s_version = version
	if len(control_plane_k8s_version) == 0:
		sys.exit(f"Failed to get kubeadm version from control plane “{control_plane_name}“ (retval: {retval}); aborting.")

	http_proxy = deep_get(iktconfig, "Network#http_proxy", None)
	https_proxy = deep_get(iktconfig, "Network#https_proxy", None)
	no_proxy = deep_get(iktconfig, "Network#no_proxy", None)
	insecure_registries = deep_get(iktconfig, "Docker#insecure_registries", [])
	registry_mirrors = deep_get(iktconfig, "Containerd#registry_mirrors", [])
	packages = deep_get(iktconfig, "Packages", {})

	values = {
		"control_plane_ip": control_plane_ip,
		"control_plane_port": control_plane_port,
		"join_token": join_token,
		"ca_cert_hash": ca_cert_hash,
		"control_plane_k8s_version": control_plane_k8s_version,
		"ntp_server": control_plane_ip,
		"http_proxy": http_proxy,
		"https_proxy": https_proxy,
		"no_proxy": no_proxy,
		"insecure_registries": insecure_registries,
		"registry_mirrors": registry_mirrors,
		"packages": packages,
	}

	ansible_set_vars(ANSIBLE_INVENTORY, "all", values)

	print("\n\n")

def align_and_pad(array, pad, fieldlen, stringlen, ralign, selected):
	if ralign:
		array = [("".ljust(fieldlen - stringlen), ("types", "generic", selected))] + array
	else:
		array.append(("".ljust(fieldlen - stringlen), ("types", "generic", selected)))
	if pad > 0:
		array.append((("separators", "pad"), selected))
	return array

def generator_basic(obj, field, fieldlen, pad, ralign, selected):
	value = getattr(obj, field)
	string = str(value)

	if string == "None":
		string = "<none>"

	if string in ["<none>", "<unset>", "<unknown>"]:
		formatting = ("types", "none", selected)
	else:
		formatting = ("types", "generic", selected)

	array = [
		(string, formatting)
	]

	return align_and_pad(array, pad, fieldlen, len(string), ralign, selected)

def datetime_to_timestamp(timestamp):
	if timestamp is None:
		string = ""
	elif timestamp == datetime.fromtimestamp(0).astimezone():
		string = "".ljust(len(str(datetime.fromtimestamp(0).astimezone())))
	else:
		string = timestamp.astimezone().strftime("%Y-%m-%d %H:%M:%S")
	return string

def generator_str_timestamp(obj, field, fieldlen, pad, ralign, selected):
	string = processor_str_timestamp(obj, field)

	if len(string) == 0:
		array = [(string, ("types", "generic", selected))]
	else:
		array = format_numerical_with_units(string, "timestamp", selected)

	return align_and_pad(array, pad, fieldlen, len(string), ralign, selected)

def generator_timestamp(obj, field, fieldlen, pad, ralign, selected):
	value = getattr(obj, field)

	string = datetime_to_timestamp(value)
	if value is None:
		array = [(string, ("types", "generic", selected))]
	elif value == datetime.fromtimestamp(0).astimezone():
		array = [(string, ("types", "generic", selected))]
	else:
		array = format_numerical_with_units(string, "timestamp", selected)

	return align_and_pad(array, pad, fieldlen, len(string), ralign, selected)

def generator_yesno(obj, field, fieldlen, pad, ralign, selected):
	value = getattr(obj, field)

	if value is None:
		string = "N/A"
	elif value == True:
		string = "Yes"
	else:
		string = "No"

	formatting = ("types", "generic", selected)

	array = [
		(string, formatting)
	]

	return align_and_pad(array, pad, fieldlen, len(string), ralign, selected)

def generator_priority_level(obj, field, fieldlen, pad, ralign, selected):
	value = getattr(obj, field)

	if value is None:
		string = "<none>"
		formatting = ("types", "none", selected)
	else:
		string = str(value)
		# We need to check whether the priority level exists or not;
		# non-existing priority levels should be highlighted
		obj = kh.get_ref_by_kind_name_namespace(("PriorityLevelConfiguration", "flowcontrol.apiserver.k8s.io"), value, "")
		if obj is None:
			formatting = ("main", "status_not_ok")
		else:
			formatting = ("types", "generic")

	array = [
		(string, formatting, selected)
	]

	return align_and_pad(array, pad, fieldlen, len(string), ralign, selected)

# Will take datetime and format it as (YYYY-MM-DD HH:MM:SS, field_colors)
def format_timestamp(timestamp, localtimezone = False):
	stringarray = []

	if timestamp is None:
		stringarray = themearray_to_strarray("none", "strings")
	else:
		if localtimezone == True:
			ftimestamp = timestamp.astimezone().strftime("%Y-%m-%d %H:%M:%S")
		else:
			ftimestamp = timestamp.strftime("%Y-%m-%d %H:%M:%S")
		stringarray = format_numerical_with_units(ftimestamp, "timestamp", False)

	return stringarray

def format_list(items, fieldlen, pad, ralign, selected, item_separator = ("separators", "list"), field_separators = [("separators", "field")], field_colors = [("types", "generic")], ellipsise = -1, ellipsis = ("separators", "ellipsis"), field_prefixes = None, field_suffixes = None):
	array = []
	totallen = 0

	if type(field_separators) != list:
		raise Exception(f"field_separators should be a list of (context, style) tuple, not a single tuple; {field_separators}")

	if type(field_colors) != list:
		raise Exception(f"field_colors should be a list of (context, style) tuple, not a single tuple; {field_colors}")

	if type(items) is not list:
		items = [items]

	elcount = 0
	skip_separator = True

	for item in items:
		if len(array) > 0:
			_context, _attr = item_separator
			totallen += len(themearray_extract_string(_attr, context = _context))
			array.append((item_separator, selected))
			elcount += 1

		if elcount == ellipsise:
			array.append((ellipsis, selected))
			break

		# Treat all types as tuples no matter if they are; since tuples consist of 2+ elements we add None
		if type(item) is not tuple:
			item = (item, None)

		for i in range(0, len(item)):
			if item[i] is None:
				continue

			string = str(item[i])

			if len(string) == 0:
				continue

			if i > 0 and skip_separator == False:
				field_separator = field_separators[min(i - 1, len(field_separators) - 1)]
				_context, _key = field_separator
				totallen += len(themearray_extract_string(_key, context = _context))
				array.append((field_separator, selected))

			if string == "<none>":
				formatting = ("types", "none", selected)
			elif string == "<not ready>":
				formatting = color_status_group(stgroup.NOT_OK, selected)
			else:
				context, attr_ref = field_colors[min(i, len(field_colors) - 1)]
				formatting = (context, attr_ref, selected)

			# OK, we know now that we'll be appending the field, so do the prefix
			if field_prefixes is not None and i < len(field_prefixes):
				for prefix in field_prefixes[i]:
					_context, _key = prefix
					totallen += len(themearray_extract_string(_key, context = _context))
					array.append((prefix, selected))
			array.append((string, formatting))
			totallen += len(string)
			# And now the suffix
			if field_suffixes is not None and i < len(field_suffixes):
				for suffix in field_suffixes[i]:
					_context, _key = suffix
					totallen += len(themearray_extract_string(_key, context = _context))
					array.append((suffix, selected))
			skip_separator = False

	return align_and_pad(array, pad, fieldlen, totallen, ralign, selected)

def generator_list(obj, field, fieldlen, pad, ralign, selected, item_separator = None, field_separators = None, field_colors = None, ellipsise = -1, ellipsis = None, field_prefixes = None, field_suffixes = None):
	items = getattr(obj, field)

	if item_separator is None:
		item_separator = ("separators", "list")

	if field_separators is None:
		field_separators = [("separators", "field")]

	if field_colors is None:
		field_colors = [("types", "generic")]

	if ellipsis is None:
		ellipsis = ("separators", "ellipsis")

	return format_list(items, fieldlen, pad, ralign, selected, item_separator = item_separator, field_separators = field_separators, field_colors = field_colors, ellipsise = ellipsise, ellipsis = ellipsis, field_prefixes = field_prefixes, field_suffixes = field_suffixes)

def generator_list_with_status(obj, field, fieldlen, pad, ralign, selected, item_separator = None, field_separators = None, field_colors = None, ellipsise = -1, ellipsis = None, field_prefixes = None, field_suffixes = None):
	items = getattr(obj, field)

	if item_separator is None:
		item_separator = ("separators", "list")

	if field_separators is None:
		field_separators = [("separators", "field")]

	if ellipsis is None:
		ellipsis = ("separators", "ellipsis")

	newitems = []

	for item in items:
		field_colors = []
		if item[1] == stgroup.OK:
			field_colors.append(("main", "status_ok"))
		elif item[1] == stgroup.NOT_OK:
			field_colors.append(("main", "status_not_ok"))
		else:
			field_colors.append(("types", "generic"))
		newitems.append(item[0])

	return format_list(newitems, fieldlen, pad, ralign, selected, item_separator = item_separator, field_separators = field_separators, field_colors = field_colors, ellipsise = ellipsise, ellipsis = ellipsis, field_prefixes = field_prefixes, field_suffixes = field_suffixes)

def generator_secret_data(obj, field, fieldlen, pad, ralign, selected):
	value = getattr(obj, field)
	string = str(value)

	if len(string) == 0:
		vtype = "empty"
	else:
		vtype = getattr(obj, "vtype", "string")

	if vtype == "empty":
		string = "[empty]"
	elif vtype == "string":
		string = string.ljust(fieldlen)
	elif vtype == "base64-utf-8":
		string = "[base64]"
	else:
		string = "[base64 (binary)]"

	formatting = ("types", "generic", selected)

	array = [
		(string, formatting)
	]

	return align_and_pad(array, pad, fieldlen, len(string), ralign, selected)

def generator_numerical_range(obj, field, fieldlen, pad, ralign, selected):
	value = getattr(obj, field)
	if type(value) == int:
		value = [value]

	i = 0
	ranges = deep_get(field_templates, f"{field}#ranges")
	formatting = ("types", "generic", selected)
	for start, end, formatting in ranges:
		if type(start) == str and int(start) < len(value):
			start = value[int(start)]
		if type(end) == str and int(end) < len(value):
			end = value[int(end)]

		if value[0] >= start and value[0] < end:
			context, attr = formatting
			formatting = (context, attr, selected)
			break

	if value[0] == -1:
		string = ""
	else:
		string = str(value[0])

	array = [
		(string, formatting)
	]

	return align_and_pad(array, pad, fieldlen, len(string), ralign, selected)

def generator_numerical(obj, field, fieldlen, pad, ralign, selected):
	value = getattr(obj, field)

	if value == -1:
		string = ""
	else:
		string = str(value)

	formatting = ("types", "timestamp", selected)

	array = [
		(string, formatting)
	]

	return align_and_pad(array, pad, fieldlen, len(string), ralign, selected)

def generator_mem_single(obj, field, fieldlen, pad, ralign, selected):
	value = getattr(obj, field)
	string = str(value)

	array = format_numerical_with_units(string, "field", selected)

	return align_and_pad(array, pad, fieldlen, len(string), ralign, selected)

def format_numerical_with_units(string, ftype, selected):
	substring = ""
	array = []
	numeric = None
	# This is necessary to be able to use pop
	string = list(string)

	while len(string) > 0:
		char = string.pop(0)
		if numeric is None:
			numeric = char.isnumeric()
			substring += char
		elif numeric == True:
			# Do we need to flush?
			if not char.isnumeric():
				array.append((substring, ("types", ftype, selected)))
				substring = ""
				numeric = False

			substring += char
		else:
			# Do we need to flush?
			if char.isnumeric():
				array.append((substring, ("types", "unit", selected)))
				substring = ""
				numeric = True
			substring += char

		if len(string) == 0:
			if numeric == True:
				array.append((substring, ("types", ftype, selected)))
			else:
				array.append((substring, ("types", "unit", selected)))

	if len(array) == 0:
		array = [("", ("types", "generic", selected))]

	return array

def generator_age(obj, field, fieldlen, pad, ralign, selected):
	value = getattr(obj, field)

	if value == -1:
		string = ""
	else:
		string = iktlib.seconds_to_age(value)

	if string in ["<none>", "<unset>", "<unknown>"]:
		formatting = ("types", "none", selected)
		array = [(string, formatting, selected)]
	else:
		array = format_numerical_with_units(string, "age", selected)

	return align_and_pad(array, pad, fieldlen, len(string), ralign, selected)

def generator_mem(obj, field, fieldlen, pad, ralign, selected):
	free, total = getattr(obj, field)

	if free is None and total is None:
		return generator_basic(obj, field, fieldlen, pad, ralign, selected)

	used = "{:0.1f}".format(100 - (100 * int(free) / int(total)))
	attribute = color_ram_usage(used, selected)

	total = "{:0.1f}".format(int(total) / (1024 * 1024))
	unit = "GiB"

	stringlen = len(used) + len(themearray_extract_string("percentage", "separators")) + len(" ") + len(themearray_extract_string("fraction", "separators")) + len(" ") + len(total) + len(unit)

	array = [
		(used, attribute),
		(("separators", "percentage"), selected),
		(" ", attribute),
		(("separators", "fraction"), selected),
		(" ", attribute),
		(total, attribute),
		(unit, ("types", "unit", selected)),
	]

	return align_and_pad(array, pad, fieldlen, stringlen, ralign, selected)

# Generator for {available,ready} / total
def generator_total(obj, field, fieldlen, pad, ralign, selected):
	free, total = getattr(obj, field)

	if free is None and total is None:
		return generator_basic(obj, field, fieldlen, pad, ralign, selected)

	# Strip millicores
	if type(free) == str and free.endswith("m"):
		_free = int(free.strip("m")) // 1000
	else:
		_free = int(free)
	if type(total) == str and total.endswith("m"):
		_total = int(total.strip("m")) // 1000
	else:
		_total = int(total)

	stringlen = len(str(_free)) + len(themearray_extract_string("fraction", "separators")) + len(str(_total))

	ranges = deep_get(field_templates, f"{field}#ranges", [])
	formatting = ("types", "numerical", selected)

	# If the values are -1/-1 we substitute with an empty string of the same length
	if free == -1 and total == -1:
		array = [
			("".ljust(stringlen), ("types", "generic", selected))
		]
	else:
		for start, end, formatting in ranges:
			# exact match against total, or value inside range
			if start == -1 and end == -1:
				if _free == _total:
					context, attr = formatting
					formatting = (context, attr, selected)
					break
				else:
					continue
			if (_free >= start and _free < end) or (_free >= start and end == -1):
				context, attr = formatting
				formatting = (context, attr, selected)
				break

		array = [
			(str(_free), formatting),
			(("separators", "fraction"), selected),
			(str(_total), ("types", "numerical", selected)),
		]

	return align_and_pad(array, pad, fieldlen, stringlen, ralign, selected)

def generator_status(obj, field, fieldlen, pad, ralign, selected):
	status = getattr(obj, field)
	status_group = getattr(obj, "status_group")
	attribute = color_status_group(status_group, selected)

	array = [
		(status, attribute)
	]
	stringlen = len(status)

	return align_and_pad(array, pad, fieldlen, stringlen, ralign, selected)

def processor_str_timestamp(obj, field):
	value = getattr(obj, field)
	timestamp = ""

	if value is not None:
		for fmt in ["%Y-%m-%d %H:%M:%S.%f%z", "%Y-%m-%d %H:%M:%S%z", "%Y-%m-%dT%H:%M:%S.%f%z", "%Y-%m-%dT%H:%M:%S%z"]:
			try:
				timestamp = datetime.strptime(value, fmt).astimezone().strftime("%Y-%m-%d %H:%M:%S")
				break
			except ValueError:
				pass

	return timestamp

def processor_timestamp(obj, field):
	value = getattr(obj, field)

	if value is None:
		return ""
	else:
		return value.astimezone().strftime("%Y-%m-%d %H:%M:%S")

def processor_total(obj, field):
	free, total = getattr(obj, field)

	# Strip millicores
	if type(free) == str and free.endswith("m"):
		_free = int(free.strip("m")) // 1000
	else:
		_free = int(free)
	if type(total) == str and total.endswith("m"):
		_total = int(total.strip("m")) // 1000
	else:
		_total = int(total)
	return "%s/%s" % (str(_free), str(_total))

# For the list processor to work we need to know the length of all the separators
def processor_list(obj, field, item_separator = ("separators", "list"), field_separators = [("separators", "field")], ellipsise = -1, ellipsis = ("separators", "ellipsis"), field_prefixes = None, field_suffixes = None):
	items = getattr(obj, field)
	totallen = 0

	_context, _attr_ref = item_separator
	item_separator = themearray_extract_string(_attr_ref, context = _context)
	_context, _attr_ref = ellipsis
	ellipsis = themearray_extract_string(_attr_ref, context = _context)

	strings = []

	elcount = 0
	skip_separator = True

	if type(items) is tuple:
		items = [items]

	for item in items:
		if elcount == ellipsise:
			strings.append(ellipsis)
			break

		if type(item) is not tuple:
			item = (item, None)

		# Join all elements of the field into one string
		string = ""

		for i in range(0, len(item)):
			if item[i] is None:
				continue

			tmp = str(item[i])

			if len(tmp) == 0:
				continue

			if i > 0 and skip_separator == False:
				_context, _attr_ref = field_separators[min(i - 1, len(field_separators) - 1)]
				field_separator = themearray_extract_string(_attr_ref, context = _context)
				string += field_separator

			if field_prefixes is not None and i < len(field_prefixes):
				for _context, _attr_ref in field_prefixes[i]:
					_str = themearray_extract_string(_attr_ref, context = _context)
					string += _str
			string += tmp
			if field_suffixes is not None and i < len(field_suffixes):
				for _context, _attr_ref in field_suffixes[i]:
					_str = themearray_extract_string(_attr_ref, context = _context)
					string += _str
			skip_separator = False

		strings.append(string)
		elcount += 1

	vstring = item_separator.join(strings)

	return vstring

# For the list processor to work we need to know the length of all the separators
def processor_list_with_status(obj, field, item_separator = ("separators", "list"), field_separators = [("separators", "field")], ellipsise = -1, ellipsis = ("separators", "ellipsis"), field_prefixes = None, field_suffixes = None):
	items = getattr(obj, field)
	totallen = 0

	_context, _attr_ref = item_separator
	item_separator = themearray_extract_string(_attr_ref, context = _context)
	_context, _attr_ref = ellipsis
	ellipsis = themearray_extract_string(_attr_ref, context = _context)

	strings = []

	elcount = 0
	skip_separator = True

	newitems = []
	for item in items:
		newitems.append(item[0])

	for item in newitems:
		if elcount == ellipsise:
			strings.append(ellipsis)
			break

		if type(item) is not tuple:
			item = (item, None)

		# Join all elements of the field into one string
		string = ""

		for i in range(0, len(item)):
			if item[i] is None:
				continue

			tmp = str(item[i])

			if len(tmp) == 0:
				continue

			if i > 0 and skip_separator == False:
				_context, _attr_ref = field_separators[min(i - 1, len(field_separators) - 1)]
				field_separator = themearray_extract_string(_attr_ref, context = _context)
				string += field_separator

			if field_prefixes is not None and i < len(field_prefixes):
				for _context, _attr_ref in field_prefixes[i]:
					_str = themearray_extract_string(_attr_ref, context = _context)
					string += _str
			string += tmp
			if field_suffixes is not None and i < len(field_suffixes):
				for _context, _attr_ref in field_suffixes[i]:
					_str = themearray_extract_string(_attr_ref, context = _context)
					string += _str
			skip_separator = False

		strings.append(string)
		elcount += 1

	vstring = item_separator.join(strings)

	return vstring

def processor_age(obj, field):
	seconds = getattr(obj, field)
	return iktlib.seconds_to_age(seconds)

def processor_mem(obj, field):
	free, total = getattr(obj, field)

	string = "{:0.1f}".format(100 - (100 * int(free) / int(total)))
	string += themearray_extract_string("percentage", context = "separators")
	string += " "
	string += themearray_extract_string("fraction", context = "separators")
	string += " "
	string += "{:0.1f}".format(int(total) / (1024 * 1024))
	string += "GiB"

	return string

default_processor = {
	generator_age: processor_age,
	generator_list: processor_list,
	generator_list_with_status: processor_list_with_status,
	generator_mem: processor_mem,
	generator_timestamp: processor_timestamp,
	generator_str_timestamp: processor_str_timestamp,
	generator_total: processor_total,
}

def update_field_widths(field_list, objects):
	linelen = 0
	pos = 0

	for field in field_list:
		field_list[field]["pos"] = pos
		field_list[field]["fieldlen"] = 0

		# These are necessary to calculate width of list items
		item_separator = field_list[field].get("item_separator", ("separators", "list"))
		field_separators = field_list[field].get("field_separators", [("separators", "field")])
		ellipsise = field_list[field].get("ellipsise", -1)
		ellipsis = field_list[field].get("ellipsis", ("separators", "ellipsis"))
		field_prefixes = field_list[field].get("field_prefixes", None)
		field_suffixes = field_list[field].get("field_suffixes", None)

		for obj in objects:
			generator = field_list[field].get("generator")
			processor = field_list[field].get("processor")

			if processor is None:
				processor = default_processor.get(generator)

			if processor is not None:
				if processor in [processor_list, processor_list_with_status]:
					tmp = processor(obj, field, item_separator = item_separator, field_separators = field_separators, ellipsise = ellipsise, ellipsis = ellipsis, field_prefixes = field_prefixes, field_suffixes = field_suffixes)
				else:
					tmp = processor(obj, field)
			else:
				tmp = str(getattr(obj, field))

			tmplen = max(field_list[field].get("fieldlen", 0), len(str(tmp)))

			field_list[field]["fieldlen"] = max(len(str(tmp)), field_list[field]["fieldlen"])

		field_list[field]["fieldlen"] = max(field_list[field]["fieldlen"], len(field_list[field]["header"]))

		linelen += field_list[field].get("fieldlen") + len(themearray_extract_string("pad", context = "separators"))
		pos = linelen

	# The last element shouldn't be padded
	if linelen > 0:
		linelen -= len(themearray_extract_string("pad", context = "separators"))

	return linelen

def get_metrics_list(extra_vars = {filter: None}):
	vlist = []

	metrics = kh.get_metrics()

	# metrics are on the form:
	# aggregator_openapi_v2_regeneration_count{apiservice="*",reason="startup"} 0
	# apiserver_requested_deprecated_apis{group="autoscaling",removed_release="1.25",resource="horizontalpodautoscalers",subresource="",version="v2beta1"} 1
	for line in metrics:
		tmp = re.match(r"^(\S+?){(.*)}\s\d+$", line)
		if tmp is None:
			continue
		metric = tmp[1]
		if deep_get(extra_vars, "filter") is not None:
			if metric not in deep_get(extra_vars, "filter"):
				continue

		fields = ['{}'.format(x) for x in next(csv.reader([tmp[2]], delimiter = ",", quotechar = "\""))]
		if len(fields) == 0:
			continue
		d = {
			"name": metric,
			"fields": {},
		}
		for field in fields:
			key_value = ['{}'.format(x) for x in next(csv.reader([field], delimiter = "=", quotechar = "\""))]
			key, value = key_value
			d["fields"][key] = value.strip("\"")

		vlist.append(d)

	return vlist

class ContextInfo:
	def __init__(self, current, name, cluster, authinfo, namespace):
		self.current = current
		self.name = name
		self.cluster = cluster
		self.authinfo = authinfo
		self.namespace = namespace
	def __repr__(self):
		return repr((self.current, self.name, self.cluster, self.authinfo, self.namespace))

def get_context_info(extra_vars = {}):
	info = []

	vlist = kh.list_contexts()

	# If the number of contexts is 0 we don't have a cluster configured
	if len(vlist) == 0:
		return []

	tag_prefix = themearray_extract_string("tag", context = "separators")
	for context in vlist:
		current, name, cluster, authinfo, namespace = context
		if current == True:
			current = tag_prefix
		else:
			current = "".ljust(len(tag_prefix))
		info.append(ContextInfo(current, name, cluster, authinfo, namespace))

	return info

class AnsibleLogInfo:
	def __init__(self, index, name, log_path, host, start_date, end_date, retval):
		self.index = index
		self.name = name
		self.ref = log_path
		self.host = host
		self.start_time = start_date
		self.completion_time = end_date
		self.retval = retval
	def __repr__(self):
		return repr((self.index, self.name, self.ref, self.host, self.created_at, self.completion_time, self.rettval))

def get_ansible_log_info(obj):
	info = []

	log_path = deep_get(obj, "log_path")
	for item in os.listdir(log_path):
		if item == "metadata.yaml":
			continue

		tmp = re.match(r"^(\d\d)-([a-z0-9][-.a-z0-9]*?[a-z0-9])_(.*)", item)
		ref = f"{log_path}/{item}"
		if tmp is not None:
			index = tmp[1]
			host = tmp[2]
		else:
			sys.exit("couldn't parse %s" % item)

		with open(f"{log_path}/{item}", "r") as f:
			d = yaml.safe_load(f)
			name = d["task"]
			start_date = deep_get(d, "start_date")
			if start_date == "None":
				start_date = None
			end_date = deep_get(d, "end_date")
			if end_date == "None":
				end_date = None
			if start_date is not None:
				start_date = datetime.strptime(start_date, "%Y-%m-%d %H:%M:%S.%f")
			if end_date is not None:
				end_date = datetime.strptime(end_date, "%Y-%m-%d %H:%M:%S.%f")
			retval = d["retval"]

		info.append(AnsibleLogInfo(index, name, ref, host, start_date, end_date, retval))

	return info

class ContainerInfo:
	def __init__(self, name, ref, container_type, image_version, instances, image_id, pods, pod_references):
		self.name = name
		self.ref = ref
		self.container_type = container_type
		self.image_version = image_version
		self.instances = instances
		self.image_id = image_id
		self.pods = pods
		self.pod_references = pod_references
	def __repr__(self):
		return repr((self.name, self.ref, self.container_type, self.image_version, self.instances, self.image_id, self.pods, self.pod_references))

def get_image_tuple(image):
	tmp = re.match(r"^(.*):(.*)", image)
	if tmp is not None:
		image_name = f"{tmp[1]}"
		image_version = f"{tmp[2]}"
	else:
		image_name = f"{image}"
		image_version = "<undefined>"
	return image_name, image_version

def __get_container_info(obj, container_type, spec_path, status_path):
	containers = {}

	for container in deep_get(obj, spec_path, []):
		container_name = deep_get(container, "name")
		container_image = deep_get(container, "image")
		image_version = kh.get_image_version(container_image)

		# To get the image ID we need to cross-reference container_image against status#{status_path}->image"
		image_id = None
		for item in deep_get(obj, status_path, []):
			if deep_get(item, "name", "") == container_name:
				image_id = deep_get(item, "imageID")
		# This (most likely) means that the pod hasn't managed to instantiate a container
		if image_id is None:
			image_id = "<unknown>"
		key = (container_name, container_type, image_version, image_id)

		# If this container is in the dict already, just add a pod reference
		# This is unlikely to ever happen (this would mean that the same pod uses the same image multiple times,
		# which seems unlikely), but better safe than sorry
		if key not in containers:
			containers[key] = {}
			containers[key]["pod_references"] = []
			containers[key]["instances"] = 0

		containers[key]["pod_references"].append(obj)
		containers[key]["instances"] += 1

	return containers

def get_container_info(extra_vars = {}):
	info = []
	containers = {}

	# There's no direct way to get a list of unique containers
	# as defined by the tuple (name, type, version, image_id),
	# so we need to iterate the list of all pods and extract this
	# information.
	vlist = kh.get_list_by_kind_namespace(("Pod", ""), "")

	for obj in vlist:
		pod_reference = obj
		tmp = __get_container_info(obj, "InitContainer", "spec#initContainers", "status#initContainerStatuses")
		for key in tmp:
			instances = tmp[key]["instances"]
			pod_references = tmp[key]["pod_references"]
			if key not in containers:
				containers[key] = {}
				containers[key]["instances"] = 0
				containers[key]["pod_references"] = []

			containers[key]["instances"] += instances
			containers[key]["pod_references"] += pod_references
		tmp = __get_container_info(obj, "Container", "spec#containers", "status#containerStatuses")
		for key in tmp:
			instances = tmp[key]["instances"]
			pod_references = tmp[key]["pod_references"]
			if key not in containers:
				containers[key] = {}
				containers[key]["instances"] = 0
				containers[key]["pod_references"] = []

			containers[key]["instances"] += instances
			containers[key]["pod_references"] += pod_references

	for name, container_type, image_version, image_id in containers:
		pod_references = containers[(name, container_type, image_version, image_id)]["pod_references"]
		pods = []
		for pod in pod_references:
			pods.append((deep_get(pod, "metadata#namespace"), deep_get(pod, "metadata#name")))
		instances = containers[(name, container_type, image_version, image_id)]["instances"]
		# This replaces ref
		obj = {
			"name": name,
			"container_type": container_type,
			"image_version": image_version,
			"image_id": image_id,
			"pod_references": pod_references,
		}
		info.append(ContainerInfo(name, obj, container_type, image_version, instances, image_id, pods, pod_references))
	return info

class LogInfo:
	def __init__(self, name, action, log_path, created_at, log_type):
		self.name = name
		self.action = action
		self.ref = log_path
		self.created_at = created_at
		self.log_type = log_type
	def __repr__(self):
		return repr((self.name, self.ref, self.created_at, self.log_type))

def get_log_info(extra_vars = {}):
	info = []

	# Get the list of available Ansible logs
	ansible_logs = ansible_get_logs()

	# TODO: Here we might possibly want to insert other logs?

	for name, action, ref, created_at in ansible_logs:
		log_type = "Ansible Play"
		info.append(LogInfo(name, action, ref, created_at, log_type))

	return info

class SidecarInfo:
	def __init__(self, traffic_type, port, ref, bind, capture_mode, default_endpoint, hosts):
		self.traffic_type = traffic_type
		self.port = port
		self.ref = ref
		self.bind = bind
		self.capture_mode = capture_mode
		self.default_endpoint = default_endpoint
		self.hosts = hosts
	def __repr__(self):
		return repr((self.traffic_type, self.port, self.ref, self.bind, self.capture_mode, self.default_endpoint, self.hosts))

def get_sidecar_info(obj):
	info = []

	if obj is None or (len(deep_get(obj, "spec#ingress", [])) == 0 and len(deep_get(obj, "spec#egress", [])) == 0):
		# traffic_type, port, ref, bind, capture_mode, default_endpoint, hosts):
		return [SidecarInfo("<none>", ("", "", ""), None, "", "", "", [])]

	for item in deep_get(obj, "spec#ingress", []):
		traffic_type = "Ingress"
		port = get_port(item)
		ref = item
		bind = deep_get(item, "bind", "")
		capture_mode = deep_get(item, "captureMode", "NONE")
		# Required
		default_endpoint = deep_get(item, "defaultEndpoint")
		# N/A
		hosts = []
		info.append(SidecarInfo(traffic_type, port, ref, bind, capture_mode, default_endpoint, hosts))

	for item in deep_get(obj, "spec#engress", []):
		traffic_type = "Engress"
		port = get_port(item)
		ref = item
		bind = deep_get(item, "bind", "")
		capture_mode = deep_get(item, "captureMode", "NONE")
		# N/A
		default_endpoint = ""
		# Required
		hosts = deep_get(item, "hosts")
		info.append(SidecarInfo(traffic_type, port, ref, bind, capture_mode, default_endpoint, hosts))

	return info

def normalise_mem_to_bytes(_mem):
	mem = 0

	if _mem.isnumeric():
		mem = _mem
	elif _mem.endswith("Ki"):
		mem = int(_mem[0:-2]) * 1024
	elif _mem.endswith("Mi"):
		mem = int(_mem[0:-2]) * 1024 ** 2
	elif _mem.endswith("Gi"):
		mem = int(_mem[0:-2]) * 1024 ** 3
	elif _mem.endswith("Ti"):
		mem = int(_mem[0:-2]) * 1024 ** 4
	elif _mem.endswith("Pi"):
		mem = int(_mem[0:-2]) * 1024 ** 5
	elif _mem.endswith("Ei"):
		mem = int(_mem[0:-2]) * 1024 ** 6
	elif _mem.endswith("Zi"):
		mem = int(_mem[0:-2]) * 1024 ** 7
	elif _mem.endswith("Yi"):
		mem = int(_mem[0:-2]) * 1024 ** 8
	else:
		raise Exception(f"FIXME: implement support for memory usage {_mem}")

	return mem

def normalise_mem_bytes_to_str(_mem):
	suffix = ""
	mem = 0

	if _mem < 1024 ** 1:
		mem = _mem
	elif _mem < 1024 ** 2:
		mem = _mem / 1024 ** 1
		suffix = "Ki"
	elif _mem < 1024 ** 3:
		mem = _mem / 1024 ** 2
		suffix = "Mi"
	elif _mem < 1024 ** 4:
		mem = _mem / 1024 ** 3
		suffix = "Gi"
	elif _mem < 1024 ** 5:
		mem = _mem / 1024 ** 4
		suffix = "Ti"
	elif _mem < 1024 ** 6:
		mem = _mem / 1024 ** 5
		suffix = "Pi"
	elif _mem < 1024 ** 7:
		mem = _mem / 1024 ** 6
		suffix = "Ei"
	elif _mem < 1024 ** 8:
		mem = _mem / 1024 ** 7
		suffix = "Zi"
	else:
		mem = _mem / 1024 ** 8
		suffix = "Yi"

	return f"{mem:0.1f}{suffix}B"

def datagetter_metrics(obj, path, default):
	if obj is None or path is None:
		return default

	# fields is a dict
	fields = deep_get(obj, "fields", {})
	result = []

	if type(path[0]) == list:
		return_type = path[1]
		rfields = path[0]
	else:
		return_type = "list"
		rfields = path

	for field in rfields:
		result.append(deep_get(obj, f"fields#{field}", ""))

	if return_type == "str":
		result = str("".join(result))
	elif return_type == "tuple":
		result = tuple(result)
	elif return_type != "list":
		raise Exception(f"datagetter_metrics: Invalid return type {return_type}; this is a programming error")

	return result, {}

def datagetter_deprecated_api(obj, path, default):
	result, extra_vars = datagetter_metrics(obj, path, default)
	kind = kh.guess_kind((result[0], result[1]))
	return (kind[0], kind[1], result[2]), extra_vars

def datagetter_pod_metrics_memory(obj, path, default):
	if obj is None or path is None:
		return default

	mem = 0

	for item in deep_get(obj, "containers", []):
		_mem = deep_get(item, path)
		mem += normalise_mem_to_bytes(_mem)
	return normalise_mem_bytes_to_str(mem), {}

def datagetter_pod_metrics_cpu(obj, path, default):
	if obj is None or path is None:
		return default

	cpu = 0

	for item in deep_get(obj, "containers", []):
		_cpu = deep_get(item, path)
		if _cpu == "0":
			pass
		elif _cpu.endswith("u"):
			cpu += int(_cpu[0:-1]) * 1000000
		elif _cpu.endswith("n"):
			cpu += int(_cpu[0:-1])
		else:
			raise Exception(f"FIXME: implement support for CPU usage {_cpu}")
	return f"{cpu / 1000000:0.1f}", {}

def datagetter_api_service(obj, path, default):
	if obj is None or path is None:
		return default

	available, service, status_message, status_group = get_as_status_and_service(deep_get(obj, "status#conditions"), deep_get(obj, "spec#service"))
	if path == "status_message":
		return status_message, {}
	elif path == "service":
		return service, {}
	elif path == "available":
		return available, {"status_group": status_group}
	else:
		raise Exception(f"Unknown API service data field {path}")

def datagetter_access_modes(obj, path, default):
	if obj is None or path is None:
		return default

	access_modes = []

	for mode in deep_get(obj, path, []):
		access_modes.append(abbreviate_access_mode(mode))

	return access_modes, {}

def datagetter_endpoint_ips(obj, path, default):
	subsets = deep_get(obj, path)
	endpoints = get_endpoint_ips(subsets)
	return endpoints, {}

def datagetter_controller(obj, path, default):
	owr = deep_get(obj, path[0])
	show_kind = path[1]
	controller = get_controller_from_owner_references(owr)
	return format_controller(controller, show_kind), {}

def datagetter_ingress_address(obj, paths, default):
	address = []

	for host in deep_get(obj, "status#loadBalancer#ingress", []):
		address.append((deep_get(host, "hostname", deep_get(host, "ip"))))

	return address, {}

def datagetter_ingress_hosts(obj, path, default):
	hosts, ports = get_ingress_hosts(obj)
	if path == "hosts":
		return hosts, {}
	elif path == "ports":
		return ports, {}
	else:
		raise Exception("Unknown ingress field")

def datagetter_subject(obj, path, default):
	users = []
	groups = []
	serviceaccounts = []

	# Note: We're not using kind for anything else than identifying the subject at the moment,
	# but let's complete it just in case we have use for it in the future.
	for subject in deep_get(obj, "subjects", []):
		sname = deep_get(subject, "name")
		kind = deep_get(subject, "kind")
		if kind == "User":
			kind = ("User", deep_get(subject, "api_group", "rbac.authorization.k8s.io"))
			users.append(sname)
		elif kind == "Group":
			kind = ("Group", deep_get(subject, "api_group", "rbac.authorization.k8s.io"))
			groups.append(sname)
		elif kind == "ServiceAccount":
			kind = ("Group", deep_get(subject, "api_group", ""))
			serviceaccounts.append(sname)
		else:
			raise Exception(f"Unknown subject kind {kind}")

	if path == "users":
		return users, {}
	elif path == "groups":
		return groups, {}
	elif path == "serviceaccounts":
		return serviceaccounts, {}
	else:
		raise Exception("Unknown subject type")

def datagetter_regex_split_to_tuples(obj, paths, default):
	if obj is None or paths is None or len(paths) < 2:
		return default

	# paths is a tuple; the first item is a regex that specifies how to split,
	# the second one is either a path to a list or a list of paths

	list_fields = []

	if type(paths[1]) == str:
		for item in deep_get(obj, paths[1], []):
			tmp = re.match(paths[0], item)
			if tmp is not None:
				# This handles ("").*("") | (""); we need to generalise this somehow
				if len(tmp.groups()) >= 2 and tmp[1] is not None and tmp[2] is not None:
					list_fields.append((tmp[1], tmp[2]))
				elif len(tmp.groups()) >= 3:
					list_fields.append(("", tmp[3]))
			else:
				list_fields.append(("", ""))
	else:
		for path in paths[1]:
			item = deep_get(obj, path, "")
			tmp = re.match(paths[0], str(item))
			if tmp is not None:
				# This handles ("").*("") | (""); we need to generalise this somehow
				if len(tmp.groups()) == 1 and tmp[1] is not None:
					list_fields.append(tmp[1])
				if len(tmp.groups()) >= 2 and tmp[1] is not None and tmp[2] is not None:
					list_fields.append((tmp[1], tmp[2]))
				elif len(tmp.groups()) >= 3:
					list_fields.append(("", tmp[3]))
			else:
				list_fields.append(("", ""))

	return list_fields, {}

def datagetter_list_len(obj, path, default):
	if obj is None or path is None or len(path) == 0:
		return 0, {}

	return len(deep_get(obj, [])), {}

def datagetter_list_fields(obj, path, default):
	if obj is None or path is None or len(path) == 0:
		return default

	list_fields = []

	subpath, fields = path

	for item in deep_get(obj, subpath):
		tmp = []
		for field in fields:
			tmp.append(deep_get(item, f"{field}", ""))
		list_fields.append(tuple(tmp))

	return list_fields, {}

def datagetter_containers(obj, paths, default):
	if obj is None or paths is None or len(paths) == 0:
		return default

	containers = []
	for path, status_path in paths:
		containers += get_containers(deep_get(obj, path, []), deep_get(obj, status_path, []))

	return containers, {}

def datagetter_request(obj, path, default):
	if obj is None or path is None or len(path) == 0:
		return default

	request = []

	for resource in deep_get(obj, "spec#hard", []):
		used = deep_get(obj, f"status#used#{resource}", [])
		hard = deep_get(obj, f"spec#hard#{resource}", [])
		request.append((resource, used, hard))

	return request, {}

def datagetter_age(obj, path, default):
	if obj is None or path is None or len(path) == 0:
		return default

	timestamp = timestamp_to_datetime(deep_get(obj, path))
	return get_since(timestamp), {}

def get_start_completion_duration(obj, start_time_path = "status#startTime", completion_time_path = "status#completionTime"):
	completion_time = timestamp_to_datetime(deep_get(obj, completion_time_path))
	if completion_time is None:
		duration = -1
		completion_time = datetime.fromtimestamp(0).astimezone()
		start_time = datetime.fromtimestamp(0).astimezone()
	else:
		start_time = timestamp_to_datetime(deep_get(obj, start_time_path))
		timediff = completion_time - start_time
		duration = timediff.days * 24 * 60 * 60 + timediff.seconds

	return start_time, completion_time, duration

def datagetter_start_completion_duration(obj, path, default):
	if obj is None or path is None or len(path) == 0:
		return default

	if len(path) == 1:
		start, completion, duration = get_start_completion_duration(obj)
	else:
		start, completion, duration = get_start_completion_duration(obj, path[1], path[2])
	if path[0] == "start":
		return start, {}
	elif path[0] == "completion":
		return completion, {}
	elif path[0] == "duration":
		return duration, {}
	else:
		raise Exception(f"Unknown start/completion/duration type {path[0]}")

def datagetter_count(obj, path, default):
	if obj is None or path is None or len(path) == 0:
		return default

	tmp = deep_get(obj, path)
	if tmp is None:
		tmp = []
	return len(tmp), {}

def datagetter_pv_status(obj, path, default):
	if obj is None:
		return default

	status, status_group = get_pv_status(obj)

	return status, {"status_group": status_group}

def datagetter_pvc_status(obj, path, default):
	if obj is None:
		return default

	status, status_group = get_pvc_status(obj)

	return status, {"status_group": status_group}

def datagetter_node_status(obj, path, default):
	if obj is None:
		return default

	status, status_group, taints, full_taints = get_node_status(obj)

	return status, {"status_group": status_group}

def datagetter_node_taints(obj, path, default):
	if obj is None:
		return default

	status, status_group, taints, full_taints = get_node_status(obj)

	return taints, {}

def datagetter_event_status(obj, path, default):
	if obj is None or path is None or len(path) == 0:
		return default

	status = deep_get(obj, "type")
	status_group = get_event_status_group(status)

	return status, {"status_group": status_group}

def datagetter_external_ips(obj, path, default):
	if obj is None:
		return default

	return get_external_ips(obj), {}

def __get_timestamp_with_fallback(obj, paths):
	return timestamp_to_datetime(deep_get_with_fallback(obj, paths))

def datagetter_timestamp_with_fallback(obj, path, default):
	if obj is None or path is None or len(path) == 0:
		return default

	timestamp = __get_timestamp_with_fallback(obj, path)
	return get_since(timestamp), {}

def datagetter_pathlist_to_tuple(obj, path, default):
	if obj is None or path is None or len(path) == 0:
		return default

	info = []

	# Path here can have multiple components; it's a list,
	# not a single path, although the name says otherwise
	for item in path:
		default = None
		if type(item) == tuple:
			default = item[1]
			item = item[0]
		info.append(deep_get(obj, item, default))

	return tuple(info), {}

def datagetter_node_addresses(obj, path, default):
	if obj is None:
		return default

	# for now we don't do anything with external IPs; we should
	name, internal_ips, external_ips = get_node_addresses(deep_get(obj, "metadata#name"), deep_get(obj, "status#addresses"))

	if path == "name":
		return name, {}
	elif path == "internal":
		return internal_ips, {}
	elif path == "external":
		return external_ips, {}
	else:
		raise Exception(f"Unknown address type {path} requested")

def datagetter_node_roles(obj, path, default):
	if obj is None:
		return default

	return kh.get_node_roles(obj), {}

def datagetter_certsignreq_conditions(obj, path, default):
	if obj is None:
		return default
	return get_certsignreq_conditions(deep_get(obj, "status")), {}

def datagetter_status_latest_condition(obj, path, default):
	status, status_group, status_message = get_status_latest_condition(deep_get(obj, path))

	return status, {"status_group": status_group, "status_message": status_message}

def datagetter_status_deployment(obj, path, default):
	status, status_group, status_message = get_dep_status(deep_get(obj, path))

	return status, {"status_group": status_group, "status_message_deployment": status_message}

def datagetter_status_message_deployment(obj, path, default):
	status, status_group, status_message = get_dep_status(deep_get(obj, path))

	return status_message, {}

def datagetter_ep_ports(obj, path, default):
	if obj is None:
		return default

	subsets = deep_get(obj, path)

	return get_endpoint_ports(subsets), {}

def datagetter_eps_ports(obj, path, default):
	if obj is None:
		return default

	return get_endpointslices_ports(obj), {}

def datagetter_eps_endpoints(obj, path, default):
	if obj is None:
		return default

	return get_endpointslices_endpoints(obj), {}

def datagetter_svc_ports(obj, path, default):
	if obj is None:
		return default

	return get_svc_ports(obj), {}

def datagetter_timestamp(obj, path, default):
	if obj is None or path is None or len(path) == 0:
		return default

	return timestamp_to_datetime(deep_get(obj, path)), {}

def datagetter_job_state(obj, path, default):
	if obj is None:
		return default

	completion_time = deep_get(obj, "status#completionTime")
	state, status_group = get_job_state(obj, (completion_time is not None))

	return state, { "status_group": status_group }

def datagetter_phase(obj, path, default):
	if obj is None:
		return default

	phase = deep_get(obj, path)

	if phase in ["Active", "Available", "Bound", "Running", "Succeeded"]:
		# Note: A Running Pod can be failed if any of its containers have failed
		status_group = stgroup.OK
	elif phase in ["Pending", "Released", "Terminating"]:
		status_group = stgroup.PENDING
	elif phase in ["Error", "Failed"]:
		status_group = stgroup.NOT_OK
	# TODO: add remaining phases, if any
	else:
		status_group = stgroup.UNKNOWN

	return phase, { "status_group": status_group }

def datagetter_pod_selector(obj, path, default):
	if obj is None:
		return default

	pod_selector = []
	for key, value in deep_get(obj, "spec#podSelector#matchLabels", {}).items():
		pod_selector.append((key, value))
	return pod_selector, {}

def datagetter_condition_field(obj, path, default):
	if obj is None:
		return default

	field = ""
	extra_vars = {}

	# path is a list of tuples
	# each tuple has element 0 as the condition list,
	# element 1 the condition type,
	# element 2 the field of the condition to return
	for conditions in path:
		for condition in deep_get(obj, conditions[0], []):
			if deep_get(condition, "type", "") == conditions[1]:
				field = deep_get(condition, conditions[2], "")
				if deep_get(condition, "status", "True") == "True":
					extra_vars = { "status_group": stgroup.OK }
				else:
					extra_vars = { "status_group": stgroup.NOT_OK }
				return field, extra_vars

	return field, extra_vars

def datagetter_condition_ready(obj, path, default):
	if obj is None:
		return default

	ready, message, status_group = get_condition_ready(obj)
	return ready, { "status_message": message, "status_group": status_group }

def datagetter_substitute_quotationmarks(obj, path, default):
	if obj is None or path is None or len(path) == 0:
		return default

	return deep_get(obj, path, "").replace("\\\"", "“").replace("\n", "\\n").rstrip(), {}

def get_default(generator):
	if generator is None:
		default = None, {}
	elif generator in [generator_basic, generator_priority_level, generator_str_timestamp]:
		# This expects indata to be a string
		default = "", {}
	elif generator == generator_status:
		# This expects indata to be a string
		default = "", {"status_group": stgroup.NOT_OK, "status_message": ""}
	elif generator == generator_yesno:
		# Valid indata is either None, True, or False,
		# yielding N/A, Yes, or No respectively.
		# Let's default to N/A
		default = None, {}
	elif generator == generator_timestamp:
		# This expects indata to be a timestamp,
		# but returns an empty string if None is passed in
		default = None, {}
	elif generator in [generator_list, generator_list_with_status]:
		# Expects a list or a tuple; an empty list yields
		# no output, which suits us fine
		default = [], {}
	elif generator == generator_secret_data:
		# Expects an extra entry that specifies the type of data;
		# passing vtype == empty will yield "[empty]" which is good enough
		default = "", {}
	elif generator in [generator_numerical, generator_numerical_range, generator_age]:
		# Yields an empty string from -1
		default = -1, {}
	elif generator in [generator_mem, generator_total]:
		default = (None, None), {}
	elif generator in [generator_mem_single]:
		default = "0", {}
	else:
		raise Exception(f"No default defined for generator {generator}")
	return default

latest_info = None

def generic_infogetter(vlist, fields):
	info = []

	# Generate an empty entry
	if vlist == []:
		return []

	for obj in vlist:
		d = {}
		d["ref"] = obj
		# fields both specify formatting and where to get the data from; here we're only concerned with the data
		for field in fields:
			if type(field) == tuple:
				field = field[0]
			generator = deep_get(field_templates, f"{field}#generator", generator_basic)
			datagetter = deep_get(field_templates, f"{field}#datagetter", None)
			path = deep_get(field_templates, f"{field}#path", "")
			default = deep_get(field_templates, f"{field}#default", get_default(generator))
			if datagetter is None:
				if type(default) is not tuple:
					extradata = {}
				else:
					default, extradata = default
				tmp = deep_get(obj, path, default)
				if type(tmp) == str:
					tmp = tmp.rstrip()
				d[field] = tmp
			else:
				# path can be empty for some datagetters
				d[field], extradata = datagetter(obj, path, default)
				for key, value in extradata.items():
					d[key] = value

		info.append(type("InfoClass", (), d))
	return info

def disksize_to_human(size):
	size_suffixes = [
		" bytes",
		"kiB",
		"MiB",
		"GiB",
		"TiB",
		"PiB",
	]
	for i in range(0, len(size_suffixes)):
		if size < 1024:
			break
		size = size // 1024
	suffix = size_suffixes[i]
	return f"{size}{suffix}"

def get_device_model(obj, device):
	for dev in deep_get(obj, "ansible_devices", {}):
		partitions = deep_get(obj, f"ansible_devices#{dev}#partitions", {})
		model = deep_get(obj, f"ansible_devices#{dev}#model", "")
		for partition in partitions:
			if device in partitions:
				return model
	return ""

# Takes a list and splits it into four lists;
# exacts, strings starting _and_ ending with "*", strings starting with "*",
# and strings ending with "*"
def split_matchlist(matchlist, exacts = [], prefixes = [], suffixes = [], ins = []):
	tmp_exacts = []
	tmp_prefixes = []
	tmp_suffixes = []
	tmp_ins = []
	for item in matchlist:
		if item.startswith("*") and item.endswith("*"):
			tmp_ins.append(item[1:-1])
		elif item.endswith("*"):
			tmp_prefixes.append(item[:-1])
		elif item.startswith("*"):
			tmp_suffixes.append(item[1:])
		else:
			tmp_exacts.append(item)
	if len(tmp_exacts) == 0:
		tmp_exacts = exacts
	if len(tmp_prefixes) == 0:
		tmp_prefixes = prefixes
	if len(tmp_suffixes) == 0:
		tmp_suffixes = suffixes
	if len(tmp_ins) == 0:
		tmp_ins = ins
	return tmp_exacts, tuple(tmp_prefixes), tuple(tmp_suffixes), tmp_ins

def check_matchlists(item, exacts = [], prefixes = (), suffixes = (), ins = []):
	if item in exacts:
		return True
	for _in in ins:
		if _in in item:
			return True
	if len(prefixes) > 0 and item.startswith(prefixes) or len(suffixes) > 0 and item.endswith(suffixes):
		return True
	return False

# Takes an unprocessed matchlist, splits it into individual matchlists, and checks for matches
def check_matchlist(item, matchlist):
	exacts, prefixes, suffixes, ins = split_matchlist(matchlist)
	return check_matchlists(item, exacts = exacts, prefixes = prefixes, suffixes = suffixes, ins = ins)

def listgetter_ansible_volumes(obj):
	info = []

	# Find all mounts
	for item in deep_get(obj, "ansible_mounts", []):
		d = {}
		device = deep_get(item, "device", "")
		if len(device) == 0:
			continue
		fstype = deep_get(item, "fstype", "")
		size_total = deep_get(item, "size_total", 0)
		if size_total == 0:
			continue
		size_available = deep_get(item, "size_available", 0)
		partition_size_total = disksize_to_human(size_total)
		partition_size_used = disksize_to_human(size_total - size_available)
		mountpoint = deep_get(item, "mount", "")
		options = deep_get(item, "options", "").split(",")
		# NFS mounts can be added directly; no need to do lookups
		if fstype == "nfs":
			d = {
				"mountpoint": mountpoint,
				"fstype": fstype,
				"options": options,
				"device": device,
				"model": "",
				"partition_size_used": partition_size_used,
				"partition_size_total": partition_size_total,
			}
			info.append(d)
			continue
		device = os.path.basename(device)

		if check_matchlist(mountpoint, deep_get(iktconfig, "Inventory#mountpoint_skiplist", ["/boot/efi", "/var/lib/origin/*", "/run/*", "*@docker*"])):
			continue
		if check_matchlist(device, deep_get(iktconfig, "Inventory#device_skiplist", ["loop*"])):
			continue

		model = get_device_model(obj, device)
		d = {
			"mountpoint": mountpoint,
			"fstype": fstype,
			"options": options,
			"device": device,
			"partition_size_used": partition_size_used,
			"partition_size_total": partition_size_total,
			"model": model,
		}
		info.append(d)

	return info

def objgetter_ansible_facts(obj):
	get_facts_path = get_playbook_path("get_facts.yaml")
	retval, ansible_results = ansible_run_playbook_on_selection(get_facts_path, [obj])
	if retval != 0:
		return {}

	ar = ansible_clean_results(ansible_results)

	return deep_get(ar, f"{obj}#TASK: Gathering host facts#ansible_facts", {})

def objgetter_ansible_log(obj):
	tmpobj = {}

	with open(f"{obj}/metadata.yaml", "r") as f:
		tmpobj = yaml.safe_load(f)
		tmpobj["log_path"] = obj

	try:
		with open(tmpobj["playbook_path"], "r") as f:
			playbook = yaml.safe_load(f)[0]
			tmpobj["name"] = deep_get(playbook, "vars#metadata#description")
			tmpobj["playbook_types"] = deep_get(playbook, "vars#metadata#playbook_types", ["<any>"])
			tmpobj["category"] = deep_get(playbook, "vars#metadata#category", "Uncategorized")
	except FileNotFoundError:
		tmpobj["name"] = "File not found"
		tmpobj["playbook_types"] = ["Unavailable"]
		tmpobj["category"] = "Unavailable"
		pass

	return tmpobj

def objgetter_ansible_task_log(obj):
	tmpobj = {}

	with open(f"{obj}", "r") as f:
		tmpobj = yaml.safe_load(f)

	return tmpobj

def get_task_log(obj):
	field = []

	stdout_lines = deep_get(obj, "stdout_lines", [])
	stderr_lines = deep_get(obj, "stderr_lines", [])
	msg_lines = deep_get(obj, "msg_lines", [])

	if len(stderr_lines) > 0:
		field.append([("stderr:", curses.A_BOLD | curses.A_UNDERLINE)])
		for line in stderr_lines:
			field.append([(f"{line}", color_status_group(stgroup.NOT_OK, False))])

	if len(stdout_lines) > 0:
		if len(field) > 0:
			field.append([("", curses.A_NORMAL)])
			field.append([("", curses.A_NORMAL)])
		field.append([("stdout:", curses.A_BOLD | curses.A_UNDERLINE)])
		for line in stdout_lines:
			field.append([(f"{line}", curses.A_NORMAL)])

	if len(msg_lines) > 0:
		if len(field) > 0:
			field.append([("", curses.A_NORMAL)])
			field.append([("", curses.A_NORMAL)])
		field.append([("msg:", curses.A_BOLD | curses.A_UNDERLINE)])
		for line in msg_lines:
			field.append([(f"{line}", color_status_group(stgroup.OK, False))])

	return field

def get_themearrays(obj):
	return obj

class NetworkPolicyRuleInfo:
	def __init__(self, policy_type, ports, ipblock, ipblock_exceptions, pod_selector, namespace_selector):
		self.policy_type = policy_type
		self.ports = ports
		self.ipblock = ipblock
		self.ipblock_exceptions = ipblock_exceptions
		self.pod_selector = pod_selector
		self.namespace_selector = namespace_selector
	def __repr__(self):
		return repr((self.policy_type, self.ports, self.ipblock, self.ipblock_exceptions, self.pod_selector, self.namespace_selector))

def get_netpol_rule_info(obj):
	info = []

	if obj is None or (len(deep_get(obj, "spec#egress", [])) == 0 and len(deep_get(obj, "spec#ingress", [])) == 0):
		# policy_type, ports, ipblock, ipblock_exceptions, pod_selector, namespace_selector
		return [NetworkPolicyRuleInfo("<none>", [], "", [], [], [])]

	policy_type = "Egress"

	for rule in deep_get(obj, "spec#egress", []):
		ports = []
		for port in deep_get(rule, "ports", []):
			ports.append((deep_get(port, "port", "*"), deep_get(port, "protocol", "TCP")))
		if len(ports) == 0:
			ports = [("*", "TCP")]
		for item in deep_get(rule, "to"):
			ipblock = deep_get(item, "ipBlock#cidr", "")
			ipblock_exceptions = deep_get(item, "ipBlock#except", [])
			pod_selector = []
			for key, value in deep_get(item, "podSelector#matchLabels", {}).items():
				pod_selector.append((key, value))
			namespace_selector = []
			for key, value in deep_get(item, "namespaceSelector#matchLabels", {}).items():
				namespace_selector.append((key, value))

			info.append(NetworkPolicyRuleInfo(policy_type, ports, ipblock, ipblock_exceptions, pod_selector, namespace_selector))

	policy_type = "Ingress"

	for rule in deep_get(obj, "spec#ingress", []):
		ports = []
		for port in deep_get(rule, "ports", []):
			ports.append((deep_get(port, "port", "*"), deep_get(port, "protocol", "TCP")))
		if len(ports) == 0:
			ports = [("*", "TCP")]
		for item in deep_get(rule, "from"):
			ipblock = deep_get(item, "ipBlock#cidr", "")
			ipblock_exceptions = deep_get(item, "ipBlock#except", [])
			pod_selector = []
			for key, value in deep_get(item, "podSelector#matchLabels", {}).items():
				pod_selector.append((key, value))
			namespace_selector = []
			for key, value in deep_get(item, "namespaceSelector#matchLabels", {}).items():
				namespace_selector.append((key, value))

			info.append(NetworkPolicyRuleInfo(policy_type, ports, ipblock, ipblock_exceptions, pod_selector, namespace_selector))

	return info

class VirtSVCRuleInfo:
	def __init__(self, rule_type, ref, destinations):
		self.rule_type = rule_type
		self.ref = ref
		self.destinations = destinations
	def __repr__(self):
		return repr((self.rule_type, self.ref, self.destinations))

def get_virtsvc_rule_info(obj):
	info = []

	if obj is None:
		# rule_type, ref, destination
		return [VirtSVCRuleInfo("<none>", None, [])]

	# XXX: Currently we have no good value for ref; set it to None
	ref = None

	for rule_path, rule_type in [("spec#http", "HTTP"), ("spec#tls", "TLS"), ("spec#tcp", "TCP")]:
		destinations = []

		for rule in deep_get(obj, rule_path, []):
			for item in deep_get(rule, "route", []):
				host = deep_get(item, "destination#host", "")
				subset = deep_get(item, "destination#subset")
				if subset is None:
					subset = "*"
				port = deep_get(item, "destination#port#number", "")
				destinations.append((host, subset, port))

		if len(destinations) > 0:
			info.append(VirtSVCRuleInfo(rule_type, ref, destinations))

	return info

def get_condition_ready(obj):
	ready = "False"
	status_group = stgroup.NOT_OK
	message = ""
	for condition in deep_get(obj, "status#conditions"):
		if len(message) == 0:
			# If we cannot find another message, then get the first
			message = deep_get(condition, "message", "")

		if deep_get(condition, "type", "") == "Ready":
			ready = deep_get(condition, "status", "False")

			if ready == "True":
				status_group = stgroup.OK
				message = deep_get(condition, "message", "")
			break
	return ready, message, status_group

def get_certsignreq_conditions(status):
	conditions = []
	for condition in deep_get(status, "conditions", {}):
		condition_name = deep_get(condition, "type", "")
		ready = deep_get(condition, "status", "False")
		if ready == "True":
			conditions.append(condition_name)

	certificate = deep_get(status, "certificate")

	if certificate is not None and len(certificate) > 0:
		conditions.append("Issued")

	return conditions

class PromRulesInfo:
	def __init__(self, group, ref, rtype, alertrecord, duration):
		self.group = group
		# The reference to the "true" resource object
		self.ref = ref
		self.rtype = rtype
		self.alertrecord = alertrecord
		self.duration = duration
	def __repr__(self):
		return repr((self.group, self.ref, self.rtype, self.alertrecord, self.duration))

def get_promrules_info(obj):
	info = []

	groups = deep_get(obj, "spec#groups", [])
	if len(groups) == 0:
		#name, ref
		return [PromRulesInfo("<none>", None, "", "", "")]

	for group in groups:
		for rule in deep_get(group, "rules"):
			name = deep_get(group, "name")
			ref = (rule, [name])
			alert = deep_get(rule, "alert", "")
			record = deep_get(rule, "record", "")
			if len(alert) > 0 and len(record) > 0:
				sys.exit("We need a better way to handle PrometheusRule; this one has both an alert and a record")
			elif len(alert) > 0:
				rtype = "Alert"
				alertrecord = alert
			elif len(record) > 0:
				rtype = "Record"
				alertrecord = record
			else:
				sys.exit("We need a better way to handle PrometheusRule; this one has neither alert nor record")
			age = deep_get(rule, "for", "")
			duration = iktlib.age_to_seconds(age)
			info.append(PromRulesInfo(name, ref, rtype, alertrecord, duration))

	return info

def get_promrule_expr(obj):
	field = []

	expr = deep_get(obj, "expr")

	tmp = split_msg(expr)

	for line in tmp:
		field.append([(f"{line}", curses.A_NORMAL)])

	return field

class SVCMonEndpointsInfo:
	def __init__(self, bearer_token_file, ref, port, target_port, interval, scheme, path, honor_labels, proxy_url):
		self.bearer_token_file = bearer_token_file
		# The reference to the "true" resource object
		self.ref = ref
		self.port = port
		self.target_port = target_port
		self.interval = interval
		self.scheme = scheme
		self.path = path
		self.honor_labels = honor_labels
		self.proxy_url = proxy_url
	def __repr__(self):
		return repr((self.bearer_token_file, self.ref, self.port, self.target_port, self.interval, self.scheme, self.path, self.honor_labels, self.proxy_url))

def get_svcmon_endpoints_info(obj):
	info = []

	if obj is None or len(deep_get(obj, "spec#endpoints", [])) == 0:
		#bearer_token_file, ref, port, target_port, interval, scheme, path, honor_labels, proxy_url
		return [SVCMonEndpointsInfo("<none>", None, "", "", "", "", "", "", "")]

	for item in deep_get(obj, "spec#endpoints"):
		bearer_token_file = deep_get(item, "bearerTokenFile", "")
		ref = item
		port = deep_get(item, "port", "")
		target_port = deep_get(item, "targetPort", "")
		interval = deep_get(item, "interval", "")
		scheme = deep_get(item, "scheme", "")
		path = deep_get(item, "path", "")
		honor_labels = deep_get(item, "honorLabels", "")
		proxy_url = deep_get(item, "proxyURL", "")
		info.append(SVCMonEndpointsInfo(bearer_token_file, ref, port, target_port, interval, scheme, path, honor_labels, proxy_url))

	return info

class GatewayServerInfo:
	def __init__(self, hosts, ref, port):
		self.hosts = hosts
		# The reference to the "true" resource object
		self.ref = ref
		self.port = port
	def __repr__(self):
		return repr((self.hosts, self.ref, self.port))

def get_gw_server_info(obj):
	info = []

	if obj is None or len(deep_get(obj, "spec#servers", [])) == 0:
		# hosts, ref, port
		return [GatewayServerInfo("<none>", None, [])]

	for item in deep_get(obj, "spec#servers"):
		hosts = deep_get(item, "hosts")
		ref = item
		port = get_port(deep_get(item, "port"))
		info.append(GatewayServerInfo(hosts, ref, port))

	return info

class LimitInfo:
	def __init__(self, ltype, name, ref, lmin, lmax, default_request, default_limit, max_lr_ratio):
		self.ltype = ltype
		self.name = name
		self.ref = ref
		self.lmin = lmin
		self.lmax = lmax
		self.default_request = default_request
		self.default_limit = default_limit
		self.max_lr_ratio = max_lr_ratio
	def __repr__(self):
		return repr((self.ltype, self.name, self.ref, self.lmin, self.lmax, self.default_request, self.default_limit, self.max_lr_ratio))

def get_limit_info(vlist):
	info = []

	if vlist == []:
		return []

	for obj in deep_get(vlist, "spec#limits", []):
		resources = set()

		for item in deep_get(obj, "default", []):
			resources.add(item)
		for item in deep_get(obj, "defaultRequest", []):
			resources.add(item)
		for item in deep_get(obj, "min", []):
			resources.add(item)
		for item in deep_get(obj, "max", []):
			resources.add(item)
		for item in deep_get(obj, "max", []):
			resources.add(item)
		for item in deep_get(obj, "maxLimitRequestRatio", []):
			resources.add(item)
		ltype = deep_get(obj, "type")
		ref = obj

		for item in resources:
			name = item
			lmin = deep_get(obj, "min#%s" % item, "-")
			lmax = deep_get(obj, "max#%s" % item, "-")
			default_request = deep_get(obj, "defaultRequest#%s" % item, "-")
			default_limit = deep_get(obj, "default#%s" % item, "-")
			max_lr_ratio = deep_get(obj, "maxLimitRequestRatio#%s" % item, "-")
			info.append(LimitInfo(ltype, name, ref, lmin, lmax, default_request, default_limit, max_lr_ratio))

	return info

class AuthRuleInfo:
	def __init__(self, source, operation, condition):
		self.source = source
		self.operation = operation
		self.condition = condition
	def __repr__(self):
		return repr((self.source, self.operation, self.condition))

def get_auth_rule_info(obj):
	info = []

	if obj == [] or len(deep_get(obj, "spec#rules", [])) == 0:
		# source, operation, condition
		return []

	for item in deep_get(obj, "spec#rules", []):
		sources = []
		operations = []
		conditions = []

		for source in deep_get(item, "from", []):
			principals = ",".join(deep_get(source, "source#principals", []))
			not_principals = ",".join(deep_get(source, "source#notPrincipals", []))
			request_principals = ",".join(deep_get(source, "source#requestPrincipals", []))
			not_request_principals = ",".join(deep_get(source, "source#notRequestPrincipals", []))
			namespaces = ",".join(deep_get(source, "source#namespaces", []))
			not_namespaces = ",".join(deep_get(source, "source#notNamespaces", []))
			ip_blocks = ",".join(deep_get(source, "source#ipBlocks", []))
			not_ip_blocks = ",".join(deep_get(source, "source#notIpBlocks", []))
			sources.append((principals, not_principals, request_principals, not_request_principals, namespaces, not_namespaces, ip_blocks, not_ip_blocks))

		for operation in deep_get(item, "to", []):
			hosts = ",".join(deep_get(operation, "operation#hosts", []))
			not_hosts = ",".join(deep_get(operation, "operation#notHosts", []))
			ports = ",".join(deep_get(operation, "operation#ports", []))
			not_ports = ",".join(deep_get(operation, "operation#notPorts", []))
			methods = ",".join(deep_get(operation, "operation#methods", []))
			not_methods = ",".join(deep_get(operation, "operation#notMethods", []))
			paths = ",".join(deep_get(operation, "operation#paths", []))
			not_paths = ",".join(deep_get(operation, "operation#notPaths", []))
			operations.append((hosts, not_hosts, ports, not_ports, methods, not_methods, paths, not_paths))

		for condition in deep_get(item, "when", []):
			key = deep_get(condition, "key")
			values = ",".join(deep_get(condition, "values", []))
			not_values = ",".join(deep_get(condition, "notValues", []))
			conditions.append((key, values, key, not_values))

		if len(sources) > 0 or len(operations) > 0 or len(conditions) > 0:
			info.append(AuthRuleInfo(sources, operations, conditions))

	return info

class PolicyRuleInfo:
	def __init__(self, api_group, resource, non_resource_urls, resource_names, verbs):
		self.api_group = api_group
		self.resource = resource
		self.non_resource_urls = non_resource_urls
		self.resource_names = resource_names
		self.verbs = verbs
	def __repr__(self):
		return repr((self.api_group, self.resource, self.non_resource_urls, self.resource_names, self.verbs))

def get_policy_rule_info(role):
	info = []

	if role == []:
		#api_group, resource, non_resource_urls, resource_names, verbs
		return [PolicyRuleInfo(["<none>"], [], [], [], [])]

	# We want one entry per resource, hence a dict
	# Structure:
	# {
	# (api_group, resource): {
	#	"non_resource_urls": [],
	#	"resource_names": [],
	#	"verbs": [],
	# }
	resources = {}

	for rule in deep_get(role, "rules", []):
		non_resource_urls = deep_get(rule, "nonResourceURLs", [])
		resource_names = deep_get(rule, "resourceNames", [])
		verbs = deep_get(rule, "verbs", [])
		api_groups = deep_get(rule, "apiGroups", [""])
		resourcelist = deep_get(rule, "resources", [""])

		for api_group in api_groups:
			for resource in resourcelist:
				tmp = (api_group, resource)
				if tmp not in resources:
					resources[tmp] = {
						"non_resource_urls": {},
						"resource_names": {},
						"verbs": {},
					}
				for non_resource_url in non_resource_urls:
					resources[tmp]["non_resource_urls"][non_resource_url] = {}
				for resource_name in resource_names:
					resources[tmp]["resource_names"][resource_name] = {}
				for verb in verbs:
					resources[tmp]["verbs"][verb] = {}

	for item in resources.items():
		resource, data = item
		resource, api_group = resource
		non_resource_urls = list(deep_get(data, "non_resource_urls", {}))
		resource_names = list(deep_get(data, "resource_names", {}))
		verbs = list(deep_get(data, "verbs", {}))
		info.append(PolicyRuleInfo(api_group, resource, non_resource_urls, resource_names, verbs))

	return info

def get_ingress_hosts(obj):
	hosts = []
	ports = []

	for rule in deep_get(obj, "spec#rules", []):
		if deep_get(rule, "host") is not None:
			hosts.append(deep_get(rule, "host"))

		if deep_get(rule, "http") is not None:
			if "80" not in ports:
				ports.append("80")

	if deep_get(obj, "spec#tls") is not None:
		if "443" not in ports:
			ports.append("443")

	if hosts == []:
		hosts.append("*")

	return hosts, ports

class IngressRuleInfo:
	def __init__(self, host, path, backends):
		self.host = host
		self.path = path
		self.backends = backends
	def __repr__(self):
		return repr((self.host, self.path, self.backends))

def get_ingress_rule_info(ingress):
	ingress_rules = []

	if deep_get(ingress, "spec#rules") is None:
		#host, path, backends
		return [IngressRuleInfo("<none>", "", [])]

	for rule in deep_get(ingress, "spec#rules", []):
		host = deep_get(rule, "host", "*")

		if deep_get(rule, "http") is not None:
			rule_backend = []
			for path in deep_get(rule, "http#paths", []):
				rule_path = deep_get(path, "path", "*")
				if "service" in deep_get(path, "backend"):
					service_name = deep_get(path, "backend#service#name")
					service_port = deep_get(path, "backend#service#number")
				else:
					service_name = deep_get(path, "backend#serviceName")
					service_port = deep_get(path, "backend#servicePort")
				ingress_rules.append(IngressRuleInfo(host, rule_path, (service_name, service_port)))
	return ingress_rules

class ServiceAccountSecretInfo:
	def __init__(self, stype, namespace, name, ref):
		self.stype = stype
		self.namespace = namespace
		self.name = name
		# The reference to the "true" resource object
		self.ref = ref
	def __repr__(self):
		return repr((self.stype, self.namespace, self.name, self.ref))

def get_sas_info(serviceaccount):
	info = []

	if serviceaccount == []:
		#stype, namespace, name, ref
		return [ServiceAccountSecretInfo("", "N/A", "<none>", None)]

	for secret in deep_get(serviceaccount, "secrets", []):
		snamespace = deep_get(secret, "namespace", deep_get(serviceaccount, "metadata#namespace"))
		secret_name = deep_get(secret, "name")

		# Get a reference to the secret
		ref = kh.get_ref_by_kind_name_namespace(("Secret", ""), secret_name, snamespace)

		info.append(ServiceAccountSecretInfo("Mountable", snamespace, secret_name, ref))

	for secret in deep_get(serviceaccount, "imagePullSecrets", []):
		snamespace = deep_get(secret, "namespace", deep_get(serviceaccount, "metadata#namespace"))
		secret_name = deep_get(secret, "name")

		# Get a reference to the secret
		ref = kh.get_ref_by_kind_name_namespace(("Secret", ""), secret_name, snamespace)

		info.append(ServiceAccountSecretInfo("Image Pull", snamespace, secret_name, ref))

	return info

class SubjectInfo:
	def __init__(self, namespace, api_group, name, ref, kind, rtype):
		self.namespace = namespace
		self.api_group = api_group
		self.name = name
		# The reference to the "true" resource object
		self.ref = ref
		self.kind = kind
		self.rtype = rtype
	def __repr__(self):
		return repr((self.namespace, self.api_group, self.name, self.ref, self.kind, self.rtype))

def get_subject_info(rolebinding):
	subject_info = []

	if rolebinding == [] or deep_get(rolebinding, "subjects") is None:
		#namespace, api_group, name, ref, kind, rtype
		return [SubjectInfo("N/A", "N/A", "<none>", None, "", "")]

	for subject in deep_get(rolebinding, "subjects", []):
		kind = deep_get(subject, "kind")
		name = deep_get(subject, "name")
		api_group = deep_get(subject, "apiGroup", "")
		namespace = deep_get(subject, "namespace", "")
		rtype = "[subject]"
		ref = None
		if kind in ["ServiceAccount", "User", "Group"]:
			try:
				ref = kh.get_ref_by_kind_name_namespace((kind, api_group), name, namespace)
			except:
				kind = f"*{kind}"
		else:
			raise Exception(f"Unknown subject type; kind: {kind}, api_group: {api_group}")
		subject_info.append(SubjectInfo(namespace, api_group, name, ref, (kind, api_group), rtype))

	kind = deep_get(rolebinding, "roleRef#kind")
	name = deep_get(rolebinding, "roleRef#name")
	api_group = deep_get(rolebinding, "roleRef#apiGroup", "")
	namespace = deep_get(rolebinding, "metadata#namespace", "")
	rtype = "[roleRef]"
	ref = kh.get_ref_by_kind_name_namespace((kind, api_group), name, namespace)
	if ref is None:
		kind = f"*{kind}"

	subject_info.append(SubjectInfo(namespace, api_group, name, ref, (kind, api_group), rtype))

	return subject_info

def get_name_by_kind_from_owner_references(owner_references = [], kind = ""):
	for owr in owner_references:
		if deep_get(owr, "kind", "") == kind:
			name = deep_get(owr, "name")
			break

	return name

def get_holder_kind_from_owner_references(owner_references = [], holder_name = ""):
	holder_kind = ""

	for owr in owner_references:
		if deep_get(owr, "name") == holder_name:
			holder_kind = deep_get(owr, "kind")
			break

	return holder_kind

def format_controller(controller, show_kind):
	if len(show_kind) > 0:
		if show_kind == "short" or len(controller[0][1]) == 0:
			controller = (f"{controller[0][0]}", f"{controller[1]}")
		elif show_kind == "full":
			controller = (f"{controller[0][0]}.{controller[0][1]}", f"{controller[1]}")
		elif show_kind == "mixed":
			# Strip the API group for standard controllers, but show for custom controllers
			if controller[0] in [("StatefulSet", "apps"), ("ReplicaSet", "apps"), ("DaemonSet", "apps"), ("Job", "batch"), ("CronJob", "batch"), ("Node", "")]:
				controller = (f"{controller[0][0]}", f"{controller[1]}")
			else:
				controller = (f"{controller[0][0]}.{controller[0][1]}", f"{controller[1]}")
		else:
			raise Exception(f"unknown value passed to show_kind: {show_kind}")
	else:
		controller = ("", f"{controller[1]}")

	return controller

def get_controller_from_owner_references(owner_references):
	controller = (("", ""), "")
	if owner_references is not None:
		for owr in owner_references:
			if deep_get(owr, "controller", False) == True:
				api_version = deep_get(owr, "apiVersion", "")
				tmp = re.match(r"(.*)/.*", api_version)
				if tmp is not None:
					api_group = tmp[1]
				else:
					api_group = ""
				kind = (deep_get(owr, "kind"), api_group)
				controller = (kind, deep_get(owr, "name"))
	return controller

class CiliumEndpointInfo:
	def __init__(self, namespace, name, ref, endpoint_id, identity_id, ingress_enforcement, egress_enforcement, visibility_policy, endpoint_state, ipv4, ipv6, age):
		self.namespace = namespace
		self.name = name
		# The reference to the "true" resource object
		self.ref = ref
		self.endpoint_id = endpoint_id
		self.identity_id = identity_id
		self.ingress_enforcement = ingress_enforcement
		self.egress_enforcement = egress_enforcement
		self.visibility_policy = visibility_policy
		self.endpoint_state = endpoint_state
		self.ipv4 = ipv4
		self.ipv6 = ipv6
		self.age = age
	def __repr__(self):
		return repr((self.namespace, self.name, self.ref, self.endpoint_id, self.identity_id, self.ingress_enforcement, self.egress_enforcement, self.visibility_policy, self.endpoint_state, self.ipv4, self.ipv6, self.age))

def get_cilium_ep_info(vlist, extra_vars = {}):
	info = []

	if vlist == []:
		return []

	for obj in vlist:
		name = deep_get(obj, "metadata#name")
		namespace = deep_get(obj, "metadata#namespace")
		ref = obj
		timestamp = timestamp_to_datetime(deep_get(obj, "metadata#creationTimestamp"))
		age = get_since(timestamp)
		endpoint_id = deep_get(obj, "status#id", "")
		identity_id = deep_get(obj, "status#identity#id", "")
		ingress_enforcement = deep_get(obj, "status#policy#ingress#enforcing", "")
		egress_enforcement = deep_get(obj, "status#policy#egress#enforcing", "")
		visibility_policy = deep_get(obj, "status#policy#visibility#enforcing", "")
		endpoint_state  = deep_get(obj, "status#state", "Unknown")
		ipv4 = []
		ipv6 = []
		for item in deep_get(obj, "status#networking#addressing", []):
			if "ipv4" in item:
				ipv4.append(item["ipv4"])
			if "ipv6" in item:
				ipv6.append(item["ipv6"])
		info.append(CiliumEndpointInfo(namespace, name, ref, endpoint_id, identity_id, ingress_enforcement, egress_enforcement, visibility_policy, endpoint_state, ipv4, ipv6, age))

	return info

def get_endpoint_ips(subsets):
	endpoints = []
	notready = 0

	if subsets is None:
		return ["<none>"]

	for subset in subsets:
		# Keep track of whether we have not ready addresses
		if deep_get(subset, "notReadyAddresses") is not None and len(deep_get(subset, "notReadyAddresses")) > 0:
			notready += 1

		if deep_get(subset, "addresses") is None:
			continue

		for address in deep_get(subset, "addresses", []):
			endpoints.append(deep_get(address, "ip"))

	if endpoints == []:
		if notready > 0:
			return ["<not ready>"]
		else:
			return ["<none>"]

	return endpoints


def get_endpointslices_ports(obj):
	ports = []
	if deep_get(obj, "ports") is None or deep_get(obj, "ports") == []:
		ports.append(("", "<none>", ""))
	else:
		for port in deep_get(obj, "ports", []):
			port_name = deep_get(port, "name", "")
			ports.append((port_name, str(deep_get(port, "port")), deep_get(port, "protocol")))
	return ports

def get_endpointslices_endpoints(obj):
	endpoints = []

	if deep_get(obj, "endpoints") is not None:
		for endpoint in deep_get(obj, "endpoints", []):
			for address in deep_get(endpoint, "addresses", []):
				ready = deep_get(endpoint, "conditions#ready", False)
				if ready == True:
					status_group = stgroup.OK
				else:
					status_group = stgroup.NOT_OK
				endpoints.append((address, status_group))
	else:
		endpoints.append(("<none>", stgroup.UNKNOWN))
	return endpoints

def get_endpoint_ports(subsets):
	ports = []

	if subsets is None:
		return [("", "<none>", "")]

	for subset in subsets:
		if deep_get(subset, "addresses") is None or deep_get(subset, "ports") is None:
			continue
		else:
			for port in deep_get(subset, "ports", []):
				name = deep_get(port, "name", "")
				ports.append((name, str(deep_get(port, "port")), deep_get(port, "protocol")))

	return ports

class SubsetsInfo:
	def __init__(self, addresses, ports_ep, status, status_group):
		self.addresses = addresses
		self.ports_ep = ports_ep
		self.status = status
		self.status_group = status_group
	def __repr__(self):
		return repr((self.addresses, self.ports_ep, self.status, self.status_group))

def get_subsets_info(ep):
	subsets = []

	if deep_get(ep, "subsets", []) == []:
		# addresses, ports, status, status_group
		return [SubsetsInfo(["<none>"], [], "", stgroup.NOT_OK)]

	# Policy for subsets expansion
	expand_subsets = deep_get(iktconfig, "Endpoints#expand_subsets", "None")

	for subset in deep_get(ep, "subsets", []):
		ready_addresses = []
		not_ready_addresses = []
		ports = []

		if deep_get(subset, "ports") is None:
			continue

		if len(deep_get(subset, "addresses", [])) == 0 and len(deep_get(subset, "notReadyAddresses", [])) == 0:
			continue

		for port in deep_get(subset, "ports", []):
			name = deep_get(port, "name", "")
			ports.append((name, deep_get(port, "port"), deep_get(port, "protocol")))

		for address in deep_get(subset, "addresses", []):
			ready_addresses.append(deep_get(address, "ip"))

		for not_ready_address in deep_get(subset, "notReadyAddresses", []):
			not_ready_addresses.append(deep_get(not_ready_address, "ip"))

		if expand_subsets == "None":
			if len(ready_addresses) > 0:
				subsets.append(SubsetsInfo(ready_addresses, ports, "Ready", stgroup.OK))
			if len(not_ready_addresses) > 0:
				subsets.append(SubsetsInfo(not_ready_addresses, ports, "Not Ready", stgroup.NOT_OK))
		elif expand_subsets == "Port":
			for port in ports:
				if len(ready_addresses) > 0:
					subsets.append(SubsetsInfo(ready_addresses, [port], "Ready", stgroup.OK))
				if len(not_ready_addresses) > 0:
					subsets.append(SubsetsInfo(not_ready_addresses, [port], "Not Ready", stgroup.NOT_OK))
		elif expand_subsets == "Address":
			for address in ready_addresses:
				subsets.append(SubsetsInfo([address], ports, "Ready", stgroup.OK))
			for address in not_ready_addresses:
				subsets.append(SubsetsInfo([address], ports, "Not Ready", stgroup.NOT_OK))
		elif expand_subsets == "Both":
			for port in ports:
				for address in ready_addresses:
					subsets.append(SubsetsInfo([address], [port], "Ready", stgroup.OK))
				for address in not_ready_addresses:
					subsets.append(SubsetsInfo([address], [port], "Not Ready", stgroup.NOT_OK))

	return subsets

class EPSSubsetsInfo:
	def __init__(self, addresstype, addresses, ports_eps, status, status_group, target_ref, topology):
		self.addresstype = addresstype
		self.addresses = addresses
		self.ports_eps = ports_eps
		self.status = status
		self.status_group = status_group
		self.target_ref = target_ref
		self.topology = topology
	def __repr__(self):
		return repr((self.addresstype, self.addresses, self.ports_eps, self.status, self.status_group, self.target_ref, self.topology))

def get_eps_subsets_info(eps):
	subsets = []

	if deep_get(eps, "endpoints") is None:
		# addresstype, addresses, ports, status, status_group, target_ref, topology
		return [EPSSubsetsInfo("", ["<none>"], [], "", stgroup.NOT_OK, [], [])]

	addresstype = deep_get(eps, "addressType")
	ports = []

	for port in deep_get(eps, "ports", []):
		port_name = deep_get(port, "name", "")
		ports.append((port_name, deep_get(port, "port"), deep_get(port, "protocol")))

	for endpoint in deep_get(eps, "endpoints", []):
		ready_addresses = []
		not_ready_addresses = []

		for address in deep_get(endpoint, "addresses", []):
			if deep_get(endpoint, "conditions#ready") == True:
				ready_addresses.append(address)
			else:
				not_ready_addresses.append(address)
		target_ref = (deep_get(endpoint, "targetRef#kind", ""), deep_get(endpoint, "targetRef#namespace", ""), deep_get(endpoint, "targetRef#name", ""))
		topology = []
		# If nodeName is available this is the new API where topology is replaced by nodeName and zone
		if "nodeName" in endpoint:
			topology.append(("nodeName", deep_get(endpoint, "nodeName", "<unset>")))
			if "zone" in endpoint:
				topology.append(("zone", deep_get(endpoint, "zone", "<unset>")))
		else:
			for key, value in deep_get(endpoint, "topology", {}).items():
				topology.append((key, value))

		if len(ready_addresses) > 0:
			subsets.append(EPSSubsetsInfo(addresstype, ready_addresses, ports, "Ready", stgroup.OK, target_ref, topology))
		if len(not_ready_addresses) > 0:
			subsets.append(EPSSubsetsInfo(addresstype, not_ready_addresses, ports, "Not Ready", stgroup.NOT_OK, target_ref, topology))

	return subsets

def get_as_status_and_service(conditions, service):
	available = "<unset>"
	message = ""
	status_group = stgroup.UNKNOWN

	for condition in conditions:
		if deep_get(condition, "type") == "Available":
			if deep_get(condition, "reason") == "Local":
				service = [("", "Local", "")]
			else:
				namespace = deep_get(service, "namespace")
				name = deep_get(service, "name")
				port = deep_get(service, "port")
				service = [(namespace, name, port)]
			available = deep_get(condition, "status")
			message = deep_get(condition, "message").rstrip()
			if available == "True":
				status_group = stgroup.OK
			else:
				status_group = stgroup.NOT_OK

	return available, service, message, status_group

class PodMetricsInfo:
	def __init__(self, name, cpu_millicores, mem_bytes):
		self.name = name
		self.cpu_millicores = cpu_millicores
		self.mem_bytes = mem_bytes
	def __repr__(self):
		return repr((self.name, self.cpu_millicores, self.mem_bytes))

def cpu_usage_to_millicores(cpu_usage):
	if cpu_usage == "0":
		return cpu_usage
	elif cpu_usage.endswith("n"):
		cpu_usage = f"{float(cpu_usage[:-1]) / 1000000:0.1f}"
		return cpu_usage
	elif cpu_usage.endswith("u"):
		cpu_usage = f"{float(cpu_usage[:-1]):0.1f}"
		return cpu_usage
	else:
		raise Exception("Unknown cpu usage metrics: {cpu_usage=}")

def get_pod_metrics_info(obj):
	info = []

	if obj is None:
		return info

	for item in deep_get(obj, "containers", []):
		name = deep_get(item, "name")
		cpu_usage = str(deep_get(item, "usage#cpu", "0"))
		cpu_millicores = cpu_usage_to_millicores(cpu_usage)
		mem_bytes = deep_get(item, "usage#memory")
		if mem_bytes.startswith("0"):
			mem_bytes = "0"
		else:
			mem_bytes += "B"
		info.append(PodMetricsInfo(name, cpu_millicores, mem_bytes))

	return info


class AppStatusInfo:
	def __init__(self, name, status, status_group, link):
		self.name = name
		self.status = status
		self.status_group = status_group
		self.link = link
	def __repr__(self):
		return repr((self.name, self.status, self.status_group, self.link))

def get_app_status_info(obj):
	info = []

	components = deep_get(obj, "status#components")

	if components is None or len(components) == 0:
		#name, status, status_group, link
		return [AppStatusInfo("N/A", "", stgroup.UNKNOWN, "")]

	for item in components:
		name = deep_get(item, "name")
		status = deep_get(item, "status")
		if status == "Ready":
			status_group = stgroup.OK
		else:
			status_group = stgroup.NOT_OK
		link = deep_get(item, "link")
		info.append(AppStatusInfo(name, status, status_group, link))

	return info

class ConfigMapDataInfo:
	def __init__(self, configmap, cm_name, cm_namespace, rtype, data):
		self.configmap = configmap
		self.cm_name = cm_name
		self.cm_namespace = cm_namespace
		self.ref = self if data is not None else None
		self.rtype = rtype
		self.data = data
	def __repr__(self):
		return repr((self.configmap, self.cm_name, self.cm_namespace, self.rtype, self.data))

def get_cm_data_info(cm):
	data = []

	cm_name = deep_get(cm, "metadata#name")
	cm_namespace = deep_get(cm, "metadata#namespace")
	for key, value in deep_get(cm, "binary_data", {}).items():
		data.append(ConfigMapDataInfo(key, cm_name, cm_namespace, "Binary", value))

	for key, value in deep_get(cm, "data", {}).items():
		data.append(ConfigMapDataInfo(key, cm_name, cm_namespace, identify_cmdata(key, cm_name, cm_namespace, value), value))

	if data == []:
		#configmap, rtype, data
		return [ConfigMapDataInfo("<none>", "", "", "Empty", None)]

	return data

class StrategyInfo:
	def __init__(self, strategy, name, operator, target):
		self.strategy = strategy
		self.name = name
		self.operator = operator
		self.target = target
	def __repr__(self):
		return repr((self.operator, self.rule, self.operator, self.target))

def get_strategy_info(vlist):
	info = []

	if vlist == []:
		return []

	deschedule_rules = deep_get(vlist, "spec#strategies#deschedule#rules", [])
	dontschedule_rules = deep_get(vlist, "spec#strategies#dontschedule#rules", [])
	scheduleonmetric_rules = deep_get(vlist, "spec#strategies#scheduleonmetric#rules", [])

	if len(deschedule_rules) > 0:
		strategy = "deschedule"
		rule = deschedule_rules[0]

		# Even though this is an array there's only one rule
		name = deep_get(rule, "metricname", "")
		operator = deep_get(rule, "operator", "")
		target = deep_get(rule, "target", -1)
		info.append(StrategyInfo(strategy, name, operator, target))

	if len(dontschedule_rules) > 0:
		strategy = "dontschedule"
		# dontschedule can have multiple rules; if it does we build a hackish tree
		if len(dontschedule_rules) > 1:
			info.append(StrategyInfo(strategy, "", "", -1))
			for rule in dontschedule_rules:
				name = rule.get("metricname", "")
				operator = rule.get("operator", "")
				target = rule.get("target", -1)
				info.append(StrategyInfo("", name, operator, target))
		else:
			rule = dontschedule_rules[0]
			name = rule.get("metricname", "")
			operator = rule.get("operator", "")
			target = rule.get("target", -1)
			info.append(StrategyInfo(strategy, name, operator, target))

	if len(scheduleonmetric_rules) > 0:
		strategy = "scheduleonmetric"
		rule = deschedule_rules[0]

		# Even though this is an array there's only one rule
		name = rule.get("metricname", "")
		operator = rule.get("operator", "")
		target = rule.get("target", -1)
		info.append(StrategyInfo(strategy, name, operator, target))

	return info

def get_job_state(obj, completed = False):
	state = "Unknown"
	status_group = stgroup.UNKNOWN

	if deep_get(obj, "status#active") == 1:
		state = "Running"
		status_group = stgroup.PENDING
	elif completed == True:
		if deep_get(obj, "status#failed") is not None:
			state = "Failed"
			status_group = stgroup.NOT_OK
		else:
			state = "Completed"
			status_group = stgroup.OK
	else:
		# Try to figure out why we neither completed nor are running
		reason = ""

		for condition in deep_get(obj, "status#conditions", []):
			if deep_get(condition, "type") != "Complete":
				reason = deep_get(condition, "reason", "").rstrip()

		if reason != "":
			state = "%s" % reason
			status_group = stgroup.NOT_OK

	return state, status_group

# Returns a list of tuples with their latest status
def get_conditions(conditions):
	condition_list = []

	if conditions is not None:
		for condition in conditions:
			ctype = deep_get(condition, "type")
			state = deep_get(condition, "status")
			reason = deep_get(condition, "reason")
			message = deep_get(condition, "message")
			condition_list.append((ctype, state, reason, message))

	return condition_list

# Raw events
def __get_events(obj, **kwargs):
	event_list = []

	kind = deep_get(obj, "kind")
	api_version = deep_get(obj, "apiVersion", "")
	name = deep_get(obj, "metadata#name")
	namespace = deep_get(obj, "metadata#namespace", "")
	event_list = get_events_by_kind_name_namespace(kh.kind_api_version_to_kind(kind, api_version), name, namespace)

	return event_list

# Raw conditions
def __get_conditions(obj, **kwargs):
	condition_list = []

	for condition in deep_get(obj, "status#conditions", []):
		ctype = deep_get(condition, "type", "")
		status = deep_get_with_fallback(condition, ["status", "phase"], "")
		last_probe = deep_get(condition, "lastProbeTime")
		if last_probe is None:
			last_probe = "<unset>"
		else:
			timestamp = timestamp_to_datetime(last_probe)
			last_probe = timestamp.astimezone().strftime("%Y-%m-%d %H:%M:%S")
		last_transition = deep_get(condition, "lastTransitionTime")
		if last_transition is None:
			last_transition = "<unset>"
		else:
			timestamp = timestamp_to_datetime(last_transition)
			last_transition = timestamp.astimezone().strftime("%Y-%m-%d %H:%M:%S")
		message = deep_get(condition, "message", "")
		condition_list.append((ctype, status, last_probe, last_transition, message))
	return condition_list

def get_status_latest_condition(conditions):
	latest_condition = None
	update_time = None

	if conditions is not None:
		for condition in conditions:
			last_heartbeat_time = deep_get(condition, "lastHeartbeatTime")
			last_transition_time = deep_get(condition, "lastTransitionTime", last_heartbeat_time)
			last_update_time = deep_get(condition, "lastUpdateTime", last_transition_time)

			if update_time is None or (last_update_time is not None and last_update_time > update_time):
				update_time = last_update_time
				latest_condition = condition

	status = "Unknown"
	status_group = stgroup.UNKNOWN
	status_message = ""

	if latest_condition is not None:
		ctype = deep_get(latest_condition, "type")
		cstatus = deep_get(latest_condition, "status")
		status_message = deep_get(latest_condition, "message", "")

		if ctype == "Failed" and cstatus == "True":
			status = ctype
			status_group = stgroup.NOT_OK
		elif ctype in ["ControllerHealthy", "Running"] and cstatus == "True":
			status = ctype
			status_group = stgroup.OK
		elif ctype == "Created" and cstatus == "True":
			status = ctype
			status_group = stgroup.PENDING

	return status, status_group, status_message

def get_dep_status(conditions):
	status = "Unknown"
	reason = ""
	status_group = stgroup.UNKNOWN

	for condition in conditions:
		if deep_get(condition, "type") == "Available":
			if deep_get(condition, "status") == "True":
				status = "Available"
				status_group = stgroup.OK
				reason = deep_get(condition, "message", "").rstrip()
				# Available has highest priority
				break
			else:
				status = "Not Available"
				status_group = stgroup.NOT_OK
				reason = deep_get(condition, "message", "").rstrip()
		elif deep_get(condition, "type") == "Progressing":
			if deep_get(condition, "status") == "True":
				status = "Progressing"
				status_group = stgroup.PENDING
				reason = deep_get(condition, "message", "").rstrip()
			else:
				status = "Not Progressing"
				status_group = stgroup.NOT_OK
				reason = deep_get(condition, "message", "").rstrip()
		elif deep_get(condition, "type") == "ReplicaFailure":
				status = "Replica Failure"
				status_group = stgroup.NOT_OK
				reason = deep_get(condition, "message", "").rstrip()
		else:
			sys.exit(f"Unknown deployment condition: {deep_get(condition, 'type')}, reason: {deep_get(condition, 'message')}")

	return status, status_group, reason

def get_desired_replicas(rs):
	desired = -1
	max_replicas = -1
	for annotation, value in deep_get(rs, "metadata#annotations", {}).items():
		if annotation == "deployment.kubernetes.io/desired-replicas":
			desired = value
		elif annotation == "deployment.kubernetes.io/max-replicas":
			max_replicas = value
	return desired, max_replicas

def get_webhook_info(vlist, extra_vars = {}):
	info = []

	if vlist == [] or len(vlist["webhooks"]) == 0:
		return []

	for obj in vlist["webhooks"]:
		d = {
			"name": obj["name"]
		}
		info.append(type("InfoClass", (), d))

	return info

class RQItemInfo:
	def __init__(self, resource, used, hard):
		self.resource = resource
		self.used = used
		self.hard = hard
	def __repr__(self):
		return repr((self.resource, self.used, self.hard))

def get_rq_item_info(obj):
	info = []

	if rqi == []:
		#resource, used, hard
		return [RQItemInfo("", "", "")]

	for resource in deep_get(obj, "spec#hard", []):
		used = deep_get(obj, f"status#used#{resource}", [])
		hard = deep_get(obj, f"spec#hard#{resource}", [])

		info.append(RQItemInfo(resource, used, hard))

	return info

class RuntimeClassOverheadInfo:
	def __init__(self, resource, overhead):
		self.resource = resource
		self.overhead = overhead
	def __repr__(self):
		return repr((self.resource, self.overhead))

def get_runtime_class_overhead_info(obj):
	info = []

	if obj is None or len(deep_get(obj, "overhead", {})) == 0:
		# resource, overhead
		return [RuntimeClassOverheadInfo("<none>", "")]

	for resource, overhead in deep_get(obj, "overhead"):
		info.append(RuntimeClassOverheadInfo(resource, overhead))

	return info

class KeyValueInfo:
	def __init__(self, key, decoded_value, value, vtype, vlen):
		self.key = key
		self.decoded_value = decoded_value
		self.value = value
		self.vtype = vtype
		self.vlen = vlen
	def __repr__(self):
		return repr((self.key, self.decoded_value, self.value, self.vtype, self.vlen))

def decode_value(value):
	# Is this base64?
	try:
		tmp = base64.b64decode(value)
		vtype = "base64"
	except Exception as e:
		vtype = "string"

	if vtype == "base64":
		try:
			tmp = base64.b64decode(value).decode("utf-8")
			if "\n" in tmp:
				vtype = "base64-utf-8"
			else:
				vtype = "string"
				value = tmp
		except UnicodeDecodeError as e:
			vtype = "base64-binary"

		try:
			tmp = base64.b64decode(base64.b64decode(value))
			if tmp[0] == 0x1f and tmp[1] == 0x8b:
				vtype = "gzip"
				value = tmp
		except:
			pass

	return vtype, value

def get_key_value_info(vlist):
	info = []

	if vlist == []:
		return []

	for key, value in vlist:
		decoded_value = ""

		vtype, value = decode_value(value)
		vlen = len(value)
		decoded_value = value

		if vlen == 0:
			value = ""
			vtype = "empty"

		if len(decoded_value) > 8192 and len(decoded_value) > 0:
			vtype = f"{vtype} [truncated]"
			decoded_value = value[0:8192 - 1]

		info.append(KeyValueInfo(key, decoded_value, value, vtype, vlen))

	return info

def get_external_ips(svc):
	external_ips = []

	for xip in deep_get(svc, "spec#externalIPs", []):
		external_ips.append(xip)

	if external_ips == []:
		external_ips.append("<none>")

	return external_ips

def get_port(obj):
	port = ("", "", "")

	if obj is not None:
		name = deep_get(obj, "name", "")
		if "port" in obj:
			pport = deep_get(obj, "port", "")
		else:
			pport = deep_get(obj, "number", "")
		protocol = deep_get(obj, "protocol", "")
		port = (name, pport, protocol)

	return port

def get_svc_ports(svc):
	ports = []

	for port in deep_get(svc, "spec#ports", []):
		ports.append(get_port(port))

	if ports == []:
		ports = [("", "", "")]
	return ports

def get_svc_port_target_endpoints(obj, **kwargs):
	svcname = deep_get(obj, "metadata#name")
	svcnamespace = deep_get(obj, "metadata#namespace")
	port_target_endpoints = []
	stype = deep_get(obj, "spec#type")
	cluster_ip = deep_get(obj, "spec#clusterIP")
	endpoints = []

	ref = kh.get_ref_by_kind_name_namespace(("Endpoints", ""), svcname, svcnamespace)
	endpoints = get_endpoint_ips(deep_get(ref, "subsets"))

	for port in deep_get(obj, "spec#ports", []):
		name = deep_get(port, "name", "")
		svcport = deep_get(port, "port", "")
		protocol = deep_get(port, "protocol", "")
		if stype in ["NodePort", "LoadBalancer"]:
			node_port = deep_get(port, "nodePort", "Auto Allocate")
		else:
			node_port = "N/A"
		if cluster_ip is not None:
			target_port = deep_get(port, "targetPort", "")
		else:
			target_port = ""
		endpointstr = (":%s, " % target_port).join(endpoints)
		if len(endpointstr) > 0:
			endpointstr += ":%s" % target_port
		port_target_endpoints.append((f"{name}:{svcport}/{protocol}", f"{node_port}", f"{target_port}/{protocol}", endpointstr))

	if len(port_target_endpoints) == 0:
		port_target_endpoints = [("<none>", "", "", "")]

	return port_target_endpoints

def get_event_status_group(status):
	if status == "Error":
		status_group = stgroup.NOT_OK
	elif status == "Warning":
		status_group = stgroup.ADMIN
	elif status == "Normal":
		status_group = stgroup.OK
	else:
		# FIXME
		status_group = stgroup.UNKNOWN

	return status_group

class FilterInfo:
	def __init__(self, name, ftype):
		self.name = name
		self.ftype = ftype
	def __repr__(self):
		return repr((self.name, self.ftype))

def get_filter_info(obj):
	info = []

	if len(deep_get(obj, "spec#filters", [])) == 0:
		#name, ftype
		return [FilterInfo("<none>", "")]

	for efilter in deep_get(obj, "spec#filters", []):
		name = deep_get(efilter, "filterName")
		ftype = deep_get(efilter, "filterType")

		#name, ftype
		info.append(FilterInfo(name, ftype))

	return info

class ResourceInfo:
	def __init__(self, resource_tuple, ref, rtype, kind, status, status_group, restarts, message, age = -1):
		self.resource_tuple = resource_tuple
		# The reference to the "true" resource object
		self.ref = ref
		self.rtype = rtype
		self.kind = kind
		self.status = status
		self.status_group = status_group
		self.restarts = restarts
		self.message = message
		self.age = age
	def __repr__(self):
		return repr((self.resource_tuple, self.ref, self.rtype, self.kind, self.status, self.status_group, self.restarts, self.message, age))

def get_container_status(src_statuses, container, kind):
	reason = "UNKNOWN"
	status_group = stgroup.UNKNOWN
	restarts = 0
	message = ""

	if src_statuses is None:
		return reason, status_group, -1, message

	for container_status in src_statuses:
		if deep_get(container_status, "name") == container:
			restarts = deep_get(container_status, "restartCount")
			running = deep_get(container_status, "state#running")

			if deep_get(container_status, "ready") == False:
				status_group = stgroup.NOT_OK

				if running is not None:
					reason = "Running"
				elif deep_get(container_status, "state#terminated") is not None:
					reason = deep_get(container_status, "state#terminated#reason", "ErrNotSet")
					if deep_get(container_status, "state#terminated#exitCode") == 0:
						status_group = stgroup.DONE

					if deep_get(container_status, "state#terminated#message") is not None:
						message = deep_get(container_status, "state#terminated#message").rstrip()
				else:
					reason = deep_get(container_status, "state#waiting#reason", "").rstrip()

					if deep_get(container_status, "state#waiting#message") is not None:
						message = deep_get(container_status, "state#waiting#message").rstrip()
			else:
				if running is None:
					reason = deep_get(container_status, "state#terminated#reason", "").rstrip()

					if deep_get(container_status, "state#terminated#message") is not None:
						message = deep_get(container_status, "state#terminated#message").rstrip()

					if deep_get(container_status, "state#terminated#exitCode") == 0:
						status_group = stgroup.DONE
					else:
						status_group = stgroup.NOT_OK
				else:
					reason = "Running"
					status_group = stgroup.OK
			break

	return reason, status_group, restarts, message

def make_set_expression(expression_list):
	expressions = []

	if expression_list is not None:
		for expression in expression_list:
			operator = deep_get(expression, "operator", "")
			if operator == "In":
				operator = "In "
			elif operator == "NotIn":
				operator = "Not In "
			elif operator == "Exists":
				operator = "Exists"
			elif operator == "DoesNotExist":
				operator = "Does Not Exist"
			elif operator == "Gt":
				operator = "> "
			elif operator == "Lt":
				operator = "< "
			key = deep_get(expression, "key", "")

			tmp = deep_get(expression, "values", [])
			values = ",".join(tmp)

			expressions.append(f"{key} {operator}{values}")

	return ",".join(expressions)

def get_pv_status(pv):
	phase = deep_get(pv, "status#phase")

	if phase in ["Bound", "Available"]:
		reason = phase
		status_group = stgroup.OK
	elif phase in ["Released", "Pending"]:
		reason = phase
		status_group = stgroup.PENDING
	else:
		reason = deep_get(pv, "status#reason", "").strip()
		status_group = stgroup.NOT_OK
	return reason, status_group

def get_pvc_status(pvc):
	phase = deep_get(pvc, "status#phase", "")

	if phase in ["Bound", "Available"]:
		status_group = stgroup.OK
	elif phase in ["Released", "Pending"]:
		status_group = stgroup.PENDING
	else:
		status_group = stgroup.NOT_OK
	return phase, status_group

def get_pv_from_pvc_name(pvc_name):
	pv = None
	pv_name = None
	field_selector = f"metadata.name={pvc_name}"

	for pvc in kh.get_list_by_kind_namespace(("PersistentVolumeClaim", ""), "", field_selector = field_selector):
		if deep_get(pvc, "metadata#name") == pvc_name:
			volume_name = deep_get(pvc, "spec#volumeName")
			if volume_name is not None:
				pv_name = volume_name
				pv = kh.get_ref_by_kind_name_namespace(("PersistentVolume", ""), pv_name, None)
				break

	return pv, pv_name

def get_pods_by_pvc(pvc):
	pods = []

	pvc_name = deep_get(pvc, "metadata#name")
	pvc_namespace = deep_get(pvc, "metadata#namespace")
	all_pods = kh.get_list_by_kind_namespace(("Pod", ""), pvc_namespace)
	for pod in all_pods:
		for volume in deep_get(pod, "spec#volumes", []):
			if deep_get(volume, "persistentVolumeClaim") is None:
				continue
			if deep_get(volume, "persistentVolumeClaim#claimName") == pvc_name:
				pods.append(pod)
	return pods

def get_pods_by_pv(pv):
	claim_ref = deep_get(pv, "spec#claimRef")

	if claim_ref is None:
		return []

	# PV<=>PVC is a 1:1 mapping, but multiple pods can share the same PVC
	name = deep_get(claim_ref, "name")
	namespace = deep_get(claim_ref, "namespace")
	pvc = kh.get_ref_by_kind_name_namespace(("PersistentVolumeClaim", ""), name, namespace)

	if pvc is None:
		return []

	return get_pods_by_pvc(pvc)

def get_pods_by_pc(pc):
	pods = []

	for pod in kh.get_list_by_kind_namespace(("Pod", ""), ""):
		if deep_get(pod, "spec#priorityClassName") == pc:
			pods.append(pod)

	return pods

def get_pods_by_node(node):
	pods = []

	for pod in kh.get_list_by_kind_namespace(("Pod", ""), ""):
		if deep_get(pod, "spec#nodeName") == node:
			pods.append(pod)

	return pods

def abbreviate_access_mode(mode):
	if mode == "ReadWriteOnce":
		abbr = "RWO "
	elif mode == "ReadWriteOncePod":
		abbr = "RWOP"
	elif mode == "ReadWriteMany":
		abbr = "RWM "
	elif mode == "ReadOnlyMany":
		abbr = "ROX "
	else:
		raise Exception(f"Unknown access mode {mode}")

	return abbr

def get_pod_log_by_name_namespace_container(name, namespace, container, tail_lines = default_tail_lines):
	internal_error  = False

	rawmsg, status = kh.read_namespaced_pod_log(name, namespace, container = container, tail_lines = tail_lines)
	if status == 200:
		# Everything is successful
		internal_error = False
	elif status == 500:
		# Not successful; error in rawmsg
		internal_error = True
	else:
		rawmsg = "%s CRITICAL: Failed to fetch log for pod (name: %s, namespace: %s, container: %s); Request Status: %s" % (datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S"), name, namespace, container, status)
		internal_error = True

	if rawmsg.startswith("Unable to retrieve container logs for"):
		rawmsg = "%s CRITICAL: %s" % (datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S"), rawmsg)
		internal_error = True

	return rawmsg, internal_error

def resource_kind_to_rtype(resource):
	rtypes = {
		("ConfigMap", ""): "[configmap]",
		("Container", ""): "[container]",
		("Controller", ""): "[controller]",
		("ControllerRevision", "apps"): "[controller_revision]",
		("CronJob", "batch"): "[job_controller]",
		("DaemonSet", "apps"): "[controller]",
		("Deployment", "apps"): "[controller]",
		("Endpoints", ""): "[endpoints]",
		("EndpointSlice", "discovery.k8s.io"): "[endpoint_slice]",
		("EphemeralContainer", ""): "[ephemeral_container]",
		("Event", ""): "[event]",
		("HorizontalPodAutoscaler", "autoscaling"): "[pod_autoscaler]",
		("Ingress", "networking.k8s.io"): "[ingress]",
		("InitContainer", ""): "[init_container]",
		("Job", "batch"): "[controller]",
		("Lease", "coordination.k8s.io"): "[lease]",
		("LimitRange", ""): "[limit]",
		("MutatingWebhookConfiguration", "admissionregistration.k8s.io"): "[webhook_configuration]",
		("Node", ""): "[node]",
		("PersistentVolume", ""): "[volume]",
		("PersistentVolumeClaim", ""): "[volume_claim]",
		("Pod", ""): "[pod]",
		("PodDisruptionBudget", "policy"): "[pod_disruption_budget]",
		("PodMetrics", "metrics.k8s.io"): "[pod_metrics]",
		("PriorityClass", "scheduling.k8s.io"): "[priority_class]",
		("ReplicaSet", "apps"): "[controller]",
		("ReplicationController", ""): "[controller]",
		("Role", "rbac.authorization.k8s.io"): "[role]",
		("RoleBinding", "rbac.authorization.k8s.io"): "[role_binding]",
		("Scheduler", ""): "[scheduler]",
		("Secret", ""): "[secret]",
		("Service", ""): "[service]",
		("ServiceAccount", ""): "[service_account]",
		("ServiceEntry", ""): "[service_entry]",
		("StatefulSet", "apps"): "[controller]",
		("TASPolicy", "telemetry.intel.com"): "[scheduling_policy]",
		("TFJob", "kubeflow.org"): "[FIXME]",
		("ValidatingWebhookConfiguration", "admissionregistration.k8s.io"): "[webhook_configuration]",
		("Workflow", ""): "[controller]",
	}

	return rtypes.get(resource, "[unknown]")

def get_resource_info_by_last_applied_configuration(configuration, resources = []):
	resource_info = []

	for resource in resources:
		for item in kh.get_list_by_kind_namespace(resource, ""):
			last_applied_configuration = deep_get(item, "metadata#annotations#kubectl.kubernetes.io/last-applied-configuration", {})
			if last_applied_configuration is None or len(last_applied_configuration) == 0:
				continue
			data = json.loads(last_applied_configuration)
			match = True
			for key in configuration:
				if key not in data or configuration[key] != data[key]:
					match = False
					break
			if match == True:
				name = deep_get(item, "metadata#name")
				namespace = deep_get(item, "metadata#namespace", "")
				d = {
					"kind": kh.guess_kind(resource),
					"metadata": {
						"namespace": namespace,
						"name": name,
					}
				}
				resource_info.append(d)

	return resource_info

def get_resource_info_by_namespace(namespace, resources):
	resource_info = []

	for resource in resources:
		for item in kh.get_list_by_kind_namespace(resource, namespace):
			ref = item
			kind = resource
			rtype = resource_kind_to_rtype(resource)
			status = "N/A"
			status_group = stgroup.OK # FIXME
			name = deep_get(item, "metadata#name")
			resource_tuple = ("", name)
			resource_info.append(ResourceInfo(resource_tuple, ref, rtype, kind, status, status_group, -1, ""))

	return resource_info

def get_resource_info(obj):
	init_containers = True
	containers = True
	ephemeral_containers = True

	resource_info = []

	pod_name = deep_get(obj, "metadata#name")
	pod_namespace = deep_get(obj, "metadata#namespace")

	if init_containers == True:
		for container in deep_get(obj, "spec#initContainers", []):
			ref = container
			container_name = deep_get(container, "name")
			resource_tuple = ("", container_name)
			kind = ("InitContainer", "")
			rtype = resource_kind_to_rtype(kind)
			status, status_group, restarts, message = get_container_status(deep_get(obj, "status#initContainerStatuses"), container_name, kind)
			resource_info.append(ResourceInfo(resource_tuple, ref, rtype, kind, status, status_group, restarts, message))

	if containers == True:
		for container in deep_get(obj, "spec#containers", []):
			ref = container
			container_name = deep_get(container, "name")
			resource_tuple = ("", container_name)
			kind = ("Container", "")
			rtype = resource_kind_to_rtype(kind)
			status, status_group, restarts, message = get_container_status(deep_get(obj, "status#containerStatuses"), container_name, kind)
			resource_info.append(ResourceInfo(resource_tuple, ref, rtype, kind, status, status_group, restarts, message))

	if ephemeral_containers == True:
		for container in deep_get(obj, "spec#ephemeralContainers", []):
			ref = container
			container_name = deep_get(container, "name")
			resource_tuple = ("", container_name)
			kind = ("EphemeralContainer", "")
			rtype = resource_kind_to_rtype(kind)
			status, status_group, restarts, message = get_container_status(deep_get(obj, "status#ephemeralContainerStatuses"), container_name, kind)
			resource_info.append(ResourceInfo(resource_tuple, ref, rtype, kind, status, status_group, restarts, message))

	# It should be impossible for a pod to have more than one node, but we'll use plural anyway
	if "nodes" not in deep_get(iktconfig, "Pods#filter_resources", []):
		name = deep_get(obj, "spec#nodeName")
		node = None
		if name is not None:
			node = kh.get_ref_by_kind_name_namespace(("Node", ""), name, None)

		if node is not None:
			ref = node
			# We ignore the taints
			status, status_group, taints, full_taints = get_node_status(node)
			kind = ("Node", "")
			rtype = resource_kind_to_rtype(kind)
			resource_tuple = ("", name)
			resource_info.append(ResourceInfo(resource_tuple, ref, rtype, kind, status, status_group, -1, ""))

	# We always need to get the controller even if we don't include it in the resource info
	owr_ref = None
	for owr in deep_get(obj, "metadata#ownerReferences", []):
		owr_kind = deep_get(owr, "kind", "")
		owr_name = deep_get(owr, "name", "")
		is_controller = deep_get(owr, "controller", False)
		if is_controller == False:
			continue
		# We don't have to guess the rtype; it's a controller
		rtype = "[controller]"

		if deep_get(owr, "controller") is not None:
			owr_ref = kh.get_ref_from_owr(owr, pod_namespace)
			if "controller" not in deep_get(iktconfig, "Pods#filter_resources", []):
				resource_tuple = (owr_kind, owr_name)
				kind = kh.guess_kind(owr_kind)
				resource_info.append(ResourceInfo(resource_tuple, owr_ref, rtype, kind, "", stgroup.OK, -1, ""))
			break

	if "persistent_volume_claims" not in deep_get(iktconfig, "Pods#filter_resources", []):
		for volume in deep_get(obj, "spec#volumes", []):
			if deep_get(volume, "persistentVolumeClaim") is None:
				continue

			kind = ("PersistentVolume", "")
			rtype = resource_kind_to_rtype(kind)

			claim_name = deep_get(volume, "persistentVolumeClaim#claimName")
			pv, pv_name = get_pv_from_pvc_name(claim_name)

			if pv is not None:
				status, status_group = get_pv_status(pv)
			else:
				pv_name = "<INVALID PV>"
				status = "ERROR"
				status_group = stgroup.NOT_OK
			ref = pv
			resource_tuple = ("", pv_name)
			resource_info.append(ResourceInfo(resource_tuple, ref, rtype, kind, status, status_group, -1, ""))

	if "service_account" not in deep_get(iktconfig, "Pods#filter_resources", []) and deep_get(obj, "spec#serviceAccountName") is not None:
		kind = ("ServiceAccount", "")
		rtype = resource_kind_to_rtype(kind)
		status = ""
		status_group = stgroup.UNKNOWN

		name = deep_get(obj, "spec#serviceAccountName")
		ref = kh.get_ref_by_kind_name_namespace(kind, name, pod_namespace)
		resource_tuple = ("", name)
		resource_info.append(ResourceInfo(resource_tuple, ref, rtype, kind, status, status_group, -1, ""))

	status = ""
	status_group = stgroup.UNKNOWN

	for vol in deep_get(obj, "spec#volumes", []):
		if deep_get(vol, "secret") is not None and "secrets" not in deep_get(iktconfig, "Pods#filter_resources", []):
			kind = ("Secret", "")
			rtype = resource_kind_to_rtype(kind)
			secret_name = deep_get(vol, "secret#secretName")
			resource_tuple = ("", secret_name)
			ref = kh.get_ref_by_kind_name_namespace(kind, secret_name, pod_namespace)
			if deep_get(vol, "secret#optional") == False:
				# Warn if a secret is non-optional and missing
				if ref is None:
					status = "Missing!"
			else:
				# Don't add optional, non-existing secrets
				if ref is None:
					continue
			resource_info.append(ResourceInfo(resource_tuple, ref, rtype, kind, status, status_group, -1, ""))
		if deep_get(vol, "configMap") is not None and "config_maps" not in deep_get(iktconfig, "Pods#filter_resources", []):
			kind = ("ConfigMap", "")
			rtype = resource_kind_to_rtype(kind)
			cm_name = deep_get(vol, "configMap#name")
			resource_tuple = ("", cm_name)
			ref = kh.get_ref_by_kind_name_namespace(kind, cm_name, pod_namespace)
			resource_info.append(ResourceInfo(resource_tuple, ref, rtype, kind, status, status_group, -1, ""))

	for vol in deep_get(obj, "spec#imagePullSecrets", []):
		if "secrets" not in deep_get(iktconfig, "Pods#filter_resources", []):
			kind = ("Secret", "")
			rtype = "[image_pull_secret]"
			secret_name = deep_get(vol, "name")
			resource_tuple = ("", secret_name)
			ref = kh.get_ref_by_kind_name_namespace(kind, secret_name, pod_namespace)
			resource_info.append(ResourceInfo(resource_tuple, ref, rtype, kind, status, status_group, -1, ""))

	if "events" not in deep_get(iktconfig, "Pods#filter_resources", []):
		kind = ("Event", "")
		rtype = resource_kind_to_rtype(kind)

		for event in kh.get_list_by_kind_namespace(kind, ""):
			event_involved_object_name = deep_get_with_fallback(event, ["regarding#name", "involvedObject#name"])
			event_involved_object_namespace = deep_get_with_fallback(event, ["regarding#namespace", "involvedObject#namespace"])
			if event_involved_object_name == pod_name and event_involved_object_namespace == pod_namespace:
				resource_tuple = (kind[0], deep_get(event, "metadata#name"))
				status = deep_get(event, "type")
				status_group = get_event_status_group(status)
				tmp = __get_timestamp_with_fallback(event, ["series#lastObservedTime", "deprecatedLastTimestamp", "lastTimestamp", "eventTime", "deprecatedFirstTimestamp", "firstTimestamp"])
				seen = get_since(tmp)
				message = deep_get(event, "message").rstrip()
				resource_info.append(ResourceInfo(resource_tuple, event, rtype, kind, status, status_group, -1, message, age = seen))

	# XXX
	if "scheduler" not in deep_get(iktconfig, "Pods#filter_resources", []):
		scheduler_name = deep_get(obj, "spec#schedulerName")
		if scheduler_name is not None:
			kind = ("Scheduler", "")
			rtype = resource_kind_to_rtype(kind)
			resource_tuple = ("", scheduler_name)
			status = ""
			status_group = stgroup.ADMIN
			resource_info.append(ResourceInfo(resource_tuple, None, rtype, kind, status, status_group, -1, ""))

	if "pod_disruption_budgets" not in deep_get(iktconfig, "Pods#filter_resources", []):
		# XXX: Are there are other means of specifying what pod to apply the PDB to than this?
		if "app" in deep_get(obj, "metadata#labels", {}):
			kind = ("PodDisruptionBudget", "policy")
			for pdb in kh.get_list_by_kind_namespace(kind, "", label_selector = "app=%s" % deep_get(obj, "metadata#labels")["app"]):
				rtype = resource_kind_to_rtype(kind)
				pdb_name = deep_get(pdb, "metadata#name")
				pdb_namespace = deep_get(pdb, "metadata#namespace")
				resource_tuple = ("", pdb_name)
				status = ""
				status_group = stgroup.ADMIN
				ref = kh.get_ref_by_kind_name_namespace(kind, pdb_name, pdb_namespace)
				resource_info.append(ResourceInfo(resource_tuple, ref, rtype, kind, status, status_group, -1, ""))

	if "priority_class" not in deep_get(iktconfig, "Pods#filter_resources", []):
		priority_class_name = deep_get(obj, "spec#priorityClassName")
		if priority_class_name is not None:
			kind = ("PriorityClass", "scheduling.k8s.io")
			resource_tuple = ("", priority_class_name)
			rtype = resource_kind_to_rtype(kind)
			status = ""
			status_group = stgroup.ADMIN
			ref = kh.get_ref_by_kind_name_namespace(kind, priority_class_name, None)
			resource_info.append(ResourceInfo(resource_tuple, ref, rtype, kind, status, status_group, -1, ""))

	if "cron_job" not in deep_get(iktconfig, "Pods#filter_resources", []):
		if owr_ref is not None and owr_kind == ("Job", "batch"):
			kind, cname = get_controller_from_owner_references(deep_get(owr_ref, "metadata#ownerReferences"))
			if kind == ("CronJob", "batch"):
				controller = kh.get_ref_by_kind_name_namespace(kind, cname, pod_namespace)
				ref = controller
				rtype = resource_kind_to_rtype(kind)
				resource_tuple = (kind, cname)
				resource_info.append(ResourceInfo(resource_tuple, ref, rtype, kind, "", stgroup.OK, -1, ""))

	if "pod_metrics" not in deep_get(iktconfig, "Pods#filter_resources", []):
		kind = ("PodMetrics", "metrics.k8s.io")
		obj = kh.get_ref_by_kind_name_namespace(kind, pod_name, pod_namespace)
		if obj is not None:
			rtype = resource_kind_to_rtype(kind)
			resource_tuple = ("", pod_name)
			resource_info.append(ResourceInfo(resource_tuple, obj, rtype, kind, "", stgroup.OK, -1, ""))

	# Lease?

	return resource_info

class InventoryInfo:
	def __init__(self, name, ips, kubernetes_roles, ansible_groups, status, status_group):
		self.name = name
		self.ref = name
		self.ips = ips
		self.kubernetes_roles = kubernetes_roles
		self.ansible_groups = ansible_groups
		self.status = status
		self.status_group = status_group
	def __repr__(self):
		return repr((self.name, self.ips, self.kubernetes_roles, self.ansible_groups, self.status, self.status_group))

def get_inventory_info(extra_vars = {}):
	global latest_info
	info = []
	nodes = []

	control_plane_node, control_plane_name = get_control_plane()

	vlist = kh.get_list_by_kind_namespace(("Node", ""), "")

	if vlist is not None:
		for node in vlist:
			roles = kh.get_node_roles(node)
			if "control-plane" in roles:
				continue
			nodes.append(deep_get(node, "metadata#name"))
		ansible_add_hosts(ANSIBLE_INVENTORY, nodes, group = "nodes")

	# This returns a YAML tree with the inventory of nodes
	inventory = ansible_get_inventory_dict()

	if deep_get(inventory, "all#hosts") == {}:
		# name, IP, isnode
		info.append(InventoryInfo("<none>", "", "", [], "", stgroup.UNKNOWN))
		return info

	# Before we ping the hosts their status is unknown
	for host in deep_get(inventory, "all#hosts", []):
		# We don't want to risk overwriting hostvars
		if deep_get(inventory, "all#hosts#%s" % host) is None:
			inventory["all"]["hosts"][host] = {}
		inventory["all"]["hosts"][host]["status"] = "UNKNOWN"
		inventory["all"]["hosts"][host]["status_group"] = stgroup.PENDING

	ping_hosts = deep_get(iktconfig, "Inventory#ping_hosts", "Lazy")

	# FIXME
	if ping_hosts == "Always" or (ping_hosts == "Lazy" and latest_info == "Inventory"):
		# ping all nodes and update their status accordingly
		retval, output = ansible_ping(ANSIBLE_INVENTORY)

		if output is None or len(output) == 0:
			sys.exit("Internal error; ansible -m ping failed with: %s" % (retval))

		# This needs to be improved; if there's a message we need to tell the difference
		# between "Permission denied" and "No route to host"
		for (host, status) in output:
			status_group = stgroup.UNKNOWN
			if status == "SUCCESS":
				status_group = stgroup.OK
			elif status == "FAILED!":
				status_group = stgroup.WARNING
			elif status == "UNREACHABLE!":
				status_group = stgroup.NOT_OK
			elif status == "COULD NOT RESOLVE":
				status_group = stgroup.CRIT
			elif status == "MISSING INTERPRETER?":
				status_group = stgroup.WARNING
			if host in deep_get(inventory, "all#hosts"):
				# We don't want to risk overwriting hostvars
				if deep_get(inventory, "all#hosts#%s" % host) is None:
					inventory["all"]["hosts"][host] = {}
				inventory["all"]["hosts"][host]["status"] = status
				inventory["all"]["hosts"][host]["status_group"] = status_group

	for host in deep_get(inventory, "all#hosts", []):
		# There's no point in trying gethostbyname() on hosts that cannot be resolved

		# For now we don't do anything with external IPs; we should
		name = host
		ips = []

		try:
			ips = [socket.gethostbyname(name)]
		except socket.gaierror as e:
			if str(e).endswith("Name or service not known"):
				pass
			elif str(e).endswith("Temporary failure in name resolution"):
				pass
			else:
				sys.exit("gethostbyname failed on %s; %s" % (name, e))
		except Exception as e:
			sys.exit("gethostbyname failed on %s; %s" % (name, e))

		# This only happens if you run the command on one of the hosts in the inventory
		if ips != [] and ips[0].startswith("127."):
			ips = []

			# Get all interfaces with global scope
			output = subprocess.check_output(["ip", "-o", "addr", "show", "scope", "global"]).decode("utf-8")
			for line in output.splitlines():
				tmp = re.match(r"^\d+: (.*?)\s+inet6? (.*?)/\d+ .*", line)
				if tmp is not None:
					iface = tmp[1]
					addr = tmp[2]
					# There are probably a lot of other things we should exclude,
					# but let's start with these
					if iface.startswith("docker") or iface.startswith("weave"):
						continue
					ips.append(addr)

		kubernetes_roles = []

		# We don't have this information if the network is down
		if host == control_plane_name:
			kubernetes_roles.append("control-plane")

		# We don't have this information if the network is down
		if host in nodes:
			kubernetes_roles.append("[node]")

			node = kh.get_ref_by_kind_name_namespace(("Node", ""), host, "")
			if node is not None:
				roles = kh.get_node_roles(node)

				for role in roles:
					if len(role) > 0 and role not in ["control-plane", "node"]:
						kubernetes_roles.append(role)
		ansible_groups = ansible_get_groups_by_host(inventory, host)
		status = deep_get(inventory, f"all#hosts#{name}#status", "UNKNOWN")
		status_group = deep_get(inventory, f"all#hosts#{name}#status_group", stgroup.UNKNOWN)
		if control_plane_name == "":
			kubernetes_roles = ["<unknown>"]
		info.append(InventoryInfo(name, ips, kubernetes_roles, ansible_groups, status, status_group))

	return info

class CSINodeDriverInfo:
	def __init__(self, name, ref, node_id, topology_keys, allocatable_count):
		self.name = name
		# The reference to the "true" resource object
		self.ref = ref
		self.node_id = node_id
		self.topology_keys = topology_keys
		self.allocatable_count = allocatable_count
	def __repr__(self):
		return repr((self.name, self.ref, self.node_id, self.topology_keys, self.allocatable_count))

def get_csi_node_driver_info(obj):
	info = []

	if obj is None or len(obj) == 0 or deep_get(obj, "spec#drivers") is None or len(deep_get(obj, "spec#drivers")) == 0:
		#name, ref, node_id, topology_keys, allocatable_count
		return [CSINodeDriverInfo("<none>", None, "", [], "")]

	for item in deep_get(obj, "spec#drivers", []):
		name = deep_get(item, "name")
		ref = item
		node_id = deep_get(item, "nodeID")
		topology_keys = deep_get(item, "topologyKeys", [])
		allocatable_count = str(deep_get(item, "allocatable#count", "<unbounded>"))

		info.append(CSINodeDriverInfo(name, ref, node_id, topology_keys, allocatable_count))

	return info

class NodeInfo:
	def __init__(self, name, ref, status, status_group, kubernetes_roles, age, kubelet_version, internal_ips, os, kernel, container_runtime, cpu, mem, taints):
		self.name = name
		# The reference to the "true" resource object
		self.ref = ref
		self.status = status
		self.status_group = status_group
		self.kubernetes_roles = kubernetes_roles
		self.age = age
		self.kubelet_version = kubelet_version
		self.internal_ips = internal_ips
		self.os = os
		self.kernel = kernel
		self.container_runtime = container_runtime
		self.cpu = cpu
		self.mem = mem
		self.taints = taints
	def __repr__(self):
		return repr((self.name, self.ref, self.status, self.status_group, self.age, self.kubernetes_roles, self.kubelet_version, self.internal_ips, self.os, self.kernel, self.container_runtime, self.cpu, self.mem, self.taints))

def get_node_status(node):
	status = "Unknown"
	status_group = stgroup.UNKNOWN
	taints = []
	full_taints = []

	for condition in deep_get(node, "status#conditions", []):
		if deep_get(condition, "type") == "Ready":
			condition_status = deep_get(condition, "status")
			if condition_status == "True":
				status = "Ready"
				status_group = stgroup.OK
			elif condition_status == "Unknown":
				status = "Unreachable"
				status_group = stgroup.NOT_OK
			else:
				status = "NotReady"
				status_group = stgroup.NOT_OK

	for nodetaint in deep_get(node, "spec#taints", []):
		key = deep_get(nodetaint, "key")
		if key == "node-role.kubernetes.io/master":
			key = "node-role.kubernetes.io/control-plane"
		effect = deep_get(nodetaint, "effect")
		full_taints.append((key, effect))

		# Control Plane having scheduling disabled
		# is expected behaviour and does not need
		# any form of highlighting
		if deep_get(nodetaint, "effect") == "NoSchedule":
			if key == "node-role.kubernetes.io/control-plane":
				taints.append(("control-plane", effect))
				continue

			if key.startswith("node.kubernetes.io/"):
				key = key[len("node.kubernetes.io/"):]

			taints.append((key, effect))

			# If status is already "worse" than OK,
			# we don't override it.
			# Scheduling being disabled is not an error,
			# but it's worth highlighting
			if status_group == stgroup.OK:
				status_group = stgroup.ADMIN
		else:
			if key.startswith("node.kubernetes.io/"):
				key = key[len("node.kubernetes.io/"):]

			taints.append((key, effect))

	return status, status_group, taints, full_taints

def get_node_addresses(name, addresses):
	iips = []
	eips = []

	for address in addresses:
		address_type = deep_get(address, "type")
		address_address = deep_get(address, "address")
		if address_type == "InternalIP":
			iips.append(address_address)
		elif address_type == "ExternalIP":
			eips.append(address_address)
		# handle external IPs too
		elif address_type == "Hostname":
			if name is None:
				name = address_address
			elif name != address_address:
				sys.exit("We need to handle multiple hostnames somehow!")
		else:
			continue

	if iips == []:
		iips = ["<unset>"]
		eips = ["<unset>"]
	if eips == []:
		eips = ["<none>"]
	if name is None:
		name = "<unset>"

	return name, iips, eips

def get_node_info(vlist, extra_vars = {}):
	info = []

	if vlist == []:
		return []

	for obj in vlist:
		# for now we don't do anything with external IPs; we should
		name, internal_ips, external_ips = get_node_addresses(deep_get(obj, "metadata#name"), deep_get(obj, "status#addresses"))
		ref = obj
		kubernetes_roles = kh.get_node_roles(obj)
		timestamp = timestamp_to_datetime(deep_get(obj, "metadata#creationTimestamp"))
		age = get_since(timestamp)
		cpu = (deep_get(obj, "status#allocatable")["cpu"], deep_get(obj, "status#capacity")["cpu"])
		# Strip Ki suffix
		mem = (deep_get(obj, "status#allocatable")["memory"][:-2], deep_get(obj, "status#capacity")["memory"][:-2])
		status, status_group, taints, full_taints = get_node_status(obj)
		kubelet_version = deep_get(obj, "status#nodeInfo#kubeletVersion")
		container_runtime = deep_get(obj, "status#nodeInfo#containerRuntimeVersion")
		os = deep_get(obj, "status#nodeInfo#osImage")
		kernel = deep_get(obj, "status#nodeInfo#kernelVersion")

		info.append(NodeInfo(name, ref, status, status_group, kubernetes_roles, age, kubelet_version, internal_ips, os, kernel, container_runtime, cpu, mem, taints))

	return info

# To make the failure case easier, return both the ref and the name of the control plane
def get_control_plane():
	vlist = kh.get_list_by_kind_namespace(("Node", ""), "")
	control_planes = []

	if vlist is None or len(vlist) == 0:
		return None, ""

	# Find control planes; but for now only return the first match
	for obj in vlist:
		labels = deep_get(obj, "metadata#labels", {})
		if "node-role.kubernetes.io/control-plane" in labels or "node-role.kubernetes.io/master" in labels:
			control_planes.append((obj, deep_get(obj, "metadata#name")))

	# If we have exactly one node, assume that it is the control plane even if it lacks that label
	if len(control_planes) == 0 and len(vlist) == 1:
		control_planes.append((vlist[0], deep_get(vlist[0], "metadata#name")))

	if len(control_planes) > 1:
		iktprint([(f"Warning: ", "warning"), ("multiple control planes not supported yet, found multiple; returning first entry:", "default")], stderr = True)
		for control_plane in control_planes:
			print(f"  {control_plane[1]}")

	return control_planes[0][0], control_planes[0][1]

class PodInfo:
	def __init__(self, namespace, name, ref, status, status_group, node, pod_ip, age, restarts, controller, tolerations, containers):
		self.namespace = namespace
		self.name = name
		# The reference to the "true" resource object
		self.ref = ref
		self.status = status
		self.status_group = status_group
		self.node = node
		self.pod_ip = pod_ip
		self.age = age
		self.restarts = restarts
		self.controller = controller
		self.tolerations = tolerations
		self.containers = containers
	def __repr__(self):
		return repr((self.namespace, self.name, self.ref, self.status, self.status_group, self.node, self.pod_ip, self.age, self.restarts, self.controller, self.tolerations, self.containers))

def get_pod_restarts_total(pod):
	restarts = 0

	for status in deep_get(pod, "status#initContainerStatuses", []):
		restarts += deep_get(status, "restartCount", 0)

	for status in deep_get(pod, "status#containerStatuses", []):
		restarts += deep_get(status, "restartCount", 0)

	return restarts

def get_pod_status(pod):
	phase = deep_get(pod, "status#phase")

	if phase == "Pending":
		status = phase
		status_group = stgroup.PENDING

		# Any containers in ContainerCreating or similar?
		for condition in deep_get(pod, "status#conditions", []):
			condition_type = deep_get(condition, "type")
			condition_status = deep_get(condition, "status")
			reason = deep_get(condition, "reason", "")

			if condition_type == "PodScheduled" and condition_status == "False" and reason == "Unschedulable":
				status = reason
				status_group = stgroup.NOT_OK
				break
			elif condition_type == "ContainersReady" and condition_status == "False":
				for container in deep_get(pod, "status#initContainerStatuses", []):
					if deep_get(container, "ready") == False:
						reason = deep_get(container, "state#waiting#reason", "").rstrip()
						if reason is not None and len(reason) > 0:
							return reason, status_group
				for container in deep_get(pod, "status#containerStatuses", []):
					if deep_get(container, "ready") == False:
						reason = deep_get(container, "state#waiting#reason", "").rstrip()
						if reason is not None and len(reason) > 0:
							return reason, status_group

		return status, status_group
	elif phase == "Running":
		status = "Running"
		status_group = stgroup.OK
		last_state = ""
		exit_code = 0

		# Any container failures?
		for condition in deep_get(pod, "status#conditions", []):
			condition_type = deep_get(condition, "type")
			condition_status = deep_get(condition, "status")
			ready = True

			if condition_type == "Ready" and condition_status == "False":
				status_group = stgroup.NOT_OK
				status = "NotReady"
				# Can we get more info? Is the host available?
				node_name = deep_get(pod, "spec#nodeName")
				node = kh.get_ref_by_kind_name_namespace(("Node", ""), node_name, None)
				node_status = get_node_status(node)
				if node_status[0] == "Unreachable":
					status = "NodeUnreachable"
				ready = False
			elif condition_type == "ContainersReady" and condition_status == "False":
				status_group = stgroup.NOT_OK

				for container in deep_get(pod, "status#initContainerStatuses", []):
					kind = "InitContainer"
					status, status_group, restarts, message = get_container_status(deep_get(pod, "status#initContainerStatuses"), deep_get(container, "name"), kind)
					# If we have a failed container,
					# break here
					if status_group == stgroup.NOT_OK:
						break

				for container in deep_get(pod, "status#containerStatuses", []):
					kind = "Container"
					status, status_group, restarts, message = get_container_status(deep_get(pod, "status#containerStatuses"), deep_get(container, "name"), kind)
					# If we have a failed container,
					# break here
					if status_group == stgroup.NOT_OK:
						break

		return status, status_group
	elif phase == "Failed":
		# Failed
		status_group = stgroup.NOT_OK
		status = deep_get(pod, "status#reason", phase).rstrip()
		return status, status_group
	else:
		# Succeeded
		status_group = stgroup.DONE
		return phase, status_group

def get_pod_affinity(obj, **kwargs):
	affinities = []

	for affinity in deep_get(obj, "spec#affinity", []):
		atype = affinity
		for policy in deep_get(obj, f"spec#affinity#{atype}", ""):
			tmp = re.match(r"^(ignored|preferred|required)DuringScheduling(Ignored|Preferred|Required)DuringExecution$", policy)
			if tmp is None:
				scheduling = "Unknown"
				execution = "Unknown"
			else:
				scheduling = tmp[1].capitalize()
				execution = tmp[2]

			selectors = ""
			for item in deep_get(obj, f"spec#affinity#{atype}#{policy}", []):
				topology = ""
				if type(item) == dict:
					items = [item]
				elif type(item) == str:
					items = deep_get(obj, f"spec#affinity#{atype}#{policy}#{item}", [])

				for selector in items:
					weight = deep_get(selector, f"weight", "")
					if type(weight) == int:
						weight = f"/{weight}"
					topology = deep_get(selector, f"topologyKey", "")
					# We're combining a few different policies, so the expressions can be in various places; not simultaneously though
					selectors += make_set_expression(deep_get(selector, "labelSelector#matchExpressions", {}))
					selectors += make_set_expression(deep_get(selector, "labelSelector#matchFields", {}))
					selectors += make_set_expression(deep_get(selector, "preference#matchExpressions", {}))
					selectors += make_set_expression(deep_get(selector, "preference#matchFields", {}))
					selectors += make_set_expression(deep_get(selector, "matchExpressions", {}))
					selectors += make_set_expression(deep_get(selector, "matchFields", {}))
					affinities.append((atype, f"{scheduling}{weight}", execution, selectors, topology))

	return affinities

def get_pod_tolerations(obj, **kwargs):
	tolerations = []

	for toleration in deep_get(obj, "spec#tolerations", []):
		status_group = stgroup.OK

		effect = deep_get(toleration, "effect", "All")
		key = deep_get(toleration, "key", "All")
		operator = deep_get(toleration, "operator", "Equal")

		# According to spec only "Exists"
		# is valid in combination with "All"
		if key == "All" and operator != "Exists":
			status_group = stgroup.NOT_OK

		# Eviction timeout
		toleration_seconds = deep_get(toleration, "tolerationSeconds")
		if toleration_seconds is None:
			timeout = "Never"
		elif toleration_seconds <= 0:
			timeout = "Immediately"
		else:
			timeout = str(toleration_seconds)

		value = deep_get(toleration, "value", "")

		# According to spec only an empty value
		# is valid in combination with "Exists"
		if operator == "Exists" and value != "":
			status_group = stgroup.NOT_OK

		tolerations.append((key, operator, value, effect, timeout))

	return tolerations

def set_cluster_context(uip, name):
	kh.set_context(name = name)

def get_security_context(obj, **kwargs):
	security_policies = []

	tmp = [
		("Run as User", deep_get_with_fallback(obj, ["spec#securityContext#runAsUser", "spec#template#spec#securityContext#runAsUser"])),
		("Run as non-Root", deep_get_with_fallback(obj, ["spec#securityContext#runAsNonRoot", "spec#template#spec#securityContext#runAsNonRoot"])),
		("Run as Group", deep_get_with_fallback(obj, ["spec#securityContext#runAsGroup", "spec#template#spec#securityContext#runAsGroup"])),
		("FS Group", deep_get_with_fallback(obj, ["spec#securityContext#fsGroup", "spec#template#spec#securityContext#fsGroup"])),
		("FS Group-change Policy", deep_get_with_fallback(obj, ["spec#securityContext#fsGroupChangePolicy", "spec#template#spec#securityContext#fsGroupChangePolicy"])),
		("Allow Privilege Escalation", deep_get_with_fallback(obj, ["spec#securityContext#allowPrivilegeEscalation", "spec#template#spec#securityContext#allowPrivilegeEscalation"])),
		("Capabilities",deep_get_with_fallback(obj, ["spec#securityContext#capabilities", "spec#template#spec#securityContext#capabilities"])),
		("Privileged", deep_get_with_fallback(obj, ["spec#securityContext#privileged", "spec#template#spec#securityContext#privileged"])),
		("Proc Mount", deep_get_with_fallback(obj, ["spec#securityContext#procMount", "spec#template#spec#securityContext#procMount"])),
		("Read-only Root Filesystem", deep_get_with_fallback(obj, ["spec#securityContext#readOnlyRootFilesystem", "spec#template#spec#securityContext#readOnlyRootFilesystem"])),
		("SELinux Options", deep_get_with_fallback(obj, ["spec#securityContext#seLinuxOptions", "spec#template#spec#securityContext#seLinuxOptions"])),
		("Seccomp Profile", deep_get_with_fallback(obj, ["spec#securityContext#seccompProfile", "spec#template#spec#securityContext#seccompProfile"])),
		("Windows Options", deep_get_with_fallback(obj, ["spec#securityContext#windowsOptions", "spec#template#spec#securityContext#windowsOptions"])),
	]

	for policy in tmp:
		if policy[1] is not None:
			security_policies.append((policy[0], str(policy[1])))

	return security_policies

def get_allowed_ips(obj, **kwargs):
	allowed_ips = []

	for addr in deep_get(obj, "spec#allowedIPs"):
		if "/" in addr:
			tmp = re.match(r"^(\d+\.\d+\.\d+\.\d+)\/(\d+)", addr)
			if tmp is None:
				raise Exception(f"Could not parse {addr} as an address/address mask")
			ip = tmp[1]
			mask = tmp[2]
			allowed_ips.append((widgetlineattrs.NORMAL, [(f"{ip}", ("windowwidget", "default")), ("/", ("windowwidget", "dim")), (f"{mask}", ("windowwidget", "default"))]))
		else:
			allowed_ips.append((widgetlineattrs.NORMAL, [(f"{addr}", ("windowwidget", "default"))]))

	return allowed_ips

def get_volume_properties(obj, **kwargs):
	volume_properties = []

	# First find out what kind of volume we're dealing with
	pv_type = get_pv_type(obj)
	if pv_type is None:
		return volume_properties

	properties = deep_get(known_pv_types, f"{pv_type}#properties", {})
	for volume_property in properties:
		default = deep_get(properties, f"{volume_property}#default", "")
		path = deep_get(properties, f"{volume_property}#path", "")
		value = deep_get(obj, f"spec#{pv_type}#{path}", default)
		if type(value) == list:
			value = ",".join(value)
		elif type(value) == dict:
			value = ",".join(f"{key}:{val}" for (key, val) in value.items())
		elif type(value) == int:
			value = str(int)
		volume_properties.append((volume_property, value))

	return volume_properties

def get_endpoint_slices(obj, **kwargs):
	svcname = deep_get(obj, "metadata#name")
	svcnamespace = deep_get(obj, "metadata#namespace")
	# We need to find all Endpoint Slices in the same namespace as the service that have this service
	# as its controller
	vlist = kh.get_list_by_kind_namespace(("EndpointSlice", "discovery.k8s.io"), svcnamespace, label_selector = f"kubernetes.io/service-name={svcname}")
	tmp = []
	for item in vlist:
		epsnamespace = deep_get(item, "metadata#namespace")
		epsname = deep_get(item, "metadata#name")
		tmp.append((epsnamespace, epsname))
	return tmp

def get_key_value(obj, **kwargs):
	vlist = []
	if "path" in kwargs:
		path = deep_get(kwargs, "path", "")
		d = deep_get(kwargs, "path", {})

		for key in d:
			vlist.append((key, d[key]))

	return vlist

def get_list_fields(obj, **kwargs):
	vlist = []

	if "path" in kwargs and "fields" in kwargs:
		path = deep_get(kwargs, "path")
		fields = deep_get(kwargs, "fields", [])
		for item in deep_get(obj, path, []):
			tmp = []
			for field in fields:
				tmp.append(str(deep_get(item, field, "")))
			vlist.append(tmp)

	return vlist

def get_alertmanagers(obj, **kwargs):
	alertmanagers = []

	for alertmanager in deep_get(obj, "spec#alerting#alertmanagers", []):
		name = deep_get(alertmanager, "name", "")
		namespace = deep_get(alertmanager, "namespace", "")
		port = deep_get(alertmanager, "port", "")
		alertmanagers.append((name, namespace, port))

	return alertmanagers

def get_resources(obj, **kwargs):
	resources = []

	for limit in list(deep_get(obj, "spec#resources#limits", {})):
		if limit == "cpu":
			resources.append(("CPU", "Limit", deep_get(obj, "spec#resources#limits#cpu")))
		elif limit == "memory":
			resources.append(("CPU", "Limit", deep_get(obj, "spec#resources#limits#memory")))
		elif limit.startswith("hugepages-"):
			resources.append((f"H{limit[1:]}", "Limit", deep_get(obj, "spec#resources#limits#" + limit)))

	for request in list(deep_get(obj, "spec#resources#requests", {})):
		if request == "cpu":
			resources.append(("CPU", "Limit", deep_get(obj, "spec#resources#requests#cpu")))
		elif request == "memory":
			resources.append(("CPU", "Limit", deep_get(obj, "spec#resources#requests#memory")))
		elif request.startswith("hugepages-"):
			resources.append((f"H{request[1:]}", "Limit", deep_get(obj, "spec#resources#requests#" + request)))

	return resources

def get_containers(containers = [], container_statuses = []):
	container_dict = {}

	container_list = []

	for container in containers:
		container_name = deep_get(container, "name")
		container_image = deep_get(container, "image")
		image_version = kh.get_image_version(container_image)
		container_dict[container_name] = image_version

	for container in container_statuses:
		container_name = deep_get(container, "name")
		container_image = deep_get(container, "image")
		if container_dict[container_name] == "<undefined>":
			image_version = kh.get_image_version(container_image, "<undefined>")
			container_list.append((container_name, image_version))
		else:
			container_list.append((container_name, container_dict[container_name]))

	return container_list

def get_workload_labels(obj, **kwargs):
	return curses_helper.get_labels(deep_get(obj, "spec#workloadLabels"))

def get_pod_info_with_kind(vlist):
	return get_pod_info(vlist, extra_vars = {"show_kind": "mixed"})

def get_events_by_kind_name_namespace(kind, name, namespace):
	events = []
	vlist = kh.get_list_by_kind_namespace(("Event", ""), "")
	for obj in vlist:
		__involved_kind = deep_get_with_fallback(obj, ["regarding#kind", "involvedObject#kind"])
		__involved_api_version = deep_get_with_fallback(obj, ["regarding#apiVersion", "involvedObject#apiVersion"])
		involved_kind = kh.kind_api_version_to_kind(__involved_kind, __involved_api_version)
		involved_name = deep_get_with_fallback(obj, ["regarding#name", "involvedObject#name"])
		ev_name = deep_get(obj, "metadata#name")
		ev_namespace = deep_get(obj, "metadata#namespace", "")
		last_seen = datetime_to_timestamp(__get_timestamp_with_fallback(obj, ["series#lastObservedTime", "deprecatedLastTimestamp", "lastTimestamp", "eventTime", "deprecatedFirstTimestamp", "firstTimestamp"]))
		status = deep_get(obj, "type", "")
		reason = deep_get(obj, "reason", "").replace("\\\"", "“").replace("\n", "\\n").rstrip()
		src_component = deep_get_with_fallback(obj, ["reportingController", "deprecatedSource#component", "source#component"], "")
		src_host = deep_get_with_fallback(obj, ["reportingInstance", "deprecatedSource#host", "source#host"], "")
		if len(src_component) == 0:
			source = src_host
		elif len(src_host) == 0:
			source = src_component
		else:
			source = f"{src_host}/{src_component}"
		first_seen = datetime_to_timestamp(__get_timestamp_with_fallback(obj, ["eventTime", "deprecatedFirstTimestamp", "firstTimestamp"]))

		count = deep_get_with_fallback(obj, ["series#count", "deprecatedCount", "count"], "")
		if count is None:
			count = ""
		else:
			count = str(count)
		message = deep_get(obj, "message", "").replace("\\\"", "“").replace("\n", "\\n").rstrip()
		if kind == involved_kind and name == involved_name and ev_namespace == namespace:
			event = (ev_namespace, ev_name, last_seen, status, reason, source, first_seen, count, message)
			events.append(event)
	return events

def get_pod_info(vlist, extra_vars = {"show_kind": "", "show_evicted": True}, filters = []):
	info = []

	if vlist == []:
		return []

	for obj in vlist:
		skip = False

		# Sadly field_labels don't support all fields we might want to filter on,
		# so we have to complicate things a bit
		if len(filters) > 0:
			for key, value in filters:
				ovalue = deep_get(obj, key, None)
				if ovalue is None or ovalue != value:
					skip = True
					break

		if skip == True:
			continue

		phase = deep_get(obj, "status#phase")
		reason = deep_get(obj, "status#reason", "").rstrip()

		if deep_get(extra_vars, "show_evicted", False) == False and phase == "Failed" and reason == "Evicted":
			continue

		namespace = deep_get(obj, "metadata#namespace")
		name = deep_get(obj, "metadata#name")
		ref = obj
		nodename = deep_get(obj, "spec#nodeName", "<none>")

		timestamp = timestamp_to_datetime(deep_get(obj, "metadata#creationTimestamp"))
		age = get_since(timestamp)

		owr = deep_get(obj, "metadata#ownerReferences", [])
		controller = get_controller_from_owner_references(owr)
		show_kind = deep_get(extra_vars, "show_kind", "").lower()
		controller = format_controller(controller, show_kind)

		pod_ip = deep_get(obj, "status#podIP", "<unset>")
		status, status_group = get_pod_status(obj)
		pod_restarts = get_pod_restarts_total(obj)
		tolerations = get_pod_tolerations(obj)
		containers = get_containers(deep_get(obj, "spec#initContainers", []), deep_get(obj, "status#initContainerStatuses", []))
		containers += get_containers(deep_get(obj, "spec#containers", []), deep_get(obj, "status#containerStatuses", []))

		info.append(PodInfo(namespace, name, ref, status, status_group, nodename, pod_ip, age, pod_restarts, controller, tolerations, containers))

	return info

def generate_listheader(uip, headerpad, field_list, is_taggable = False):
	headerarray = []
	first = True

	# Is the list taggable?
	if is_taggable:
		headerarray.append(("separators", "tag"))

	for field in field_list:
		generator = field_list[field].get("generator")
		if generator is None:
			continue

		theme = get_theme_ref()
		if uip.get_sortcolumn() == field:
			if uip.reversible == False:
				sort_direction_char = theme["boxdrawing"]["arrownone"]
			elif uip.sortorder_reverse:
				sort_direction_char = theme["boxdrawing"]["arrowup"]
			else:
				sort_direction_char = theme["boxdrawing"]["arrowdown"]
		else:
			sort_direction_char = theme["boxdrawing"]["arrownone"]

		# We always want this much padding between the headers,
		# except if this is the first header
		#
		# Note that we need to subtract the width of sort direction char
		# from the width of pad; this only works if len(pad) > 0
		if first == False:
			headerarray.append(("".ljust(len(themearray_extract_string("pad", context = "separators")) - len(sort_direction_char)), curses.A_NORMAL))

		# This tells the length of the alignment of the header
		fieldlen = deep_get(field_list, f"{field}#fieldlen")
		header = deep_get(field_list, f"{field}#header")
		ralign = deep_get(field_list, f"{field}#ralign", False)

		# We cannot use ljust/rjust on the string,
		# because we want the string and arrow in different colours,
		# so just prepend/append whitespace instead
		if ralign == True:
			headerarray.append(("".ljust(fieldlen - len(header)), curses.A_NORMAL))
		headerarray.append((header, ("main", "listheader", uip.get_sortcolumn() == field)))
		headerarray.append((sort_direction_char, ("main", "listheader_arrows")))
		if ralign == False:
			headerarray.append(("".ljust(fieldlen - len(header)), curses.A_NORMAL))
		first = False

	# We've processed all fields, time to output the header
	uip.addthemearray(headerpad, headerarray, y = 0, x = 0)

def generate_list(uip, listpad, data, field_list, ypos, is_selected, is_taggable = False, is_tagged = False):
	first = True

	i = 0

	for field in field_list:
		i += 1

		attribute = curses.A_NORMAL

		if is_taggable:
			tagprefixlen = len(themearray_extract_string("tag", context = "separators"))

			if is_tagged:
				tagprefix = [("separators", "tag")]
			else:
				tagprefix = [("".ljust(tagprefixlen), ("types", "generic"))]
		else:
			tagprefix = ""
			tagprefixlen = 0

		generator = field_list[field].get("generator")
		if generator is None:
			continue

		fieldlen = field_list[field]["fieldlen"]
		if i < len(field_list):
			fpad = len(themearray_extract_string("pad", context = "separators"))
		else:
			fpad = 0
		ralign = field_list[field].get("ralign", False)
		item_separator = field_list[field].get("item_separator", ("separators", "list"))
		field_separators = field_list[field].get("field_separators", [("separators", "field")])
		field_colors = field_list[field].get("field_colors", [("types", "field")])
		ellipsise = field_list[field].get("ellipsise", -1)
		ellipsis = field_list[field].get("ellipsis", ("separators", "ellipsis"))
		field_prefixes = field_list[field].get("field_prefixes", None)
		field_suffixes = field_list[field].get("field_suffixes", None)

		if generator in [generator_list, generator_list_with_status]:
			tmp = generator(data, field, fieldlen, fpad, ralign, is_selected, item_separator = item_separator, field_separators = field_separators, field_colors = field_colors, ellipsise = ellipsise, ellipsis = ellipsis, field_prefixes = field_prefixes, field_suffixes = field_suffixes)
		else:
			tmp = generator(data, field, fieldlen, fpad, ralign, is_selected)

		pos = field_list[field]["pos"]

		if first and is_taggable:
			uip.addthemearray(listpad, tagprefix, y = ypos, x = pos)
			first = False

		uip.addthemearray(listpad, tmp, y = ypos, x = pos + tagprefixlen)

field_templates = {
	"access_modes": {
		"header": "Access Modes:",
		"path": "spec#accessModes",
		"datagetter": datagetter_access_modes,
		"generator": generator_list,
	},
	"action": {
		"header": "Action:",
	},
	"active": {
		"header": "Active:",
		"datagetter": datagetter_list_len,
		"generator": generator_numerical,
		"ralign": True,
	},
	"addresses": {
		# [address, ...]
		"header": "Addresses:",
		"generator": generator_list,
	},
	"addresstype": {
		"header": "Type:",
		"path": "addressType",
	},
	"age": {
		"header": "Age:",
		"path": "metadata#creationTimestamp",
		"datagetter": datagetter_age,
		"generator": generator_age,
		"ralign": True,
	},
	"alertrecord": {
		"header": "Alert/Record:",
	},
	"allocatable_count": {
		"header": "Allocatable (Count):",
		"generator": generator_numerical,
		"ralign": True,
	},
	"allowed_disruptions": {
		"header": "Allowed Disruptions:",
		"path": "status#disruptionsAllowed",
		"generator": generator_numerical,
		"ralign": True,
	},
	"allow_volume_expansion": {
		"header": "Allow Volume Expansion:",
		"path": "allowVolumeExpansion",
		"default": False,
	},
	"ansible_groups": {
		# [group, ...]
		"header": "Ansible Groups:",
		"generator": generator_list,
	},
	"api_group": {
		"header": "API Group:",
	},
	"api_group_crd": {
		"header": "API Group:",
		"path": "spec#group",
	},
	"api_service": {
		# (namespace, name, port)
		"header": "Service",
		"path": "service",
		"datagetter": datagetter_api_service,
		"generator": generator_list,
		"field_colors": [("types", "namespace"), ("types", "generic"), ("types", "port")],
		"field_separators": [("separators", "namespace"), ("separators", "port")],
	},
	"api_service_status_message": {
		"header": "Status:",
		"path": "status_message",
		"datagetter": datagetter_api_service,
	},
	"assured_concurrency_shares": {
		"header": "Assured Concurrency Shares:",
		"path": "spec#limited#assuredConcurrencyShares",
		"generator": generator_numerical,
		"ralign": True,
	},
	"attach_required": {
		"header": "Attach Required:",
		"path": "spec#attachRequired",
	},
	"authinfo": {
		"header": "Authinfo:",
	},
	"available": {
		"header": "Available:",
		"path": "available",
		"datagetter": datagetter_api_service,
		"generator": generator_status,
	},
	"available_replicas": {
		"header": "Available:",
		"path": "status#availableReplicas",
		"generator": generator_numerical,
		"ralign": True,
	},
	"available_replicas_ds": {
		"header": "Available:",
		"path": "status#numberAvailable",
		"generator": generator_numerical,
		"ralign": True,
	},
	"backends": {
		# [(service_name, service_port), ...]
		"header": "Backends:",
		"generator": generator_list,
		"field_separators": [("separators", "port")],
	},
	"bearer_token_file": {
		"header": "Bearer Token File:",
	},
	"bind": {
		"header": "Bind:",
	},
	"capabilities": {
		# [capability, ...]
		"header": "Capabilities:",
		"path": "spec#allowedCapabilities",
		"generator": generator_list,
	},
	"capacity": {
		"header": "Capacity:",
		# This should get either PV capacity or PVC capacity
		"path": ["spec#capacity#storage", "status#capacity#storage"],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_list,
		"ralign": True,
	},
	"capture_mode": {
		"header": "Capture Mode:",
	},
	"catalogue": {
		"header": "Catalogue",
		"path": "status#catalogSourceDisplayName",
	},
	"claim": {
		# (namespace, name)
		"header": "Claim:",
		"path": [("spec#claimRef#namespace", ""), ("spec#claimRef#name", "")],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_list,
		"field_colors": [("types", "namespace"), ("types", "generic")],
		"field_separators": [("separators", "namespace")],
	},
	"cluster": {
		"header": "Cluster:",
	},
	"cluster_ip": {
		"header": "Cluster-IP:",
		"path": "spec#clusterIP",
	},
	"completions": {
		"header": "Completions:",
		"path": [("status#succeeded", 0), "spec#completions"],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_list,
		"field_separators": [("separators", "fraction")],
		"ralign": True,
	},
	"completion_time": {
		"header": "Completion Time:",
		"path": ["completion"],
		"datagetter": datagetter_start_completion_duration,
		"generator": generator_timestamp,
	},
	"concurrency_policy": {
		"header": "Concurrency:",
		"path": "spec#concurrencyPolicy",
	},
	"condition": {
		"header": "Condition:",
		"generator": generator_list,
		"field_prefixes": [
			[("separators", "no_pad")],
			[("separators", "set_start")],
			[("separators", "no_pad")],
			[("separators", "set_start")],
		],
		"field_suffixes": [
			[("separators", "no_pad")],
			[("separators", "set_end")],
			[("separators", "no_pad")],
			[("separators", "set_end")],
		],
		# equals, logical and, not equals
		# key=[...] ∧ key≠[...], ...
		"field_separators": [("separators", "equals"), ("separators", "logical_and"), ("separators", "not_equals")],
		# Logical or
		"item_separator": ("separators", "logical_or"),
	},
	"condition_ready": {
		"header": "Ready:",
		"datagetter": datagetter_condition_ready,
	},
	"configmap": {
		"header": "Data:",
	},
	"container_runtime": {
		"header": "Container-Runtime:",
		"path": "status#nodeInfo#containerRuntimeVersion",
	},
	"container_count": {
		"header": "# of Containers:",
		"path": "containers",
		"datagetter": datagetter_count,
		"generator": generator_numerical,
		"ralign": True,
	},
	"containers": {
		# [(name, version), ...]
		"header": "Containers:",
		"path": [("spec#initContainers", "status#initContainerStatuses"), ("spec#containers", "status#containerStatuses")],
		"datagetter": datagetter_containers,
		"generator": generator_list,
		"field_colors": [("types", "generic"), ("types", "version")],
		"field_separators": [("separators", "version")],
	},
	"containers_jobtemplate": {
		# [(name, version), ...]
		"header": "Containers:",
		"path": [("spec#jobTemplate#spec#template#initContainers", "spec#jobTemplate#spec#template#spec#initContainers"), ("spec#jobTemplate#spec#template#spec#containers", "spec#jobTemplate#spec#template#spec#containers")],
		"datagetter": datagetter_containers,
		"generator": generator_list,
		"field_colors": [("types", "generic"), ("types", "version")],
		"field_separators": [("separators", "version")],
	},
	"containers_template": {
		# [(name, version), ...]
		"header": "Containers:",
		"path": [("spec#template#spec#initContainers", "spec#template#spec#initContainers"), ("spec#template#spec#containers", "spec#template#spec#containers")],
		"datagetter": datagetter_containers,
		"generator": generator_list,
		"field_colors": [("types", "generic"), ("types", "version")],
		"field_separators": [("separators", "version")],
	},
	"container_type": {
		"header": "Type:",
	},
	"controller": {
		# (kind, name)
		"header": "Controller:",
		"path": ("metadata#ownerReferences", "mixed"),
		"datagetter": datagetter_controller,
		"generator": generator_list,
		"field_colors": [("types", "kind"), ("types", "generic")],
	},
	"controller_conditions": {
		"header": "Status:",
		"path": "controllerConditions",
		"datagetter": datagetter_status_latest_condition,
		"generator": generator_status,
	},
	"count": {
		"header": "Count:",
		"generator": generator_numerical,
		"path": "count",
		"ralign": True,
	},
	"cpu": {
		"header": "CPUs:",
		"path": ["status#allocatable#cpu", "status#capacity#cpu"],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_total,
		"ralign": True,
	},
	"cpu_millicores": {
		"header": "CPU (millicores):",
		"path": "cpu_millicores",
		"ralign": True,
	},
	"cpu_pod_metrics_total": {
		"header": "CPU (millicores):",
		"path": "usage#cpu",
		"datagetter": datagetter_pod_metrics_cpu,
		"generator": generator_mem_single,
		"ralign": True,
	},
	"created_at": {
		"header": "Created At:",
		"path": "metadata#creationTimestamp",
		"datagetter": datagetter_timestamp,
		"generator": generator_timestamp,
	},
	"csr_conditions": {
		# [condition, ...]
		"header": "Conditions:",
		"generator": generator_list,
	},
	"current": {
		"header": "Current:",
	},
	"current_replicas": {
		"header": "Current:",
		"path": "status#replicas",
		"generator": generator_numerical,
		"ralign": True,
	},
	"current_replicas_ds": {
		"header": "Current:",
		"path": "status#currentNumberScheduled",
		"generator": generator_numerical,
		"ralign": True,
	},
	"current_replicas_tuple": {
		# (current, desired)
		"header": "Replicas:",
		"path": ["status#currentReplicas", "status#desiredReplicas"],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_total,
		"ralign": True,
	},
	"danger_count": {
		"header": "Danger #:",
		"generator": generator_numerical_range,
		"ranges": [(0, 1, ("types", "watermark_low")), (1, sys.maxsize, ("types", "watermark_high"))],
		"path": "report#summary#dangerCount",
		"ralign": True,
	},
	"data": {
		"header": "# of Data:",
		"path": "data",
		"datagetter": datagetter_count,
		"generator": generator_numerical,
		"ralign": True,
	},
	"default_endpoint": {
		"header": "Default Endpoint:",
	},
	"default_request": {
		"header": "Default Request:",
	},
	"default_limit": {
		"header": "Default Limit:",
	},
	"deprecated_api": {
		"header": "Deprecated API:",
		"path": [["resource", "group", "version"], "tuple"],
		"datagetter": datagetter_deprecated_api,
		"generator": generator_list,
		"field_colors": [("types", "kind"), ("types", "api_group")],
		"field_separators": [("separators", "kind_apigroup"), ("separators", "kind")],
	},
	"deprecated_removed_release": {
		"header": "Planned Removal:",
		"path": [["removed_release"], "str"],
		"datagetter": datagetter_metrics,
	},
	"description": {
		"header": "Description:",
		"path": "spec#descriptor#description",
		"datagetter": datagetter_substitute_quotationmarks,
	},
	"desired_replicas": {
		"header": "Desired:",
		"path": "spec#replicas",
		"generator": generator_numerical,
		"ralign": True,
	},
	"desired_replicas_ds": {
		"header": "Desired:",
		"path": "status#desiredNumberScheduled",
		"generator": generator_numerical,
		"ralign": True,
	},
	"destinations": {
		# (host, subset, port)
		"header": "Destinations:",
		"generator": generator_list,
		"field_colors": [("types", "host"), ("types", "subset"), ("types", "port")],
		"field_separators": [("separators", "list_start"), ("separators", "list_end_colon")],
	},
	"device": {
		"header": "Device:",
		"path": "device",
	},
	"disk_usage_partition": {
		"header": "Disk Usage:",
		"path": ["partition_size_used", "partition_size_total"],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_list,
		"ralign": True,
	},
	"display_name": {
		"header": "Display Name:",
		"path": "spec#displayName",
	},
	"distinguisher_method": {
		"header": "Distinguisher Method:",
		"path": "spec#distinguisherMethod#type",
	},
	"docker_reference": {
		"header": "Docker Reference:",
		"path": "dockerImageReference",
	},
	"docker_reference_image": {
		"header": "Docker Reference:",
		"path": "image#dockerImageReference",
	},
	"docker_repo": {
		"header": "Docker Repo:",
		"path": "status#dockerImageRepository",
	},
	"drivers": {
		"header": "Drivers:",
		"path": "spec#drivers",
		"datagetter": datagetter_count,
		"generator": generator_numerical,
		"ralign": True,
	},
	"duration": {
		"header": "Duration:",
		"path": ["duration"],
		"datagetter": datagetter_start_completion_duration,
		"generator": generator_age,
		"ralign": True,
	},
	"egress_enforcement": {
		"header": "Egress Enforcement:",
	},
	"endpoint_id": {
		"header": "Endpoint ID:",
	},
	"endpoint_state": {
		"header": "Endpoint State:",
	},
	"endpoints": {
		"header": "Endpoints:",
		"path": "subsets",
		"datagetter": datagetter_endpoint_ips,
		"generator": generator_list,
		"ellipsise": 3,
	},
	"endpoints_eps": {
		"header": "Endpoints:",
		"path": "subsets",
		"datagetter": datagetter_eps_endpoints,
		"generator": generator_list_with_status,
		"ellipsise": 3,
	},
	"error": {
		"header": "Error:",
	},
	"event_reason": {
		"header": "Reason:",
		"path": "reason",
		"datagetter": datagetter_substitute_quotationmarks,
	},
	"event_status": {
		"header": "Status:",
		"path": "type",
		"datagetter": datagetter_event_status,
		"generator": generator_status,
	},
	"external_ip": {
		"header": "External-IPs:",
		"generator": generator_list,
	},
	"fail_count": {
		"header": "Fail:",
		"path": "report#summary#failCount",
		"generator": generator_numerical_range,
		"ranges": [(0, 1, ("types", "watermark_low")), (1, sys.maxsize, ("types", "watermark_high"))],
		"ralign": True,
	},
	"first_seen": {
		"header": "First Seen:",
		"path": ["eventTime", "deprecatedFirstTimestamp", "firstTimestamp"],
		"datagetter": datagetter_timestamp_with_fallback,
		"generator": generator_age,
		"ralign": True,
	},
	"from": {
		"header": "Source:",
		"generator": generator_list,
		"field_prefixes": [
			[("separators", "principals"), ("separators", "list_start")],
			[("separators", "not"),
			 ("separators", "principals"), ("separators", "list_start")],
			[("separators", "request_principals"), ("separators", "list_start")],
			[("separators", "not"),
			 ("separators", "request_principals"), ("separators", "list_start")],
			[("separators", "namespaces"), ("separators", "list_start")],
			[("separators", "not"),
			 ("separators", "namespaces"), ("separators", "list_start")],
			[("separators", "ipblocks"), ("separators", "list_start")],
			[("separators", "not"),
			 ("separators", "ipblocks"), ("separators", "list_start")],
		],
		"field_suffixes": [
			[("separators", "list_end")],
			[("separators", "list_end")],
			[("separators", "list_end")],
			[("separators", "list_end")],
			[("separators", "list_end")],
			[("separators", "list_end")],
			[("separators", "list_end")],
			[("separators", "list_end")],
		],
		# Logical and
		"field_separators": [("separators", "logical_and")],
		# Logical or
		"item_separator": ("separators", "logical_or"),
	},
	"fsgroup": {
		"header": "FS Group:",
		"path": "spec#fsGroup#rule",
	},
	"fstype": {
		"header": "Filesystem:",
		"path": "fstype",
	},
	"ftype": {
		"header": "Type:",
	},
	"fullname": {
		"header": "Full Name:",
		"path": "fullName",
	},
	"gateways": {
		# [gateway, ...]
		"header": "Gateways:",
		"path": "spec#gateways",
		"generator": generator_list,
	},
	"group": {
		"header": "Group:",
	},
	"groups": {
		# [group, ...]
		"header": "Groups:",
		"path": "groups",
		"generator": generator_list,
	},
	"global_default": {
		"header": "Global Default:",
		"path": "globalDefault",
		"default": "False",
	},
	"handler": {
		"header": "Handler:",
		"path": "handler",
	},
	"hand_size": {
		"header": "Hand Size:",
		"path": "spec#limited#limitResponse#queuing#handSize",
		"generator": generator_numerical,
		"ralign": True,
	},
	"hard": {
		"header": "Hard:",
	},
	"healthy_antrea": {
		"header": "Healthy:",
		"datagetter": datagetter_condition_field,
		"generator": generator_status,
		"path": [("agentConditions", "AgentHealthy", "status"), ("controllerConditions", "ControllerHealthy", "status")],
	},
	"high_severity": {
		"header": "High:",
		"path": "report#summary#highCount",
		"generator": generator_numerical_range,
		"ranges": [(0, 1, ("types", "watermark_low")), (1, sys.maxsize, ("types", "watermark_high"))],
		"ralign": True,
	},
	"holder": {
		"header": "Holder:",
		"path": "spec#holderIdentity",
	},
	"honor_labels": {
		"header": "Honor Labels:",
	},
	"host": {
		"header": "Host:",
		"path": "spec#host",
	},
	"hosts": {
		# [host, ...]
		"header": "Hosts:",
		"path": "spec#hosts",
		"generator": generator_list,
	},
	"identity": {
		"header": "Identity:",
		"path": "identity#name",
	},
	"identity_id": {
		"header": "Identity ID:",
	},
	"identities": {
		# [identity, ...]
		"header": "Identities:",
		"path": "identities",
		"generator": generator_list,
	},
	"image_id": {
		"header": "Image ID:",
	},
	"image_reference": {
		"header": "Image ID:",
	},
	"image_version": {
		"header": "Version:",
	},
	"index": {
		"header": "Index:",
		"ralign": True,
	},
	"info_count": {
		"header": "Info:",
		"path": "report#summary#infoCount",
		"generator": generator_numerical_range,
		"ranges": [(0, 1, ("types", "watermark_low")), (1, sys.maxsize, ("types", "watermark_medium"))],
		"ralign": True,
	},
	"ingress_address": {
		# (hostname, ip) (xor)
		"header": "Address:",
		"datagetter": datagetter_ingress_address,
		"generator": generator_list,
	},
	"ingress_enforcement": {
		"header": "Ingress Enforcement:",
	},
	"ingress_hosts": {
		"header": "Hosts:",
		# Note: Artificial path
		"path": "hosts",
		"datagetter": datagetter_ingress_hosts,
		"generator": generator_list,
	},
	"ingress_ports": {
		"header": "Ports:",
		# Note: Artificial path
		"path": "ports",
		"datagetter": datagetter_ingress_hosts,
		"generator": generator_list,
		"field_colors": [("types", "service"), ("types", "port"), ("types", "protocol")],
		"field_separators": [("separators", "port"), ("separators", "service")],
	},
	"instances": {
		"header": "Instances:",
		"generator": generator_numerical,
		"ralign": True,
	},
	"internal_ips": {
		# [ip, ...]
		"header": "Internal-IPs:",
		"path": "internal",
		"datagetter": datagetter_node_addresses,
		"generator": generator_list,
		"ellipsise": 3,
	},
	"interval": {
		"header": "Interval:",
	},
	"ipblock": {
		"header": "IP Block:",
	},
	"ipblock_exceptions": {
		"header": "IP Block Exceptions:",
		"generator": generator_list,
	},
	"ips": {
		# [ip, ...]
		"header": "IPs:",
		"generator": generator_list,
		"ellipsise": 3,
	},
	"ipv4": {
		"header": "IPv4:",
		"generator": generator_list,
	},
	"ipv6": {
		"header": "IPv6:",
		"generator": generator_list,
	},
	"issuer": {
		# (kind, name)
		"header": "Issuer:",
		"path": ["spec#issuerRef#kind", "spec#issuerRef#name"],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_list,
		"field_colors": [("types", "kind"), ("types", "generic")],
	},
	"job_state": {
		"header": "State:",
		"datagetter": datagetter_job_state,
		"generator": generator_status,
	},
	"kernel": {
		"header": "Kernel-Version:",
		"path": "status#nodeInfo#kernelVersion",
	},
	"key": {
		"header": "Key:",
	},
	"kind": {
		# (kind, api_group)
		"header": "Kind:",
		"path": "kind",
		"generator": generator_list,
		"field_colors": [("types", "kind"), ("types", "api_group")],
		"field_separators": [("separators", "kind_apigroup")],
	},
	"kubelet_version": {
		"header": "Version:",
		"path": "status#nodeInfo#kubeletVersion",
	},
	"kubernetes_roles": {
		# [role, ...]
		"header": "Roles:",
		"datagetter": datagetter_node_roles,
		"generator": generator_list,
	},
	"last_heartbeat_antrea": {
		"header": "Last Heartbeat:",
		"datagetter": datagetter_condition_field,
		"generator": generator_str_timestamp,
		"path": [("agentConditions", "AgentHealthy", "lastHeartbeatTime"), ("controllerConditions", "ControllerHealthy", "lastHeartbeatTime")],
	},
	"last_schedule": {
		"header": "Last Schedule:",
		"path": "status#lastScheduleTime",
		"datagetter": datagetter_age,
		"generator": generator_age,
		"ralign": True,
	},
	"limit": {
		"header": "Limit:",
		# FIXME datagetter
	},
	"link": {
		"header": "Link:",
	},
	"lmax": {
		"header": "Max:",
	},
	"lmin": {
		"header": "Min:",
	},
	"location": {
		"header": "Location:",
		"path": "spec#location",
	},
	"log_type": {
		"header": "Log Type:",
	},
	"low_severity": {
		"header": "Low:",
		"path": "report#summary#lowCount",
		"generator": generator_numerical_range,
		"ranges": [(0, 1, ("types", "watermark_low")), (1, sys.maxsize, ("types", "watermark_medium"))],
		"ralign": True,
	},
	"ltype": {
		"header": "Type:",
	},
	"match": {
		"header": "Match:",
		"generator": generator_list,
		"field_colors": [("types", "key"), ("types", "value")],
		"field_separators": [("separators", "keyvalue")],
	},
	"matching_precedence": {
		"header": "Matching Precedence:",
		"path": "spec#matchingPrecedence",
		"generator": generator_numerical,
		"ralign": True,
	},
	"max_lr_ratio": {
		"header": "Max Limit/Request Ratio:",
	},
	"maxpods": {
		"header": "Max Pods:",
		"path": "spec#maxReplicas",
		"generator": generator_numerical,
		"ralign": True,
	},
	"max_replicas": {
		"header": "Max:",
		"generator": generator_numerical,
		"ralign": True,
	},
	"max_unavailable": {
		"header": "Max Unavailable:",
		"path": "spec#minAvailable",
		"default": "N/A",
		"ralign": True,
	},
	"medium_severity": {
		"header": "Medium:",
		"path": "report#summary#mediumCount",
		"generator": generator_numerical_range,
		"ranges": [(0, 1, ("types", "watermark_low")), (1, sys.maxsize, ("types", "watermark_medium"))],
		"ralign": True,
	},
	"mem": {
		"header": "Mem% / Total:",
		"path": (r"^(\d+).*", ["status#allocatable#memory", "status#capacity#memory"]),
		"datagetter": datagetter_regex_split_to_tuples,
		"generator": generator_mem,
		"ralign": True,
	},
	"mem_bytes": {
		"header": "Memory:",
		"path": "mem_bytes",
		"generator": generator_mem_single,
		"ralign": True,
	},
	"mem_pod_metrics_total": {
		"header": "Memory:",
		"path": "usage#memory",
		"datagetter": datagetter_pod_metrics_memory,
		"generator": generator_mem_single,
		"ralign": True,
	},
	"message": {
		"header": "Message:",
		"path": "message",
		"datagetter": datagetter_substitute_quotationmarks,
	},
	"methods": {
		"header": "Methods:",
		"generator": generator_list,
	},
	"min_available": {
		"header": "Min Available:",
		"path": "spec#minAvailable",
		"default": "N/A",
		"ralign": True,
	},
	"minpods": {
		"header": "Min Pods:",
		"path": "spec#minReplicas",
		"generator": generator_numerical,
		"ralign": True,
	},
	"misscheduled_replicas": {
		"header": "Misscheduled:",
		"path": "status#numberMisscheduled",
		"generator": generator_numerical_range,
		"ranges": [(0, 1, ("types", "generic")), (1, sys.maxsize, ("types", "watermark_high"))],
		"ralign": True,
	},
	"model": {
		"header": "Model:",
		"path": "model",
	},
	"mountpoint": {
		"header": "Mountpoint:",
		"path": "mountpoint",
	},
	"name": {
		"header": "Name:",
		"path": "metadata#name",
	},
	"name_version": {
		"header": "Name:",
		"path": (r"^(.*):(.*)", ["metadata#name"]),
		"datagetter": datagetter_regex_split_to_tuples,
		"generator": generator_list,
		"field_colors": [("types", "generic"), ("types", "version")],
		"field_separators": [("separators", "version")],
	},
	"namespace": {
		"header": "Namespace:",
		"path": "metadata#namespace",
	},
	"namespace_selector": {
		# [(key, value), ...]
		"header": "Pod Selector:",
		"generator": generator_list,
		"field_colors": [("types", "generic"), ("types", "generic")],
		"field_separators": [("separators", "selector")],
	},
	"node": {
		"header": "Node:",
	},
	"node_id": {
		"header": "Node ID:",
	},
	"node_selector": {
		"header": "Node Selector:",
		"path": "spec#nodeSelector",
	},
	"node_status": {
		"header": "Status:",
		"datagetter": datagetter_node_status,
		"generator": generator_status,
	},
	"non_resource_urls": {
		"header": "Non-Resource URLs:",
		"generator": generator_list,
	},
	"nrofsecrets": {
		"header": "# of Secrets:",
		"path": "secrets",
		"datagetter": datagetter_count,
		"generator": generator_numerical,
		"ralign": True,
	},
	"obj": {
		# (kind, name)
		"header": "Object:",
		"path": ["involvedObject#kind", "involvedObject#name"],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_list,
		"field_colors": [("types", "kind"), ("types", "generic")],
	},
	"operation": {
		"header": "Operation:",
		"generator": generator_list,
		"field_prefixes": [
			[("separators", "hosts"), ("separators", "list_start")],
			[("separators", "not"),
			 ("separators", "hosts"), ("separators", "list_start")],
			[("separators", "ports"), ("separators", "list_start")],
			[("separators", "not"),
			 ("separators", "ports"), ("separators", "list_start")],
			[("separators", "methods"), ("separators", "list_start")],
			[("separators", "not"),
			 ("separators", "methods"), ("separators", "list_start")],
			[("separators", "paths"), ("separators", "list_start")],
			[("separators", "not"),
			 ("separators", "paths"), ("separators", "list_start")],
		],
		"field_suffixes": [
			[("separators", "list_end")],
			[("separators", "list_end")],
			[("separators", "list_end")],
			[("separators", "list_end")],
			[("separators", "list_end")],
			[("separators", "list_end")],
			[("separators", "list_end")],
			[("separators", "list_end")],
		],
		# Logical and
		"field_separators": [("separators", "logical_and")],
		# Logical or
		"item_separator": ("separators", "logical_or"),
	},
	"operator": {
		"header": "Operator:",
	},
	"os": {
		"header": "OS-Image:",
		"path": "status#nodeInfo#osImage",
	},
	"options": {
		"header": "Options:",
		"path": "options",
		"generator": generator_list,
	},
	"overhead": {
		"header": "Overhead:",
	},
	"pass_count": {
		"header": "Pass:",
		"path": "report#summary#infoCount",
		"generator": generator_numerical,
		"ralign": True,
	},
	"path": {
		"header": "Path:",
	},
	"paths": {
		"header": "Paths:",
		"generator": generator_list,
	},
	"phase": {
		"header": "Phase:",
		"path": "status#phase",
		"datagetter": datagetter_phase,
		"generator": generator_status,
	},
	"pods": {
		"header": "Pods:",
		# [(namespace, name), ...]
		"generator": generator_list,
		"field_colors": [("types", "namespace"), ("types", "generic")],
		"field_separators": [("separators", "namespace")],
	},
	"pod_info_on_mount": {
		"header": "Pod Info on Mount:",
		"path": "spec#podInfoOnMount",
	},
	"pod_ip": {
		"header": "Pod IP:",
	},
	"pod_selector": {
		# [(key, value), ...]
		"header": "Pod Selector:",
		"datagetter": datagetter_pod_selector,
		"generator": generator_list,
		"field_colors": [("types", "generic"), ("types", "generic")],
		"field_separators": [("separators", "selector")],
	},
	"policy_type": {
		"header": "Policy Type:",
	},
	"port": {
		# (name, port, protocol)
		"header": "Port:",
		"generator": generator_list,
		"field_colors": [("types", "service"), ("types", "port"), ("types", "protocol")],
		"field_separators": [("separators", "port"), ("separators", "service")],
	},
	"port_linkerd": {
		"header": "Port:",
		"path": "spec#port",
	},
	"ports_ep": {
		# (name, port, protocol)
		"header": "Ports:",
		"path": "subsets",
		"datagetter": datagetter_ep_ports,
		"generator": generator_list,
		"field_colors": [("types", "service"), ("types", "port"), ("types", "protocol")],
		"field_separators": [("separators", "port"), ("separators", "service")],
	},
	"ports_eps": {
		# (name, port, protocol)
		"header": "Ports:",
		"path": "ports",
		"datagetter": datagetter_eps_ports,
		"generator": generator_list,
		"field_colors": [("types", "service"), ("types", "port"), ("types", "protocol")],
		"field_separators": [("separators", "port"), ("separators", "service")],
	},
	"ports_svc": {
		# (name, port, protocol)
		"header": "Ports:",
		"datagetter": datagetter_svc_ports,
		"generator": generator_list,
		"field_colors": [("types", "service"), ("types", "port"), ("types", "protocol")],
		"field_separators": [("separators", "port"), ("separators", "service")],
	},
	"priority_tier": {
		"header": "Priority:",
		"path": "spec#priority",
		"generator": generator_numerical,
	},
	"priority": {
		"header": "Value:",
		"path": "value",
		"generator": generator_numerical,
	},
	"priority_level": {
		"header": "Priority Level:",
		"path": "spec#priorityLevelConfiguration#name",
		"generator": generator_priority_level,
	},
	"priority_type": {
		"header": "Priority Type:",
		"path": "spec#type",
	},
	"privileged": {
		"header": "Privileged:",
		"path": "spec#privileged",
		"default": "False",
	},
	"protocol": {
		"header": "Protocol:",
		"path": "spec#proxyProtocol",
	},
	"provider": {
		"header": "Provider:",
		"path": "providerName",
	},
	"providerusername": {
		"header": "Provider User Name:",
		"path": "providerUserName",
	},
	"provisioner": {
		"header": "Provisioner:",
		"path": "provisioner",
	},
	"proxy_url": {
		"header": "Proxy URL:",
	},
	"publisher": {
		"header": "Publisher:",
		"path": "spec#publisher",
	},
	"pv_status": {
		"header": "Status:",
		"datagetter": datagetter_pv_status,
		"generator": generator_status,
	},
	"pvc_status": {
		"header": "Status:",
		"datagetter": datagetter_pvc_status,
		"generator": generator_status,
	},
	"readonlyrootfs": {
		"header": "R/O Root FS:",
		"path": "spec#readOnlyRootFilesystem",
		"default": "False",
	},
	"repository": {
		"header": "Repository:",
		"path": "report#artifact#repository",
	},
	"queues": {
		"header": "Queues:",
		"path": "spec#limited#limitResponse#queuing#queues",
		"generator": generator_numerical,
		"ralign": True,
	},
	"queue_length_limit": {
		"header": "Queue Length List:",
		"path": "spec#limited#limitResponse#queuing#queueLengthLimit",
		"generator": generator_numerical,
		"ralign": True,
	},
	"ready": {
		"header": "Ready:",
		"generator": generator_status,
		"sortkey1": "status_group",
	},
	"ready_scheduled_plugins": {
		"header": "Ready:",
		"path": [("status#numberReady", 0), ("status#desiredNumberScheduled", 0)],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_total,
		"ranges": [(-1, -1, ("types", "numerical")), (0, 1, ("types", "watermark_high")), (1, -1, ("types", "watermark_medium"))],
		"ralign": True,
	},
	"ready_replicas": {
		"header": "Ready:",
		"path": [("status#readyReplicas", 0), ("status#replicas", 0)],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_numerical_range,
		"ranges": [(0, "1", ("types", "watermark_high")), ("1", sys.maxsize, ("types", "generic"))],
		"ralign": True,
	},
	"ready_replicas_ds": {
		"header": "Ready:",
		"path": ["status#numberReady", "status#currentNumberScheduled"],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_numerical_range,
		"ranges": [(0, "1", ("types", "watermark_high")), ("1", sys.maxsize, ("types", "generic"))],
		"ralign": True,
	},
	"ready_replicas_tuple": {
		# (ready, total)
		"header": "Ready:",
		"path": [("status#readyReplicas", 0), ("status#replicas", 0)],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_total,
		"ranges": [(-1, -1, ("types", "numerical")), (0, 1, ("types", "watermark_high")), (1, -1, ("types", "watermark_medium"))],
		"ralign": True,
	},
	"reason": {
		"header": "Reason:",
		"path": "status#reason",
	},
	"reclaim_policy": {
		"header": "Reclaim Policy:",
		"path": ["reclaimPolicy", "spec#persistentVolumeReclaimPolicy"],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_list,
	},
	"reference": {
		# (kind, name)
		"header": "Reference:",
		"path": ["spec#scaleTargetRef#kind", "spec#scaleTargetRef#name"],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_list,
		"field_colors": [("types", "kind"), ("types", "generic")],
	},
	"replicas": {
		"header": "Replicas:",
		"generator": generator_numerical,
		"path": "spec#replicas",
		"ralign": True,
	},
	"repo_type": {
		"header": "Type:",
		"path": "spec#type",
	},
	"repo_url": {
		"header": "URL:",
		"path": "spec#url",
	},
	"request": {
		# [(resource, used, hard), ...]
		"header": "Request:",
		"datagetter": datagetter_request,
		"generator": generator_list,
		"field_colors": [("types", "resource"), ("types", "numerical"), ("types", "numerical")],
		"field_separators": [("separators", "resource"), ("separators", "fraction")],
	},
	"requestor": {
		"header": "Requesting User:",
		"path": "spec#username",
	},
	"resolution": {
		"header": "Resolution:",
		"path": "spec#resolution",
	},
	"resource": {
		"header": "Resource:",
	},
	"resource_tuple": {
		# (kind, name)
		"header": "Resource:",
		"generator": generator_list,
		"field_colors": [("types", "kind"), ("types", "generic")],
	},
	"resource_names": {
		"header": "Resource Names:",
		"generator": generator_list,
	},
	"restarts": {
		"header": "Restarts:",
		"generator": generator_numerical,
		"ralign": True,
	},
	"retval": {
		"header": "Return Value:",
		"ralign": True,
	},
	"revision": {
		"header": "Revision:",
		"path": "revision",
		"ralign": True,
	},
	"role": {
		"header": "Role:",
		"path": "roleRef#name",
	},
	"rtype": {
		"header": "Type:",
	},
	"rule_type": {
		"header": "Rule Type:",
	},
	"runasuser": {
		"header": "Run as User:",
		"path": "spec#runAsUser#rule",
	},
	"scanner": {
		"header": "Scanner:",
		"path": "report#scanner#name",
	},
	"schedule": {
		"header": "Schedule:",
		"path": "spec#schedule",
	},
	"scheme": {
		"header": "Scheme:",
	},
	"secret": {
		"header": "Secret:",
		"path": "spec#secretName",
	},
	"secret_type": {
		"header": "Type:",
		"path": (r"^(.*)/(.*)|^(.*)", ["type"]),
		"datagetter": datagetter_regex_split_to_tuples,
		"generator": generator_list,
		"field_colors": [("types", "secret_type"), ("types", "generic")],
	},
	"seen": {
		"header": "Last Seen:",
		# If there's no lastObservedTime we fallback to eventTime
		"path": ["series#lastObservedTime", "deprecatedLastTimestamp", "lastTimestamp", "eventTime", "deprecatedFirstTimestamp", "firstTimestamp"],
		"datagetter": datagetter_timestamp_with_fallback,
		"generator": generator_age,
		"ralign": True,
	},
	"selinux": {
		"header": "SELinux:",
		"path": "spec#seLinux#rule",
	},
	"server": {
		"header": "Server:",
		"path": "spec#server#name",
	},
	"service": {
		# (namespace, name, port)
		"header": "Service:",
		"generator": generator_list,
		"field_colors": [("types", "namespace"), ("types", "generic"), ("types", "port")],
		"field_separators": [("separators", "namespace"), ("separators", "port")],
	},
	"serviceaccounts": {
		"header": "Service Accounts:",
		"generator": generator_list,
	},
	"services": {
		"header": "Services:",
		"generator": generator_list,
	},
	"signername": {
		"header": "Signer:",
		"path": "spec#signerName",
	},
	"source": {
		# (component, host)
		"header": "Source:",
		"path": ["source#component", "source#host"],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_list,
	},
	"source_port": {
		"header": "Source Port:",
	},
	"source_type": {
		"header": "Source Type:",
		"path": "spec#sourceType",
	},
	"start_time": {
		"header": "Start Time:",
		"generator": generator_timestamp,
	},
	"state": {
		"header": "State:",
		"generator": generator_status,
	},
	"status": {
		"header": "Status:",
		"generator": generator_status,
	},
	"status_deployment": {
		"header": "Status:",
		"path": "status#conditions",
		"datagetter": datagetter_status_deployment,
		"generator": generator_status,
	},
	"status_latest_condition": {
		"header": "Status:",
		"path": "status#conditions",
		"datagetter": datagetter_status_latest_condition,
		"generator": generator_status,
	},
	"status_message": {
		"header": "Status:",
		"datagetter": datagetter_condition_ready,
	},
	"status_message_deployment": {
		"header": "Message:",
		"path": "status#conditions",
		"datagetter": datagetter_status_message_deployment,
	},
	"storage_class": {
		"header": "Storage Class:",
		"path": "spec#storageClassName",
	},
	"strategy": {
		"header": "Strategy:",
	},
	"stype": {
		"header": "Type:",
	},
	"subject_groups": {
		# [group, ...]
		"header": "Groups:",
		"path": "groups",
		"datagetter": datagetter_subject,
		"generator": generator_list,
	},
	"subject_serviceaccounts": {
		"header": "Service Accounts:",
		"path": "serviceaccounts",
		"datagetter": datagetter_subject,
		"generator": generator_list,
	},
	"subject_users": {
		# [user, ...]
		"header": "Users:",
		"path": "users",
		"datagetter": datagetter_subject,
		"generator": generator_list,
	},
	"supgroups": {
		# [group, ...]
		"header": "Supplemental Groups:",
		"path": "spec#supplementalGroups#rule",
		"generator": generator_list,
	},
	"suspend": {
		"header": "Suspend:",
		"path": "spec#suspend",
	},
	"svc_type": {
		"header": "Type:",
		"path": "spec#type",
	},
	"tag": {
		"header": "Tag:",
		"path": "report#artifact#tag",
	},
	"tags": {
		"header": "Tags:",
		"path": ("status#tags", ["tag"]),
		"datagetter": datagetter_list_fields,
		"generator": generator_list,
	},
	"taints": {
		# [(key, value), ...]
		"header": "Taints:",
		"datagetter": datagetter_node_taints,
		"generator": generator_list,
		"field_colors": [("types", "key"), ("types", "value")],
		"field_separators": [("separators", "keyvalue")],
	},
	"target_port": {
		"header": "Target Port:",
	},
	"target_ref": {
		# (kind, namespace, name)
		"header": "Target Reference:",
		"generator": generator_list,
		"field_colors": [("types", "kind"), ("types", "namespace"), ("types", "generic")],
		"field_separators": [("separators", "kind"), ("separators", "namespace")],
	},
	"target": {
		"header": "Target:",
		"generator": generator_numerical,
		"ralign": True,
	},
	"targets": {
		# [(current, target), ...]
		"header": "Targets:",
		"path": [("target#currentCPUUtilizationPercentage", "<unknown>"), "spec#targetCPUUtilizationPercentage"],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_list,
		"field_colors": [("types", "numerical"), ("types", "numerical")],
		"field_separators": [("separators", "fraction")],
		"field_suffix": [("", ("types", "generic")), ("separators", "percentage")],
	},
	"timestamp": {
		"header": "Start Time:",
		"datagetter": datagetter_timestamp,
		"generator": generator_timestamp,
		"path": "timestamp",
	},
	"topology": {
		"header": "Topology:",
		"generator": generator_list,
		"field_separators": [("separators", "equals")],
	},
	"topology_keys": {
		"header": "Topology Keys:",
		"generator": generator_list,
	},
	"traffic_type": {
		"header": "Traffic Type:",
	},
	"uid": {
		"header": "UID:",
		"path": "metadata#uid",
	},
	"unknown_severity": {
		"header": "Unknown:",
		"path": "report#summary#unknownCount",
		"generator": generator_numerical_range,
		"ranges": [(0, 1, ("types", "watermark_low")), (1, sys.maxsize, ("types", "watermark_high"))],
		"ralign": True,
	},
	"uptodate_replicas": {
		"header": "Up to Date:",
		"path": "status#updatedReplicas",
		"generator": generator_numerical,
		"ralign": True,
	},
	"uptodate_replicas_ds": {
		"header": "Up to Date:",
		"path": "status#updatedNumberScheduled",
		"generator": generator_numerical,
		"ralign": True,
	},
	"used": {
		"header": "Used:",
	},
	"user_uid_tuple": {
		# user:uid
		"header": "User:",
		"path": ["user#name", "user#uid"],
		"datagetter": datagetter_pathlist_to_tuple,
		"generator": generator_list,
		"field_colors": [("types", "user"), ("types", "uid")],
		"field_separators": [("separators", "user_uid")],
	},
	"users": {
		# [user, ...]
		"header": "Users:",
		"path": "users",
		"generator": generator_list,
		"field_colors": [("types", "generic")],
	},
	"value": {
		"header": "Value:",
	},
	"decoded_value": {
		"header": "Value:",
	},
	"verbs": {
		"header": "Verbs:",
		"generator": generator_list,
	},
	"version": {
		"header": "Version:",
		"path": "spec#version",
	},
	"visibility_policy": {
		"header": "Visibility Policy:",
	},
	"vlen": {
		"header": "Length:",
		"ralign": True,
	},
	"volume": {
		"header": "Volume:",
		"path": "spec#volumeName",
	},
	"volume_binding_mode": {
		"header": "Volume Binding Mode:",
		"path": "volumeBindingMode",
		"default": "VolumeBindingImmediate",
	},
	"volume_mode": {
		"header": "Volume Mode:",
		"path": "spec#volumeMode",
	},
	"volumes": {
		# [volume, ...]
		"header": "Volumes:",
		"path": "spec#volumes",
		"generator": generator_list,
		"default": ["<none>"],
	},
	"volume_lifecycle_modes": {
		# [mode, ...]
		"header": "Modes:",
		"path": "spec#volumeLifecycleModes",
		"generator": generator_list,
	},
	"vtype": {
		"header": "Type:",
	},
	"warn_count": {
		"header": "Warn:",
		"path": "report#summary#warnCount",
		"generator": generator_numerical_range,
		"ranges": [(0, 1, ("types", "watermark_low")), (1, sys.maxsize, ("types", "watermark_medium"))],
		"ralign": True,
	},
	"warning_count": {
		"header": "Warning #:",
		"path": "report#summary#warningCount",
		"generator": generator_numerical_range,
		"ranges": [(0, 1, ("types", "watermark_low")), (1, sys.maxsize, ("types", "watermark_medium"))],
		"ralign": True,
	},
	"webhooks": {
		# Number of webhooks
		"header": "Webhooks:",
		"path": "webhooks",
		"datagetter": datagetter_count,
		"generator": generator_numerical,
		"ralign": True,
	},
	"window": {
		"header": "Duration:",
		"path": "window",
		"ralign": True,
	},
}

def fieldgenerator(view, fields = None, sortcolumn = None, blacklist = []):
	global namespace

	if namespace != "" and "namespace" not in blacklist:
		blacklist.append("namespace")

	if fields is None:
		if deep_get(views, f"{view}#fields") is None:
			return None, None

		fields = []
		if "custom_fields" in views[view]:
			fields += views[view]["custom_fields"]
		else:
			fields += views[view]["fields"]

	# delete all blacklisted fields
	for field in blacklist:
		try:
			fields.remove(field)
		except ValueError:
			pass

	# OK, we've pruned all unwanted fields; now we need to check that the sort column
	# is still valid, and if not pick another one
	if sortcolumn is not None and sortcolumn not in fields:
		sortcolumn = "name"
	elif sortcolumn is None:
		sortcolumn = deep_get(iktconfig, f"{view}#sortcolumn", "")
		if sortcolumn not in fields:
			sortcolumn = deep_get(views, f"{view}#sortcolumn", "")
		if sortcolumn not in fields:
			sortcolumn = "name"

	tmp_field = OrderedDict()

	for field in fields:
		header = ""

		# If we get a tuple the first part is field, the second overrides header
		if type(field) is tuple and len(field) > 1:
			field, header = field

		if field not in field_templates:
			raise Exception(f"Programming error! Field “{field}“ not defined in field_templates")

		if "header" not in deep_get(field_templates, field):
			raise Exception(f"Programming error! Field “{field}“ lacks mandatory field “header“")

		if len(header) == 0:
			header = deep_get(field_templates, f"{field}#header")
		generator = deep_get(field_templates, f"{field}#generator", generator_basic)
		processor = deep_get(field_templates, f"{field}#processor", None)
		if processor is None:
			processor = default_processor.get(generator)
		ralign = deep_get(field_templates, f"{field}#ralign", False)
		field_colors = deep_get(field_templates, f"{field}#field_colors", [("types", "field")])
		field_prefixes = deep_get(field_templates, f"{field}#field_prefixes", None)
		field_suffixes = deep_get(field_templates, f"{field}#field_suffixes", None)
		field_separators = deep_get(field_templates, f"{field}#field_separators", [("separators", "field")])
		ellipsise = deep_get(field_templates, f"{field}#ellipsise", -1)
		ellipsis = deep_get(field_templates, f"{field}#ellipsis", ("separators", "ellipsis"))
		item_separator = deep_get(field_templates, f"{field}#item_separator", ("separators", "list"))
		sortkey1 = field
		# If sortkey1 == sortcolumn this "fails", but it's good enough
		sortkey2 = sortcolumn
		# Special case:
		if field == "status":
			sortkey2 = "status"
		if sortkey1 == sortkey2 and "status" in fields:
			sortkey2 = "status_group"

		tmp_field[field] = {
			"header": header,
			"generator": generator,
			"processor": processor,
			"ralign": ralign,
			"field_colors": field_colors,
			"field_prefixes": field_prefixes,
			"field_suffixes": field_suffixes,
			"field_separators": field_separators,
			"ellipsise": ellipsise,
			"ellipsis": ellipsis,
			"item_separator": item_separator,
			"sortkey1": sortkey1,
			"sortkey2": sortkey2,
		}

	return tmp_field, sortcolumn

def genericlistloop(stdscr, view):
	global namespace
	global initial_name
	global initial_namespace
	global latest_info
	blacklist = deep_get(views, f"{view}#field_blacklist", [])

	field_list, sortcolumn = fieldgenerator(view, blacklist = blacklist)
	uip = UIProps(stdscr)

	windowheader = view
	viewref = views[view]

	kind = viewref["kind"]

	if "helptext" in views[view]:
		helptext = viewref["helptext"]
	else:
		helptext = generate_helptext(view, "listview", [])
	if "activatedfun" in viewref:
		activatedfun = viewref["activatedfun"]
	else:
		activatedfun = genericinfoloop
	update_delay = deep_get(viewref, "update_delay", -1)

	sortorder_reverse = deep_get(viewref, "sortorder_reverse", False)
	is_taggable = deep_get(viewref, "is_taggable", True)
	extra_vars = deep_get(viewref, "extra_vars", {})

	uip.init_window(field_list, view = kind, windowheader = windowheader, update_delay = update_delay, sortcolumn = sortcolumn, sortorder_reverse = sortorder_reverse, helptext = helptext, activatedfun = activatedfun)

	# For the status bar; position is always at the bottom of the screen and the entire width of the screen
	statusbar = uip.init_statusbar()

	# For the list
	headerpad, listpad = uip.init_listpad(listheight = 1, width = -1, ypos = 1, xpos = 1)

	label_selector = ""

	while True:
		uip.countdown_to_update()

		if uip.is_update_triggered():
			uip.update_window()

			# The data in some fields might become shorter, so we need to trigger a clear
			uip.statusbar.erase()

			if uip.continuous_log == True:
				interval = "Continuous"
			else:
				interval = "Manual"

			statusarray1 = [
			]
			if len(label_selector) > 0:
				statusarray1 += [
					("Label selector: ", ("statusbar", "infoheader")), (f"{label_selector}", ("statusbar", "highlight"))
				]
			statusarray2 = [
			]
			if view != "Contexts" and ("namespace" in blacklist or "namespace" in deep_get(viewref, "fields")):
				statusarray2 += [("Namespace: ", ("statusbar", "infoheader")), (f"{namespace if len(namespace) > 0 else '<All>'}", ("statusbar", "highlight"))]

			uip.addthemearray(statusbar, statusarray1, y = 0, x = 0)
			uip.addthemearray(statusbar, statusarray2, y = 1, x = 0)

			if "listgetter" in viewref:
				listgetter = deep_get(viewref, "listgetter")
			else:
				listgetter = "kh.get_list_by_kind_namespace"

			infogetter = deep_get(viewref, "infogetter", generic_infogetter)
			infogetter_extra_vars = deep_get(viewref, "extra_vars#infogetter", {})

			if listgetter is not None:
				listgetter = eval(listgetter)
				listgetter_extra_vars = deep_get(viewref, "extra_vars#listgetter", {})
				if kind[0].startswith("__"):
					vlist = listgetter(extra_vars = listgetter_extra_vars)
				else:
					vlist = listgetter(deep_get(viewref, "kind"), namespace, label_selector = label_selector)
				if infogetter == generic_infogetter:
					fields = deep_get(viewref, "fields")
					info = infogetter(vlist, fields)
				else:
					info = infogetter(vlist, extra_vars = infogetter_extra_vars)
			else:
				info = infogetter(extra_vars = infogetter_extra_vars)
			latest_info = view
			listlen = uip.update_info(info)
			linelen = update_field_widths(field_list, uip.info)
			if is_taggable == True:
				linelen += len("✓ ")
			uip.resize_listpad(listlen, linelen)

			tagged_items = set()


		# Output list
		y = 0

		uip.update_sorted_list()
		generate_listheader(uip, headerpad, field_list, is_taggable = is_taggable)
		for item in uip.sorted_list:
			uip.select_if_y(y, item)
			is_tagged = item in tagged_items
			generate_list(uip, listpad, item, field_list, y, uip.is_selected(item), is_taggable = is_taggable, is_tagged = is_tagged)
			y += 1

		uip.refresh_window()
		uip.refresh_listpad()
		uip.refresh_statusbar()
		curses.doupdate()

		unique_match = uip.goto_first_match_by_name_namespace(initial_name, initial_namespace)
		initial_name = None
		initial_namespace = None
		if unique_match is not None:
			selected = uip.get_selected()
			retval = uip.activatedfun(uip.stdscr, unique_match, kind)
			if retval == curses_helper.retval.RETURNFULL:
				return retval
			uip.force_update()
			uip.refresh_all()
			continue

		uip.stdscr.timeout(100)
		c = uip.stdscr.getch()
		retval = uip.generic_keycheck(c)

		if retval == curses_helper.retval.MATCH:
			continue
		elif retval == curses_helper.retval.RETURNONE:
			return curses_helper.retval.RETURNDONE
		elif retval == curses_helper.retval.RETURNFULL:
			return retval

		if c == curses.KEY_F2:
			retval = selectwindow(stdscr, uip)
			if retval == curses_helper.retval.RETURNFULL:
				return retval
			uip.refresh_all()
		elif c == curses.KEY_F3:
			retval = selectwindow(stdscr, uip, refresh_apis = True)
			if retval == curses_helper.retval.RETURNFULL:
				return retval
			uip.refresh_all()
		elif c == ord("B"):
			uip.toggle_borders()
			uip.refresh_all()
			uip.force_update()
		elif c == curses.KEY_F7:
			action_title = "Perform cluster-wide actions"
			action_src_list = listviewactions

			# Populate the list of actions
			actions, actionlist = populate_actionlist(action_list = action_src_list)

			if len(actions) == 0:
				continue

			tmpselection = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, actions, title = action_title)
			selection = None
			if tmpselection is not None and type(tmpselection[0]) == int:
				selection = ""
				for i in range(1, len(tmpselection)):
					if len(tmpselection[i]) > 0:
						selection += tmpselection[i][0][0]
			uip.refresh_all()
			curses.doupdate()

			if selection is not None and selection != "":
				actionfunc = None

				# Map the description back to key
				for action in actionlist:
					description = deep_get(actionlist, f"{action}#description")
					tmpdescription = ""
					if type(description) == str:
						tmpdescription = description
					else:
						for i in range(0, len(description)):
							tmpdescription += description[i][0]
					tmpmetadata = deep_get(actionlist, f"{action}#metadata", [])
					for metadata in tmpmetadata:
						tmpdescription += metadata[0]
					if tmpdescription == selection:
						actionfunc = deep_get(actionlist, f"{action}#actionfunc", command_hosts)
						# These are only relevant for node and inventory view
						allowoncontrolplane = deep_get(actionlist, f"{action}#allow_on_control_plane", True)
						singleoncontrolplane = deep_get(actionlist, f"{action}#single_on_control_plane", False)
						confirm = deep_get(actionlist, f"{action}#confirm", False)
						query = deep_get(actionlist, f"{action}#query")
						queryval = deep_get(actionlist, f"{action}#queryval")
						queryfunc = deep_get(actionlist, f"{action}#queryfunc")
						extravars = deep_get(actionlist, f"{action}#extravars", {})
						break

				if actionfunc is None:
					continue

				selection_vars = extravars

				# This should be modified; we might want to ask multiple queries.
				# At the very least asking for one input plus confirmation.
				# A multi input box might be useful too.
				if queryfunc is not None and queryval is not None and query is not None:
					if queryfunc == "string":
						# Not supported by inputbox widget
						# querydefault = ""
						string = curses_helper.inputbox(uip.stdscr, uip.maxy // 2, 1, uip.maxy - 1, uip.maxx - 1, f"{query}: ")
						if string is None or string == "":
							continue
						else:
							selection_vars[queryval] = string
					elif queryfunc == "confirm":
						querydefault = False
						if curses_helper.confirmationbox(uip.stdscr, uip.maxy // 2, uip.maxx // 2, title = f"{query}?", default = querydefault) == False:
							continue
					elif queryfunc == "filechooser":
						# It's a programming error if either of these two are not set, so it's OK to get an exception
						listgetter = extravars["listgetter"]
						basedir = extravars["basedir"]
						selected_file = None
						selected_ptype = None

						while basedir is not None:
							tmp_file_list = listgetter(basedir)
							file_list = []
							for item in tmp_file_list:
								realpath, filename, ptype = item
								if ptype in ["File", "Configuration File", "Kustomization"]:
									formatting = ("windowwidget", "default")
								elif ptype in ["<dir>"]:
									formatting = ("windowwidget", "highlight")
								else:
									raise Exception(f"Unknown ptype {ptype}")
								file_list.append((widgetlineattrs.NORMAL, [(f"{filename}", formatting)], [(f"{ptype}", ("windowwidget", "description"))]))
							tmpselection = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, file_list, title = query, cursor = True)
							if tmpselection is None or tmpselection == (0, [("", None)]):
								# This will break out of the loop
								basedir = None
								selected_file = None
								continue
							selected_entry = tmpselection[1][0][0]
							# Find entry in tmp_file_list
							for item in tmp_file_list:
								realpath, filename, ptype = item
								if filename == selected_entry:
									if ptype in ["<dir>"]:
										basedir = realpath
										uip.refresh_all()
									elif ptype in ["File", "Configuration File", "Kustomization"]:
										selected_file = realpath
										selected_ptype = ptype
										basedir = None
										break

						if selected_file is None:
							uip.force_update()
							continue
						selection_vars = extra_vars
						selection_vars["resource_path"] = (selected_file, selected_ptype)

				if confirm == True:
					if curses_helper.confirmationbox(uip.stdscr, uip.maxy // 2, uip.maxx // 2, title = f"Perform “{description}“?", default = False) == False:
						continue

				actionfunc(uip, items = [], actions = actionlist[action], values = selection_vars, kind = kind)

				uip.force_update()
				tagged_items.clear()
				continue
			tagged_items.clear()
		elif c == ord("N"):
			if "namespace" not in deep_get(viewref, "fields", []):
				continue

			all_ns = "<All>"
			if namespace == "":
				preselection = all_ns
			else:
				preselection = namespace
			namespace_list = [(widgetlineattrs.NORMAL, [(f"{all_ns}", ("windowwidget", "default"))])]

			tmp = kh.get_list_by_kind_namespace(("Namespace", ""), "")
			for ns in [deep_get(item, "metadata#name") for item in tmp]:
				namespace_list.append((widgetlineattrs.NORMAL, [(f"{ns}", ("windowwidget", "default"))]))

			tmpselection = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, namespace_list, title = "Select Namespace", cursor = True, preselection = preselection)
			selection = None
			if tmpselection is not None and type(tmpselection[0]) == int:
				selection = tmpselection[1][0][0]
			uip.refresh_all()
			if selection is not None and len(selection) > 0:
				if selection == all_ns:
					namespace = ""
				else:
					namespace = selection
			field_list, sortcolumn = fieldgenerator(view, blacklist = [])
			uip.init_window(field_list, view = kind, windowheader = windowheader, update_delay = update_delay, sortcolumn = sortcolumn, sortorder_reverse = sortorder_reverse, helptext = helptext, activatedfun = activatedfun)
			headerpad, listpad = uip.init_listpad(listheight = 1, width = -1, ypos = 1, xpos = 1)

			# For the status bar; position is always at the bottom of the screen and the entire width of the screen
			statusbar = uip.init_statusbar()
		elif (c == ord("t") or c == ord(" ")) and is_taggable == True:
			selected = uip.get_selected()

			if selected is not None:
				if selected in tagged_items:
					tagged_items.remove(selected)
				else:
					tagged_items.add(selected)
				# After we tag an item we advance the cursor (when possible);
				# this way we can select multiple continuous items in a straight-forward manner
				uip.move_cur_with_offset(1)
		elif c == ord("T") and is_taggable == True:
			# Tag by pattern
			pattern = curses_helper.inputbox(uip.stdscr, uip.maxy // 2, 1, uip.maxy - 1, uip.maxx - 1, "Tag '%s' matching: " % (uip.sortcolumn)).rstrip().lower()
			if pattern is None or pattern == "":
				continue

			# XXX: This should search the specified sortcolumn, not always in name
			for item in uip.sorted_list:
				try:
					#Exact match
					#tmp = re.match(r"%s$" % pattern, item.name):
					tmp = re.match(pattern, item.name)

					if tmp is not None and len(tmp[0]) > 0:
						if item not in tagged_items:
							tagged_items.add(item)
				except re.error:
					continue
		elif c == ord("") and is_taggable == True:
			if len(tagged_items) == 0:
				continue

			# Untag by pattern
			pattern = curses_helper.inputbox(uip.stdscr, uip.maxy // 2, 1, uip.maxy - 1, uip.maxx - 1, "Untag '%s' matching: " % (uip.sortcolumn)).rstrip().lower()
			if pattern is None or pattern == "":
				continue

			# XXX: This should search the specified sortcolumn, not always in name
			for item in uip.sorted_list:
				try:
					#Exact match
					#tmp = re.match(r"%s$" % pattern, item.name):
					tmp = re.match(pattern, item.name)

					if tmp is not None and len(tmp[0]) > 0:
						if item in tagged_items:
							tagged_items.remove(item)
				except re.error:
					continue
		elif c == ord("l") and is_taggable == True and len(vlist) > 0:
			# List the union of all labels of the tagged objects (if the objects support labels)
			if len(tagged_items) == 0:
				selected = uip.get_selected()
				tagged_items.add(selected)
			labellist = []
			labels = []
			if "namespace" in deep_get(vlist[0], "metadata", ""):
				itemlist = [ (item.namespace, item.name) for item in tagged_items ]
			else:
				itemlist = [ item.name for item in tagged_items ]

			if len(itemlist) == 0:
				continue

			for obj in vlist:
				if type(itemlist[0]) == tuple:
					if (deep_get(obj, "metadata#namespace", ""), deep_get(obj, "metadata#name")) not in itemlist:
						continue
				else:
					if deep_get(obj, "metadata#name") not in itemlist:
						continue

				labelref = deep_get(obj, "metadata#labels", {})
				for key in labelref:
					if (key, labelref[key]) not in labels:
						labels.append((key, labelref[key]))
			if len(labels) == 0:
				continue
			for key, value in sorted(labels):
				labellist.append((widgetlineattrs.NORMAL, [(f"{key}", ("windowwidget", "default"))], [(f"{value}", ("windowwidget", "default"))]))
			label_headers = ["Label:", "Value:"]
			curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, labellist, headers = label_headers, title = "Labels", cursor = False)
			tagged_items.clear()
			uip.refresh_all()
			curses.doupdate()
		elif c == ord("f") and is_taggable == True and len(vlist) > 0:
			# List the union of all labels of the tagged objects and create a label selector
			# from the choices made by the user (if the objects support labels)
			if len(tagged_items) == 0:
				selected = uip.get_selected()
				tagged_items.add(selected)

			labellist = []
			labels = []
			if "namespace" in deep_get(vlist[0], "metadata", ""):
				itemlist = [ (item.namespace, item.name) for item in tagged_items ]
			else:
				itemlist = [ item.name for item in tagged_items ]

			if len(itemlist) == 0:
				continue

			for obj in vlist:
				if type(itemlist[0]) == tuple:
					if (deep_get(obj, "metadata#namespace", ""), deep_get(obj, "metadata#name")) not in itemlist:
						continue
				else:
					if deep_get(obj, "metadata#name") not in itemlist:
						continue

				labelref = deep_get(obj, "metadata#labels", {})
				for key in labelref:
					if (key, labelref[key]) not in labels:
						labels.append((key, labelref[key]))
			if len(labels) == 0:
				continue
			for key, value in labels:
				labellist.append((widgetlineattrs.NORMAL, [(f"{key}", ("windowwidget", "default"))], [(f"{value}", ("windowwidget", "default"))]))
			label_headers = ["Label:", "Value:"]
			tagged_labels = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, labellist, headers = label_headers, title = "Labels", cursor = True, taggable = True)
			if len(tagged_labels) > 0:
				selectors = {}
				for i in range(0, len(labels)):
					if i in tagged_labels:
						selectors[labels[i][0]] = labels[i][1]
				label_selector = kh.make_selector(selectors)
				uip.force_update()
			tagged_items.clear()
			uip.refresh_all()
			curses.doupdate()
		elif c == ord("A") and view == "Inventory" and len(tagged_items) == 0:
			selected = uip.get_selected()
			groups = ansible_get_groups(ANSIBLE_INVENTORY)
			# These Ansible groups shouldn't be possible to toggle on/off
			for skip in ["all", "controlplane", "controlplanes", "master", "node", "nodes"]:
				try:
					groups.remove(skip)
				except:
					pass
			host_groups = ansible_get_groups_by_host(ANSIBLE_INVENTORY, selected.name)
			grouplist = []
			preselection = set()
			for i in range(0, len(groups)):
				grouplist.append((widgetlineattrs.NORMAL, [(f"{groups[i]}", ("windowwidget", "default"))]))
				if groups[i] in host_groups:
					preselection.add(i)
			group_headers = ["Ansible Groups:"]
			selection = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, grouplist, headers = group_headers, title = "Ansible Groups", cursor = True, taggable = True, preselection = preselection)
			new_groups = []
			for keep in ["all", "controlplane", "controlplanes", "master", "node", "nodes"]:
				if keep in host_groups:
					new_groups.append(keep)
			for i in selection:
				new_groups.append(groups[i])

			add_groups = set(new_groups) - set(host_groups)
			remove_groups = set(host_groups) - set(new_groups)

			for group in add_groups:
				retval = ansible_add_hosts(inventory = ANSIBLE_INVENTORY, hosts = [selected.name], group = group, skip_all = False)
			for group in remove_groups:
				retval = ansible_remove_hosts(inventory = ANSIBLE_INVENTORY, hosts = [selected.name], group = group)

			if len(add_groups) > 0 or len(remove_groups) > 0:
				uip.force_update()
			uip.refresh_all()
			curses.doupdate()
		elif c == ord("F") and is_taggable == True:
			# Clear the label selector
			if len(label_selector) > 0:
				label_selector = ""
				uip.force_update()
				uip.refresh_all()
				curses.doupdate()
		elif c == ord("L") and is_taggable == True:
			# List tagged items
			taggeditemlist = []
			if len(tagged_items) == 0:
				continue
			for item in tagged_items:
				taggeditemlist.append((widgetlineattrs.NORMAL, [("separators", "widgetbullet"), (f"{item.name}", ("windowwidget", "default"))]))
			curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, taggeditemlist, title = "Tagged items", cursor = False)
			uip.refresh_all()
			curses.doupdate()
		elif c == ord(";") and is_taggable == True and len(vlist) > 0:
			cluster_available = True
			items = []
			control_plane = False

			if len(tagged_items) == 0:
				selected = uip.get_selected()
				tagged_items.add(selected)

			for item in tagged_items:
				# XXX once all infogetters have been fixed we don't need to check for <none>
				if len(item.name) == 0 or item.name == "<none>":
					break

				if hasattr(item, "kubernetes_roles"):
					if "control-plane" in item.kubernetes_roles:
						control_plane = True
					if "<unknown>" in item.kubernetes_roles:
						cluster_available = False
				if hasattr(item, "namespace"):
					items.append((item.namespace, item.name))
				else:
					items.append(item.name)

			if len(items) == 0:
				tagged_items.clear()
				continue

			action_title = deep_get(viewref, "actions#title", "Perform action on tagged items")
			tmp_action_src_list = deep_get(viewref, "actions#actionlist", {})

			action_src_list = {
				"View YAML dump": {
					"description": "View YAML dump of resource",
					"actionfunc": view_yaml_dump,
					"title": "YAML dump",
					"read_only": True,
				},
				"Edit resource": {
					"description": "Edit resource",
					"actionfunc": edit_resource,
					"single_only": True,
				},
			}

			for action in tmp_action_src_list:
				# Override default actions by passing an empty action by the same name
				if action in action_src_list and tmp_action_src_list[action] == {}:
					action_src_list.pop(action)
				else:
					action_src_list[action] = tmp_action_src_list[action]

			# Populate the list of actions
			action_context = deep_get(viewref, "actions#actionlist#context", None)
			actions, actionlist = populate_actionlist(context = action_context, action_list = action_src_list, control_plane_selected = control_plane, single_item = (len(items) == 1), cluster_available = cluster_available)
			# If there are playbook actions, add those too
			if len(deep_get(viewref, "actions#playbooklist", {})) > 0:
				playbook_context = deep_get(viewref, "actions#playbooklist#context", None)
				actions, actionlist = populate_playbooklist(context = playbook_context, actions = [], action_list = actionlist, control_plane_selected = control_plane, single_item = (len(items) == 1), cluster_available = cluster_available)

			if len(actions) == 0:
				continue

			tmpselection = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, actions, title = action_title)
			selection = None
			if tmpselection is not None and type(tmpselection[0]) == int:
				selection = ""
				for i in range(1, len(tmpselection)):
					if len(tmpselection[i]) > 0:
						selection += tmpselection[i][0][0]
			uip.refresh_all()
			curses.doupdate()

			if selection is not None and selection != "":
				actionfunc = None

				# Map the description back to key
				for action in actionlist:
					description = deep_get(actionlist, f"{action}#description")
					tmpdescription = ""
					if type(description) == str:
						tmpdescription = description
					else:
						for i in range(0, len(description)):
							tmpdescription += description[i][0]
					tmpmetadata = deep_get(actionlist, f"{action}#metadata", [])
					for metadata in tmpmetadata:
						tmpdescription += metadata[0]
					if tmpdescription == selection:
						actionfunc = deep_get(actionlist, f"{action}#actionfunc", command_hosts)
						# These are only relevant for node and inventory view
						allowoncontrolplane = deep_get(actionlist, f"{action}#allow-on-control-plane", True)
						singleoncontrolplane = deep_get(actionlist, f"{action}#single-on-control-plane", False)
						confirm = deep_get(actionlist, f"{action}#confirm", False)
						query = deep_get(actionlist, f"{action}#query")
						queryval = deep_get(actionlist, f"{action}#queryval")
						queryfunc = deep_get(actionlist, f"{action}#queryfunc")
						extravars = deep_get(actionlist, f"{action}#extravars", {})
						title = deep_get(actionlist, f"{action}#title")
						break

				if actionfunc is None:
					continue

				selection_vars = extravars

				# This should be modified; we might want to ask multiple queries.
				# At the very least asking for one input plus confirmation.
				# A multi input box might be useful too.
				if queryfunc is not None and queryval is not None and query is not None:
					if queryfunc == "string":
						# Not supported by inputbox widget
						# querydefault = ""
						string = curses_helper.inputbox(uip.stdscr, uip.maxy // 2, 1, uip.maxy - 1, uip.maxx - 1, f"{query}: ")
						if string is None or string == "":
							continue
						else:
							selection_vars[queryval] = string
					elif queryfunc == "confirm":
						querydefault = False
						if curses_helper.confirmationbox(uip.stdscr, uip.maxy // 2, uip.maxx // 2, title = f"{query}?", default = querydefault) == False:
							continue
					elif queryfunc == "filechooser":
						# It's a programming error if either of these two are not set, so it's OK to get an exception
						listgetter = extravars["listgetter"]
						basedir = extravars["basedir"]
						selected_file = None
						selected_ptype = None

						while basedir is not None:
							tmp_file_list = listgetter(basedir)
							file_list = []
							for item in tmp_file_list:
								realpath, filename, ptype = item
								if ptype in ["File", "Configuration File", "Kustomization"]:
									formatting = ("windowwidget", "default")
								elif ptype in ["<dir>"]:
									formatting = ("windowwidget", "highlight")
								else:
									raise Exception(f"Unknown ptype {ptype}")
								file_list.append((widgetlineattrs.NORMAL, [(f"{filename}", formatting)], [(f"{ptype}", ("windowwidget", "description"))]))
							tmpselection = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, file_list, title = query, cursor = True)
							if tmpselection is None or tmpselection == (0, [("", None)]):
								# This will break out of the loop
								basedir = None
								selected_file = None
								continue
							selected_entry = tmpselection[1][0][0]
							# Find entry in tmp_file_list
							for item in tmp_file_list:
								realpath, filename, ptype = item
								if filename == selected_entry:
									if ptype in ["<dir>"]:
										basedir = realpath
										uip.refresh_all()
									elif ptype in ["File", "Configuration File", "Kustomization"]:
										selected_file = realpath
										selected_ptype = ptype
										basedir = None
										break

						if selected_file is None:
							uip.force_update()
							continue
						selection_vars["resource_path"] = (selected_file, selected_ptype)

				if confirm == True:
					if curses_helper.confirmationbox(uip.stdscr, uip.maxy // 2, uip.maxx // 2, title = f"Are you sure you want to perform “{description}“?", default = False) == False:
						continue

				actionfunc(uip, items, actionlist[action], values = selection_vars, kind = kind, title = title)

				uip.force_update()
				tagged_items.clear()
				continue
			tagged_items.clear()

		shortcuts = deep_get(viewref, "shortcuts", {})

		for key, value in shortcuts.items():
			shortcut_keys = deep_get(value, "shortcut")
			if shortcut_keys is None:
				continue

			if type(shortcut_keys) != list:
				shortcut_keys = [shortcut_keys]

			if c not in shortcut_keys:
				continue

			query = deep_get(value, "query")
			queryval = deep_get(value, "queryval")
			queryfunc = deep_get(value, "queryfunc")
			force_update = deep_get(value, "force_update", True)

			selection_vars = None

			# This should be modified; we might want to ask multiple queries.
			# At the very least asking for one input plus confirmation.
			# A multi input box might be useful too.
			if queryfunc is not None and queryval is not None and query is not None:
				if queryfunc == "string":
					# Not supported by inputbox widget
					# querydefault = ""
					string = curses_helper.inputbox(uip.stdscr, uip.maxy // 2, 1, uip.maxy - 1, uip.maxx - 1, "%s: " % query)
					if string is None or string == "":
						continue
					else:
						selection_vars = { queryval: string }
				elif queryfunc == "confirm":
					querydefault = False
					if curses_helper.confirmationbox(uip.stdscr, uip.maxy // 2, uip.maxx // 2, title = "%s?" % query, default = querydefault) == False:
						continue

			if deep_get(value, "confirm") == True:
				title = deep_get(value, "title", "")

				uip.force_update()
				if curses_helper.confirmationbox(uip.stdscr, uip.maxy // 2, uip.maxx // 2, title = title, default = False) == False:
					continue

			selected = uip.get_selected()
			call = deep_get(value, "call", None)
			if call is not None:
				if queryval is not None:
					retval = call(uip, selected, title, values = selection_vars)
				else:
					retval = call(uip.stdscr, selected.name)
				if retval is not None and retval == curses_helper.retval.RETURNFULL:
					return retval

			_eval = deep_get(value, "eval", None)
			if _eval is not None:
				expr = compile(_eval, "<string>", "exec")
				eval(expr)

			if force_update == True:
				uip.force_update()

def clusteroverviewloop(stdscr, view):
	field_list, sortcolumn = fieldgenerator(view)
	uip = UIProps(stdscr)

	windowheader = view
	helptext = views[view]["helptext"]
	activatedfun = views[view]["activatedfun"]
	update_delay = views[view].get("update_delay", -1)

	sortorder_reverse = views[view].get("sortorder_reverse", False)

	uip.init_window(field_list, windowheader = windowheader, update_delay = update_delay, sortcolumn = sortcolumn, sortorder_reverse = sortorder_reverse, helptext = helptext, activatedfun = activatedfun)

	infopadheight = 9

	# For generic information
	infopad = uip.init_infopad(height = infopadheight, width = -1, ypos = 1, xpos = 1)

	# For the status panes
	headerpad, listpad = uip.init_listpad(listheight = 1, width = -1, ypos = infopadheight + 2, xpos = 1, header = False)

	# For the status bar; position is always at the bottom of the screen and the entire width of the screen
	statusbar = uip.init_statusbar()

	selected_heatmap = "Node"
	selected_node = 0
	selected_pod = 0

	nodeinfo = None

	while True:
		uip.countdown_to_update()

		if uip.is_update_triggered():
			# The data in some fields might become shorter, so we need to trigger a clear
			uip.infopad.erase()
			uip.statusbar.erase()

			uip.update_window()

			# Get control plane node
			control_plane_node, control_plane_name = get_control_plane()
			control_plane_addresses = deep_get(control_plane_node, "status#addresses", [])
			name, iips, eips = get_node_addresses(control_plane_name, control_plane_addresses)

			# XXX: Once we can run Ansible jobs in the background we can replace this with the data from the load_ping playbook
			# Check whether we're running local or not; if we're running local this is easy
			if os.path.exists("/etc/hostname"):
				f = open("/etc/hostname", "r")
				hostname = f.readline().strip()
				islocal = (hostname == name)
				f.close()
			#else:
			#WTF do we do if /etc/hostname doesn't exist?

			if islocal == True:
				# FIXME: handle non-existance
				if os.path.exists("/proc/loadavg"):
					f = open("/proc/loadavg", "r")
					loadavg = f.readline()
					f.close()

					tmp = re.match(r"([0-9]+\.[0-9]+) ([0-9]+\.[0-9]+) ([0-9]+\.[0-9]+) ([0-9]+)/([0-9]+) ([0-9]+)", loadavg)
					if tmp is not None:
						avg1min = tmp[1]
						avg5min = tmp[2]
						avg15min = tmp[3]
						running = tmp[4]
						tasks = tmp[5]

				if os.path.exists("/proc/meminfo"):
					f = open("/proc/meminfo", "r")

					for tmp in f:
						values = tmp.split()
						if values[0] == "MemTotal:":
							memtotal = int(values[1])
						elif values[0] == "MemFree:":
							memfree = int(values[1])
						elif values[0] == "MemShared:":
							memshared = int(values[1])
						elif values[0] == "Buffers:":
							buffers = int(values[1])
						elif values[0] == "Cached:":
							memcached = int(values[1])
						elif values[0] == "SwapTotal:":
							swaptotal = int(values[1])
						elif values[0] == "SwapFree:":
							swapfree = int(values[1])
						elif values[0] == "Shmem:":
							memshared = int(values[1])
						elif values[0] == "SReclaimable:":
							sreclaimable = int(values[1])
					f.close()

					# Total used mem; includes buffers and cache
					memused = memtotal - memfree
					memcached = memcached + sreclaimable - memshared
					swapused = swaptotal - swapfree
					# FIXME
					threads = 42

				if os.path.exists("/proc/stat"):
					f = open("/proc/stat", "r")
					stat = f.readline()
					f.close()

				# cpu  30175835 25947 13018006 435764225 200633 0   3229829 0     0     0
				# ...  normal   niced system   idle      iowait irq softirq steal guest guest_nice
				#      1        2     3        4         5      6   7       8     9     10
				tmp = re.match(r"[A-Za-z]+ +([0-9]+) +([0-9]+) +([0-9]+) +([0-9]+) ([0-9]+) +([0-9]+) +([0-9]+) +([0-9]+) +([0-9]+) ([0-9]+)", stat)
				if tmp is not None:
					cputimeuser = int(tmp[1])
					cputimeusernice = int(tmp[2])
					cputimesystem = int(tmp[3])
					cputimeidle = int(tmp[4])
					cputimeiowait = int(tmp[5])
					cputimeirq = int(tmp[6])
					cputimesoftirq = int(tmp[7])
					cputimesteal = int(tmp[8])
					cputimeguest = int(tmp[9])
					cputimeguestnice = int(tmp[10])
					cputimeuser = cputimeuser - cputimeguest
					cputimeusernice = cputimeusernice - cputimeguestnice
					cputimetotalidle = cputimeidle + cputimeiowait
					cputimetotalsystem = cputimesystem + cputimeirq + cputimesoftirq
					cputimetotalguest = cputimeguest + cputimeguestnice
					cputimetotal = cputimeuser + cputimeusernice + cputimetotalsystem + cputimetotalidle + cputimesteal + cputimetotalguest
					cputimetotalused = cputimeuser + cputimeusernice + cputimetotalsystem + cputimesteal + cputimetotalguest

			# Get the list of all nodes
			vlist = kh.get_list_by_kind_namespace(("Node", ""), "")
			nodeinfo = get_node_info(vlist)
			node_statuses = [s.status_group for s in nodeinfo]

			# Get the list of all pods
			vlist = kh.get_list_by_kind_namespace(("Pod", ""), "")
			podinfo = get_pod_info(vlist)
			pod_statuses = [s.status_group for s in podinfo]

			node_heatmap = curses_helper.generate_heatmap(((uip.maxx - uip.minx - 1) // 2) - 1, node_statuses, selected_node)
			pod_heatmap = curses_helper.generate_heatmap(((uip.maxx - uip.minx - 1) // 2) - 1, pod_statuses, selected_pod)

			# Resize the list pad to fit the heatmaps
			# XXX this needs to be fixed when we add more information
			uip.resize_listpad(max(len(node_heatmap), len(pod_heatmap)), -1)

		namearray = [
			("Control Plane: ", ("main", "infoheader")),
			(f"{name}", ("types", "generic"))
		]
		contextarray = [
			("Cluster Context: ", ("main", "infoheader")),
			(f"{kh.context_name}", ("types", "generic"))
		]
		internaliparray = [
			("Internal IP-address(es): ", ("main", "infoheader")),
		]
		internaliparray += format_list(iips, 0, 0, False, False)
		externaliparray = [
			("External IP-address(es): ", ("main", "infoheader")),
		]
		externaliparray += format_list(eips, 0, 0, False, False)
		# Kubernetes port
		# KubeDNS
		# control plane load, disk, mem, uptime (requires ansible or local)

		uip.addthemearray(infopad, namearray, y = 0, x = 0)
		uip.addthemearray(infopad, contextarray, y = 1, x = 0)
		uip.addthemearray(infopad, internaliparray, y = 2, x = 0)
		uip.addthemearray(infopad, externaliparray, y = 3, x = 0)

		if islocal == True:
			tasksarray = [
				("Tasks: ", ("main", "infoheader")),
				(f"{tasks}", ("types", "numerical")),
				#(", ", ("types", "separator")),
				#(f"{threads} ", ("types", "numerical")),
				#("threads", ("types", "generic")),
				("; ", ("types", "separator")),
				(f"{running} ", ("types", "numerical")),
				("running", ("types", "generic")),
			]

			cpuarray = [
				(" CPU: ", ("main", "infoheader")),
			]
			cpuusagearray = [
				(f"{str(100 * cputimetotalused // cputimetotal).rjust(3)}", ("types", "numerical")),
				("separators", "percentage"),
			]
			memarray = [
				(" Mem: ", ("main", "infoheader")),
			]
			memusagearray = [
				("%s" % str(100 * memused // memtotal).rjust(3), ("types", "numerical")),
				("separators", "percentage"),
			]
			swaparray = [
				("Swap: ", ("main", "infoheader")),
			]
			disabledarray = [
				("Disabled", ("main", "highlight")),
			]

			uip.addthemearray(infopad, tasksarray, y = 5, x = 0)
			uip.addthemearray(infopad, cpuarray, y = 6, x = 0)
			uip.addthemearray(infopad, cpuusagearray, y = 6, x = (uip.infopadwidth - 8))
			uip.addthemearray(infopad, memarray, y = 7, x = 0)
			uip.addthemearray(infopad, memusagearray, y = 7, x = (uip.infopadwidth - 8))
			uip.addthemearray(infopad, swaparray, y = 8, x = 0)

			# cpu usage:
			# low-priority (bold blue) / normal (green) / kernel (red) / virtualized (cyan)
			# This is aggregate over all CPUs
			curses_helper.percentagebar(infopad, 6, 6, uip.infopadwidth - 10, cputimetotal, [
				(cputimeusernice, attr_to_curses_merged("types", "cputime_user_nice")),
				(cputimeuser, attr_to_curses_merged("types", "cputime_user")),
				(cputimetotalsystem, attr_to_curses_merged("types", "cputime_total_system")),
				(cputimetotalguest, attr_to_curses_merged("types", "cputime_total_guest"))
			])

			# memory usage:
			# used (green) / buffers (bold blue) / cache (yellow)
			curses_helper.percentagebar(infopad, 7, 6, uip.infopadwidth - 10, memtotal, [
				(memused - memcached - buffers, attr_to_curses_merged("types", "mem")),
				(buffers, attr_to_curses_merged("types", "buffers")),
				(memcached, attr_to_curses_merged("types", "cached"))
			])

			# swap usage:
			# used (red)
			if swaptotal == 0:
				uip.addthemearray(infopad, disabledarray, y = 8, x = 6)
			else:
				curses_helper.percentagebar(infopad, 8, 6, uip.infopadwidth - 10, swaptotal, [
					(swapused, ("types", "swap_used"))
				])
				swapusagearray = [
					(f"{100 * swapused // swaptotal}", ("main", "dim")),
					("separators", "percentage"),
				]
				uip.addthemearray(infopad, swapusagearray, y = 8, x = (uip.infopadwidth - 8))

		heatmap_width = 0
		if len(nodeinfo) > 0:
			heatmap_width = ((uip.maxx - uip.minx - 1) // 2) - 1
			node_heatmap_xpos = 0
			pod_heatmap_xpos = heatmap_width - 1
			uip.addthemearray(listpad, [("Node heatmap:", ("main", "listheader", selected_heatmap == "Node"))], y = 0, x = node_heatmap_xpos)
			selectednodearray = [
				("N", ("main", "listheader", selected_heatmap == "Node")),
				("ode: ", ("main", "listheader")),
				(f"{nodeinfo[selected_node].name}".ljust(heatmap_width), ("main", "highlight")),
			]
			selectednodestatusarray = [
				("Status: ", ("main", "infoheader")),
				(f"{nodeinfo[selected_node].status}".ljust(heatmap_width), color_status_group(nodeinfo[selected_node].status_group, selected = False)),
			]
			uip.addthemearray(listpad, selectednodearray, y = 1, x = node_heatmap_xpos + 1)
			uip.addthemearray(listpad, selectednodestatusarray, y = 2, x = node_heatmap_xpos + 1)

			node_heatmap = curses_helper.generate_heatmap(heatmap_width, node_statuses, selected_node)
			pod_heatmap = curses_helper.generate_heatmap(heatmap_width, pod_statuses, selected_pod)

			y = 5
			for row in node_heatmap:
				uip.addthemearray(listpad, row, y = y, x = node_heatmap_xpos)
				y += 1

			uip.addthemearray(listpad, [("Pod heatmap:", ("main", "listheader", selected_heatmap == "Pod"))], y = 0, x = pod_heatmap_xpos)
			selectedpodarray = [
				("P", ("main", "listheader", selected_heatmap == "Pod")),
				("od: ", ("main", "listheader")),
				(f"{podinfo[selected_pod].name}".ljust(heatmap_width), ("main", "highlight")),
			]
			selectedpodnamespacearray = [
				("N", ("main", "listheader", selected_heatmap == "Pod")),
				("amespace: ", ("main", "listheader")),
				(f"{podinfo[selected_pod].namespace}".ljust(heatmap_width), ("main", "highlight")),
			]
			selectedpodstatusarray = [
				("Status: ", ("main", "infoheader")),
				(f"{podinfo[selected_pod].status}".ljust(heatmap_width), color_status_group(podinfo[selected_pod].status_group, selected = False)),
			]
			uip.addthemearray(listpad, selectedpodarray, y = 1, x = pod_heatmap_xpos + 1)
			uip.addthemearray(listpad, selectedpodnamespacearray, y = 2, x = pod_heatmap_xpos + 1)
			uip.addthemearray(listpad, selectedpodstatusarray, y = 3, x = pod_heatmap_xpos + 1)

			y = 5
			for row in pod_heatmap:
				uip.addthemearray(listpad, row, y = y, x = pod_heatmap_xpos)
				y += 1

		uip.refresh_window()
		uip.refresh_infopad()
		uip.refresh_listpad()
		uip.refresh_statusbar()
		curses.doupdate()

		uip.stdscr.timeout(100)
		c = uip.stdscr.getch()

		if c == curses.KEY_RESIZE:
			uip.resize_window()
		elif c == ord(""):
			sys.exit()
		elif c == curses.KEY_F1 or c == ord("H"):
			if helptext is not None:
				curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, uip.helptext, title = "Help", cursor = False)
			uip.refresh_all()
		elif c == curses.KEY_F2:
			retval = selectwindow(stdscr, uip)
			if retval == curses_helper.retval.RETURNFULL:
				return retval
			uip.refresh_all()
		elif c == curses.KEY_F3:
			retval = selectwindow(stdscr, uip, refresh_apis = True)
			if retval == curses_helper.retval.RETURNFULL:
				return retval
			uip.refresh_all()
		elif c == ord("B"):
			uip.toggle_borders()
			uip.refresh_all()
			uip.force_update()
		elif c == curses.KEY_SRIGHT:
			if selected_heatmap == "Node":
				selected_heatmap = "Pod"
			uip.refresh_all()
		elif c == curses.KEY_SLEFT:
			if selected_heatmap == "Pod":
				selected_heatmap = "Node"
			uip.refresh_all()
		elif c == curses.KEY_LEFT:
			if selected_heatmap == "Pod":
				selected_pod = max(0, selected_pod - 1)
			elif selected_heatmap == "Node":
				selected_node = max(0, selected_node - 1)
			uip.refresh_all()
		elif c == curses.KEY_RIGHT:
			if selected_heatmap == "Pod":
				selected_pod = min(len(pod_statuses) - 1, selected_pod + 1)
			elif selected_heatmap == "Node":
				selected_node = min(len(node_statuses) - 1, selected_node + 1)
			uip.refresh_all()
		elif c == curses.KEY_UP:
			if selected_heatmap == "Pod":
				if (selected_pod - heatmap_width + 1) >= 0:
					selected_pod -= heatmap_width + 1
			elif selected_heatmap == "Node":
				if (selected_node - heatmap_width + 1) >= 0:
					selected_node -= heatmap_width + 1
		elif c == curses.KEY_DOWN:
			if selected_heatmap == "Pod":
				if (selected_pod + heatmap_width + 1) < len(pod_statuses):
					selected_pod += heatmap_width + 1
			elif selected_heatmap == "Node":
				if (selected_node + heatmap_width + 1) < len(node_statuses):
					selected_node += heatmap_width + 1
		elif c == curses.KEY_HOME:
			if selected_heatmap == "Pod":
				selected_pod = max((selected_pod // (heatmap_width + 1)) * (heatmap_width + 1), 0)
			elif selected_heatmap == "Node":
				selected_node = max((selected_node // (heatmap_width + 1)) * (heatmap_width + 1), 0)
		elif c == curses.KEY_END:
			if selected_heatmap == "Pod":
				selected_pod = min((selected_pod // (heatmap_width + 1)) * (heatmap_width + 1) + heatmap_width, len(pod_statuses) - 1)
			elif selected_heatmap == "Node":
				selected_node = min((selected_node // (heatmap_width + 1)) * (heatmap_width + 1) + heatmap_width, len(node_statuses) - 1)
		elif c == curses.KEY_SHOME:
			if selected_heatmap == "Pod":
				selected_pod = selected_pod - (selected_pod // (heatmap_width + 1)) * (heatmap_width + 1)
			elif selected_heatmap == "Node":
				selected_node = selected_node - (selected_node // (heatmap_width + 1)) * (heatmap_width + 1)
		elif c == curses.KEY_SEND:
			if selected_heatmap == "Pod":
				tmp = selected_pod - (selected_pod // (heatmap_width + 1)) * (heatmap_width + 1) + (len(pod_statuses) // (heatmap_width + 1)) * (heatmap_width + 1)
				if tmp < len(pod_statuses):
					selected_pod = tmp
			elif selected_heatmap == "Node":
				tmp = selected_node - (selected_node // (heatmap_width + 1)) * (heatmap_width + 1) + (len(node_statuses) // (heatmap_width + 1)) * (heatmap_width + 1)
				if tmp < len(node_statuses):
					selected_node = tmp
		elif c == ord("\t"):
			if selected_heatmap == "Pod":
				if selected_pod == len(podinfo) -1:
					continue

				for i in range(selected_pod + 1, len(podinfo)):
					if podinfo[i].status_group != stgroup.OK:
						selected_pod = i
						uip.force_update()
						break
			elif selected_heatmap == "Node":
				if selected_node == len(nodeinfo) - 1:
					continue

				for i in range(selected_node + 1, len(nodeinfo)):
					if nodeinfo[i].status_group != stgroup.OK:
						selected_node = i
						uip.force_update()
						break
		elif c == curses.KEY_BTAB:
			if selected_heatmap == "Pod":
				if selected_pod == 0:
					continue

				for i in range(1, selected_pod):
					if podinfo[selected_pod - i].status_group != stgroup.OK:
						selected_pod -= i
						uip.force_update()
						break
			elif selected_heatmap == "Node":
				if selected_node == 0:
					continue

				for i in range(1, selected_node):
					if nodeinfo[selected_node - i].status_group != stgroup.OK:
						selected_node -= i
						uip.force_update()
						break
		elif c == ord("P"):
			if selected_heatmap == "Pod":
				retval = resourceinfodispatch(stdscr, podinfo[selected_pod].ref, ("Pod", ""))
				if retval is not None and retval == curses_helper.retval.RETURNFULL:
					return retval
			uip.force_update()
		elif c == ord("N"):
			if selected_heatmap == "Pod":
				ref = kh.get_ref_by_kind_name_namespace(("Namespace", ""), podinfo[selected_pod].namespace, "")
				retval = resourceinfodispatch(stdscr, ref, ("Namespace", ""))
				if retval is not None and retval == curses_helper.retval.RETURNFULL:
					return retval
			elif selected_heatmap == "Node":
				retval = resourceinfodispatch(stdscr, nodeinfo[selected_node].ref, ("Node", ""))
				if retval is not None and retval == curses_helper.retval.RETURNFULL:
					return retval
			uip.force_update()
		elif c == ord("V"):
			win = curses_helper.notice(uip.stdscr, uip.maxy // 2, uip.maxx // 2, message = "Fetching package versions...")
			_package_versions = get_package_versions(control_plane_name)
			package_versions = []
			for package, version in _package_versions:
				if version == "N/A":
					versionattr = ("windowwidget", "dim")
				else:
					versionattr = ("windowwidget", "default")
				package_versions.append((widgetlineattrs.NORMAL, [(f"{package}", ("windowwidget", "default"))], [(f"{version}", versionattr)]))
			uip.refresh_window()
			uip.refresh_infopad()
			uip.refresh_listpad()
			uip.refresh_statusbar()
			curses.doupdate()
			if len(package_versions) == 0:
				continue
			title = "Package Versions:"
			headers = ["Package:", "Version:"]
			curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, package_versions, title = title, headers = headers, cursor = False)

cmdata_format = [
	# cm namespace, cm name prefix, cmdata prefix, cmdata suffix, dataformat
	# To do an exact match on cmdata set both cmdata prefix and cmdata suffix to the same string
	# (this will work unless you have a string that contains the same substring twice)
	("", "", "", ".crt", "CRT"),
	("", "", "", ".pem", "CRT"),
	("", "", "", "client-ca-file", "CRT"),
	("", "", "", ".ini", "INI"),
	("", "", "", ".json", "JSON"),
	("", "", "", ".sh", "Shell Script"),
	("", "", "", ".toml", "TOML"),
	("", "", "", ".xml", "XML"),
	("", "", "", ".yaml", "YAML"),
	("", "", "", ".yml", "YAML"),
	("", "canal-config", "cni_network_config", "", "JSON"),
	("", "", "fluentbit.conf", "", "FluentBit"),
	("istio-system", "istio", "", "", "YAML"),
	("", "k10-k10-metering-config", "", "", "YAML"),
	("", "kubeapps-clusters-config", "clusters.conf", "", "JSON"),
	("", "kubeapps-internal-kubeappsapis-configmap", "plugins.conf", "", "JSON"),
	("kube-public", "cluster-info", "kubeconfig", "", "YAML"),
	("kube-system", "antrea", "antrea-agent", "", "YAML"),
	("kube-system", "antrea", "antrea-controller", "", "YAML"),
	("kube-system", "antrea", "antrea-cni", "", "JSON"),
	("kube-system", "coredns", "Corefile", "", "CaddyFile"),
	("kube-system", "kubeadm-config", "", "", "YAML"),
	("kube-system", "kubelet-config", "", "", "YAML"),
	("kube-system", "kube-proxy", "", "config.conf", "YAML"),
	("kube-system", "scheduler-extender-policy", "policy.cfg", "", "JSON"),
	("", "kubeapps", "vhost.conf", "vhost.conf", "NGINX"),
	("", "kubeapps", "k8s-api-proxy.conf", "k8s-api-proxy.conf", "NGINX"),
	("", "linkerd-config", "values", "", "YAML"),
	("", "nfd-worker-conf", "nfd-worker.conf", "", "YAML"),
	("", "", "", ".py", "Python"),
	# Openshift
	("", "dns-default", "Corefile", "", "CaddyFile"),
	("", "v4-0-config-system-cliconfig", "v4-0-config-system-cliconfig", "", "JSON"),
	# Keep last; match everything that doesn't match anything
	("", "", "", "", "Text"),
]

def identify_cmdata(cmdata_name, cm_name, cm_namespace, data):
	if len(data) > 0:
		for match_cm_namespace, match_cm_name, match_cmdata_prefix, match_cmdata_suffix, dataformat in cmdata_format:
			if (len(match_cm_namespace) == 0 or match_cm_namespace == cm_namespace) and cm_name.startswith(match_cm_name) and cmdata_name.startswith(match_cmdata_prefix) and cmdata_name.endswith(match_cmdata_suffix):
				break
	else:
		dataformat = "Empty"
	return dataformat

def cmdataloop(stdscr, obj, view, info = None):
	uip = UIProps(stdscr)

	uip.init_window(field_list = None, windowheader = "Config Map Data", helptext = helptexts.configmapdata)

	# For generic information
	infopad = uip.init_infopad(height = 1, width = -1, ypos = 1, xpos = 1)

	# For the status bar; position is always at the bottom of the screen and the entire width of the screen
	statusbar = uip.init_statusbar()

	dataarray = [
		("Data: ", ("main", "infoheader")),
		(f"{obj.configmap}", ("types", "generic"))
	]
	uip.addthemearray(infopad, dataarray, y = 0, x = 0)

	# For the configmap data
	tspad, logpad = uip.init_logpad(width = -1, ypos = 3, xpos = 1, timestamps = False)
	raw_output = False

	while True:
		uip.countdown_to_update()

		if uip.is_update_triggered():
			messages = []

			# We cannot reliably detect yaml
			dataformat = identify_cmdata(obj.configmap, obj.cm_name, obj.cm_namespace, obj.data)
			if raw_output == True:
				dataformat = "__Raw"

			if dataformat in ["YAML", "JSON"]:
				messages = iktlib.format_yaml([obj.data])
			elif dataformat == "TOML":
				messages = iktlib.format_toml(split_msg(obj.data))
			elif dataformat == "CRT":
				messages = iktlib.format_crt(split_msg(obj.data))
			elif dataformat == "XML":
				messages = iktlib.format_xml(split_msg(obj.data))
			elif dataformat == "INI":
				messages = iktlib.format_ini(split_msg(obj.data))
			elif dataformat == "FluentBit":
				messages = iktlib.format_fluentbit(split_msg(obj.data))
			elif dataformat == "CaddyFile":
				messages = iktlib.format_caddyfile(split_msg(obj.data))
			elif dataformat == "NGINX":
				messages = iktlib.format_nginx(split_msg(obj.data))
			else:
				messages = []
				for line in split_msg(obj.data):
					messages.append([(line, ("types", "generic"))])
			uip.update_log_info(None, None, None, messages)
			uip.loglen = len(messages)
			uip.update_window()

		if uip.refresh:
			# The data in some fields might become shorter, so we need to trigger a clear
			uip.statusbar.erase()

			# Resize the logpad as needed
			uip.logpad.erase()
			#uip.resize_logpad(uip.maxy - uip.logpadypos - 2, maxlen)

			maxx = 0

			# Populate the logpad
			for y in range(0, min(uip.logpadheight, uip.loglen)):
				ypos, xpos = uip.addthemearray(logpad, messages[uip.yoffset + y], y = y, x = 0)
				maxx = max(maxx, xpos)
			uip.resize_logpad(uip.maxy - uip.logpadypos - 2, maxx)

			uip.refresh_window()
			uip.refresh_infopad()
			uip.refresh_logpad()
			uip.refresh_statusbar()
			curses.doupdate()
			uip.refresh = False

		uip.stdscr.timeout(100)
		c = uip.stdscr.getch()
		retval = uip.generic_keycheck(c)

		if retval == curses_helper.retval.MATCH:
			continue
		elif retval == curses_helper.retval.RETURNONE:
			return curses_helper.retval.RETURNDONE
		elif retval == curses_helper.retval.RETURNFULL:
			return retval

		if c == curses.KEY_F2:
			retval = selectwindow(stdscr, uip)
			if retval == curses_helper.retval.RETURNFULL:
				return retval
			uip.refresh_all()
		elif c == curses.KEY_F3:
			retval = selectwindow(stdscr, uip, refresh_apis = True)
			if retval == curses_helper.retval.RETURNFULL:
				return retval
			uip.refresh_all()
		elif c == ord("B"):
			uip.toggle_borders()
			uip.refresh_all()
			uip.force_update()
		elif c == ord("R"):
			raw_output = not raw_output
			uip.yoffset = 0
			uip.xoffset = 0
			uip.force_update()

def check_cni_updates(cni, current_version):
	new_version = None

	if cni == "weave":
		# GET /report -H 'Accept: application/json'
		# This should be the IP address of the control plane
		# XXX: We probably need to do this locally on the control plane (via ansible) rather than remotely connecting to the control plane
		weaveaddr = "127.0.0.1"

		conn = http.client.HTTPConnection(weaveaddr, 6784)
		headers = {
			"Accept": "application/json"
		}

		try:
			conn.request("GET", "/report", headers = headers)
			r1 = conn.getresponse()
		except ConnectionRefusedError:
			return "<Version check failed>"

		if r1.status == 200:
			weavestatus = json.loads(r1.read())
			if deep_get(weavestatus, "VersionCheck#Enabled", False) == True and deep_get(weavestatus, "VersionCheck#Success", False) == True:
				candidate_version = deep_get(weavestatus, "VersionCheck#NewVersion", None)
				if candidate_version is not None and versiontuple(current_version) < versiontuple(candidate_version):
					new_version = candidate_version
		conn.close()
	else:
		new_version = "<Update check not implemented>"

	return new_version

# FIXME
def networkinfoloop(stdscr, view):
	field_list, sortcolumn = fieldgenerator(view)
	uip = UIProps(stdscr)

	windowheader = view
	helptext = views[view]["helptext"]
	activatedfun = views[view]["activatedfun"]
	update_delay = views[view].get("update_delay", -1)

	uip.init_window(field_list, windowheader = windowheader, update_delay = update_delay, sortcolumn = sortcolumn, helptext = helptext, activatedfun = activatedfun)

	# For generic information
	infopad = uip.init_infopad(height = 9, width = -1, ypos = 1, xpos = 1)

	# For the status bar; position is always at the bottom of the screen and the entire width of the screen
	statusbar = uip.init_statusbar()

	candidate_version = ""

	while True:
		uip.countdown_to_update()

		if uip.is_update_triggered():
			# The data in some fields might become shorter, so we need to trigger a clear
			uip.infopad.erase()
			uip.statusbar.erase()

			uip.update_window()

			# Try to figure out which CNI we're using, if any
			_cni = kh.identify_cni()

			if len(_cni) == 0:
				cni = "<unknown>"
				cnistr = [(f"{cni}", color_status_group(stgroup.UNKNOWN, False))]
				cni_version = "N/A"
				cni_version_str = [(f"{cni_version}", color_status_group(stgroup.UNKNOWN, False))]
				cni_status = ("N/A", stgroup.UNKNOWN)
			elif len(_cni) == 1:
				cni = _cni[0][0]
				cnistr = [(f"{cni}", ("types", "generic"))]
				cni_version = _cni[0][1]
				cni_version_str = [(f"{cni_version}", ("types", "version"))]
				cni_status = _cni[0][2]
			else:
				cni = "<unknown>"
				cnistr = [(f"Could not uniquely identify CNI ", ("main", "status_not_ok")), ("(", ("types", "generic")), ("Candidates: ", ("main", "infoheader"))]
				for i in range(0, len(_cni)):
					cnistr += [(f"{_cni[i][0]}", ("types", "generic")), ("separators", "version"), (f"{_cni[i][1]}", ("types", "version"))]
					if i < len(_cni) - 1:
						cnistr.append(("separators", "list"))
				cnistr.append((")", ("types", "generic")))
				cni_version = "N/A"
				cni_version_str = [(f"{cni_version[0]}", color_status_group(stgroup.UNKNOWN, False))]
				cni_status = ("N/A", stgroup.UNKNOWN)

			cni_status_str = [(f"{cni_status[0]}", color_status_group(cni_status[1], False))]

			versionarray = [
				("Version: ", ("main", "infoheader")),
			]
			versionarray +=	cni_version_str
			candidateversionarray = [
				("Candidate version: ", ("main", "infoheader")),
			]

			new_version = check_cni_updates(cni, cni_version)
			if new_version is not None:
				candidateversionarray.append((f"{new_version}", ("types", "version")))
			else:
				candidateversionarray.append(("No newer version available", ("types", "generic")))


			namearray = [
				("Container Network Interface: ", ("main", "infoheader")),
			]
			namearray += cnistr
			statusarray = [
				("Status: ", ("main", "infoheader")),
			]
			statusarray += cni_status_str

			uip.addthemearray(infopad, namearray, y = 0, x = 0)
			uip.addthemearray(infopad, versionarray, y = 1, x = 0)
			uip.addthemearray(infopad, candidateversionarray, y = 2, x = 0)
			uip.addthemearray(infopad, statusarray, y = 3, x = 0)

		uip.refresh_window()
		uip.refresh_infopad()
		uip.refresh_statusbar()
		curses.doupdate()

		uip.stdscr.timeout(100)
		c = uip.stdscr.getch()
		retval = uip.generic_keycheck(c)

		if retval == curses_helper.retval.MATCH:
			continue
		elif retval == curses_helper.retval.RETURNONE:
			return curses_helper.retval.RETURNDONE
		elif retval == curses_helper.retval.RETURNFULL:
			return retval

		if c == curses.KEY_F2:
			retval = selectwindow(stdscr, uip)
			if retval == curses_helper.retval.RETURNFULL:
				return retval
			uip.refresh_all()
		elif c == curses.KEY_F3:
			retval = selectwindow(stdscr, uip, refresh_apis = True)
			if retval == curses_helper.retval.RETURNFULL:
				return retval
			uip.refresh_all()
		elif c == ord("B"):
			uip.toggle_borders()
			uip.refresh_all()
			uip.force_update()
		"""
		elif c == ord("U"): # and len(candidate_version) > 0:
			# If we don't recognise the CNI we cannot update it
			if cni == "Unknown":
				continue

			# Try to download and install an update for weave
			if curses_helper.confirmationbox(uip.stdscr, uip.maxy // 2, uip.maxx // 2, title = f"Update Weave to version {candidate_version}:", default = False):
				weaveupdateaddr = "https://cloud.weave.works"

				# Get the Kubernetes version
				# XXX: we can get this using the playbook
				args = [ "kubectl", "version", "-o", "yaml" ]
				result = subprocess.run(args, stdout = PIPE, stderr = PIPE, universal_newlines = True)
				kubeversionoutput = yaml.safe_load(result.stdout)
				kubeversion = deep_get(kubeversionoutput, "serverVersion#major") + "." + deep_get(kubeversionoutput, "serverVersion#minor")

				page = "/k8s/" + kubeversion + "/net.yaml"

				# FIXME: We need some way to handle no_proxy
				https_proxy = deep_get(iktconfig, "Network#https_proxy", "")
				if https_proxy is not None and https_proxy != "":
					pm = urllib3.ProxyManager(https_proxy)
				else:
					pm = urllib3.PoolManager()

				r1 = pm.request("GET", weaveupdateaddr + page)

				if r1.status == 200:
					# Try to get the new weave version, to do a final check
					# whether we need to update or not
					weaveupdate = yaml.safe_load(r1.data)
					new_version = ""
					for item in weaveupdate.get("items"):
						if item.get("kind") == "DaemonSet":
							containers = deep_get(item, "spec#template#spec#containers")

							for container in containers:
								if container.get("name") == "weave":
									tmp = container.get("image", "")
									new_version = re.sub(r".*:", r"", tmp)

					if new_version != "" and versiontuple(current_version) < versiontuple(candidate_version):
						f = tempfile.NamedTemporaryFile()
						f.write(r1.data)
						f.flush()

						# Apply the new configuration
						args = [ "kubectl" , "apply", "-f" ]
						executecommand_multiple(stdscr, f"Updating weave to {new_version}", args, [ f.name ], True)
						f.close()
				pm.clear()
			uip.refresh_all()
		"""

def field_processor_kind(obj, path, extra_path = None, fallback = None, formatting = None):
	match = True
	if path is None or type(path) != list or len(path) != 2 or obj is None:
		match = False
	else:
		resource = deep_get(obj, path[0])
		group = deep_get(obj, path[1])
		kind = kh.guess_kind((deep_get(obj, path[0]), deep_get(obj, path[1])))

	if match == False:
		if fallback is None:
			field = [("strings", "none")]
		else:
			field = fallback
	else:
		resource, group = kind
		if formatting is not None:
			field = __field_processor_list([resource, group], formatting = formatting)
		else:
			field = __field_processor_list([resource, group])
	return field

def field_processor_latest_api_version(obj, path, extra_path = None, fallback = None, formatting = None):
	match = True
	if path is None or type(path) != list or len(path) != 3 or obj is None:
		match = False
	else:
		resource = deep_get(obj, path[0])
		group = deep_get(obj, path[1])
		deprecated_api = deep_get(obj, path[2])
		if resource is None or len(resource) == 0:
			match = False
		else:
			kind = kh.guess_kind((deep_get(obj, path[0]), deep_get(obj, path[1])))
			latest_api = kh.get_latest_api(kind)

	if match == False:
		if fallback is None:
			field = [("strings", "none")]
		else:
			field = fallback
	else:
		if latest_api == deprecated_api:
			value = "No newer API known; the API might be deprecated"
			formatting = ("main", "status_not_ok")
			field = [(f"{value}", formatting)]
		else:
			group, version = latest_api.split("/")
			if formatting is not None:
				field = __field_processor_list([group, version], formatting = formatting)
			else:
				field = __field_processor_list([group, version])
	return field

# All field processors have to return format lists (aka lists of tuples)
def field_processor_empty(obj, path, extra_paths = None, fallback = None, formatting = None):
	value = deep_get(obj, path)

	if value is None or value == "" or value == -1:
		if fallback is None:
			field = [("strings", "none")]
		else:
			field = fallback
	else:
		if formatting is None:
			formatting = ("types", "generic")
		field = [(f"{value}", formatting)]

	return field

# All field processors have to return format lists (aka lists of tuples)
def field_processor_description(obj, path, extra_paths = None, fallback = None, formatting = None):
	value = deep_get(obj, path)

	if value is None or value == "" or value == -1:
		if fallback is None:
			field = [("strings", "none")]
		else:
			field = fallback
	else:
		# We're not OK with newlines, but " is fine
		value = value.replace("\n", "\\\n").replace("\\\"", "\"")
		if formatting is None:
			formatting = ("types", "generic")
		field = [(f"{value}", formatting)]

	return field

def __field_processor_list(values, formatting = None):
	prefix = deep_get(formatting, "prefix", [])
	field_colors = deep_get(formatting, "field_colors", [("types", "field")])
	item_separators = deep_get(formatting, "item_separators", [("separators", "list")])
	suffix = deep_get(formatting, "suffix")
	conditionals = deep_get(formatting, "conditionals", {})
	field = prefix

	i = 0

	for value in values:
		if i < len(field_colors):
			field_color = field_colors[i]
		# An empty value means that we want the separators listed, but no value
		if value != "":
			conditional = deep_get(conditionals, value)
			if conditional is None:
				tmpvalue = f"{value}:<<<{i}>>>"
				conditional = deep_get(conditionals, tmpvalue)
				if conditional is not None:
					value = tmpvalue
			# If we don't match the conditional, or the conditional doesn't apply to this field
			if conditional is None:
				# Check against matchany conditional
				conditional = deep_get(conditionals, "<<<matchany>>>")
				if conditional is None:
					tmpvalue = f"<<<matchany>>>:<<<{i}>>>"
					conditional = deep_get(conditionals, tmpvalue)
					if conditional is not None:
						value = tmpvalue

			if conditional is not None:
				skip = deep_get(conditional, "skip")
				substitute = deep_get(conditional, "substitute")
				stop = deep_get(conditional, "stop")
				field_color = deep_get(conditional, "field_color", field_color)

				if substitute is not None:
					value = substitute
				if skip is None:
					field.append((f"{value}", field_color))
				if stop is not None:
					break
			else:
				field.append((f"{value}", field_color))

		# If this is the last element in the list, append the suffix instead
		if i == len(values) - 1:
			item_separator = suffix
		elif i < len(item_separators):
			item_separator = item_separators[i]
		i += 1
		# Inserting an item_separator of None
		# means that these two items shouldn't be separated
		if item_separator is None:
			continue
		field.append(item_separator)
	return field

def field_processor_ansible_processor(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	# This assumes symmetrical processors
	data = deep_get(obj, "ansible_processor", "")
	if data is not None and len(data) > 0:
		value = data[2]
	else:
		value = "<unavailable>"
	if formatting is None:
		formatting = ("types", "generic")
	field = [(f"{value}", formatting)]
	return field

def field_processor_ansible_facts(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	values = []
	for path in extra_paths:
		values.append(str(deep_get(obj, path, "")))

	return __field_processor_list(values, formatting)

# Items in extra_paths are processed one by one,
# and printed according to formatting, as follows:
# [prefix]item1[separator(s)]item2[separator(s)]item...[suffix]
#
# path can be used as conditional; if path returns None or len(path) is 0,
# it will be substited with fallback
def field_processor_list(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	if path is not None:
		cpath = deep_get(obj, path)
		if cpath is None or len(cpath) == 0:
			return fallback

	values = []

	# Create a list of values
	if type(extra_paths) is str:
		values = deep_get(obj, extra_paths)
		if values is None:
			values = []
	else:
		for epath in extra_paths:
			if epath == "":
				value = ""
			else:
				value = deep_get(obj, epath)
				value = "<<<none>>>" if value is None else str(value)
			values.append(value)

	return __field_processor_list(values, formatting)

# The indata is a string that's been formatted as a list;
# Either
# "[key:value],[key:value],..."
# or
# "value,value,..."
# Trying to infer the format by parsing might not be reliable,
# so pass a flag in formatting to decide
def field_processor_str_to_list(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	if obj is None or path is None or len(path) == 0:
		if fallback is None:
			return [("strings", "unset")]
		else:
			return fallback

	iskeyvalue = False

	if formatting is not None:
		 iskeyvalue = deep_get(formatting, "iskeyvalue", False)

	tmp = deep_get(obj, path, "")

	# Start by splitting the string into substrings
	values = tmp.split(",")

	# If this is key:value pairs we need to split these strings again
	if iskeyvalue == True:
		values = list(s.split(":") for s in values)

	return __field_processor_list(values, formatting)

def field_processor_external_ips(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	external_ips, _ = datagetter_node_addresses(obj, "external", [])
	field = format_list(external_ips, 0, 0, False, False, field_colors = [("types", "ipaddress")])
	return field

def field_processor_internal_ips(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	internal_ips, _ = datagetter_node_addresses(obj, "internal", [])
	field = format_list(internal_ips, 0, 0, False, False, field_colors = [("types", "ipaddress")])
	return field

def field_processor_ipam_block(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	blocks = []

	for block, value in deep_get(obj, path, {}).items():
		# split the block to make the CIDR formatting nice and neat
		cidr = block.split("/")
		# XXX: Find out the value from the block is;
		#      it seems to be meaningless
		blocks.append((cidr[0], cidr[1])) #, value))

	# XXX: Once we know what value is used for we might add it here; for now don't do anything
	field = format_list(blocks, 0, 0, False, False, field_colors = [("types", "ipaddress"), ("types", "ipmask")], field_separators = [(("separators", "ipmask"))])
	return field

def field_processor_cidr(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	tmp = deep_get(obj, path, "").split("/")
	cidr = (tmp[0], tmp[1])

	# FIXME
	field = format_list(cidr, 0, 0, False, False, field_colors = [("types", "ipaddress"), ("types", "ipmask")], field_separators = [("separators", "ipmask")])
	return field

def field_processor_images(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	# This is to handle Prometheus
	if path is None:
		path = "spec#image"
	base_image = deep_get(obj, "spec#baseImage")
	image = deep_get(obj, path, base_image)

	if image is not None:
		name, version = get_image_tuple(image)
		if version == "<undefined>":
			field = format_list((name, version), 0, 0, False, False, field_colors = [("types", "generic"), ("types", "undefined")], field_separators = [("separators", "version")])
		else:
			field = format_list((name, version), 0, 0, False, False, field_colors = [("types", "generic"), ("types", "version")], field_separators = [("separators", "version")])
	else:
		field = [("strings", "none")]
	return field

def field_processor_percentage(obj, path, extra_paths = None, fallback = None, formatting = None):
	value = deep_get(obj, path)

	if value is None or value == "" or value == -1:
		if fallback is None:
			field = [("strings", "none")]
		else:
			field = fallback
	else:
		if formatting is None:
			formatting = ("types", "generic")
		field = [
			(f"{str(value)}", formatting),
			("separators", "percentage"),
		]

	return field

def field_processor_priority_level(obj, path, extra_paths = None, fallback = None, formatting = None):
	value = deep_get(obj, path)

	if value is None or value == "":
		if fallback is None:
			field = [("strings", "none")]
		else:
			field = fallback
	else:
		obj = kh.get_ref_by_kind_name_namespace(("PriorityLevelConfiguration", "flowcontrol.apiserver.k8s.io"), value, "")
		if obj is None:
			field = [
				(f"{value}", ("main", "status_not_ok")),
				(" (Referenced PriorityLevelConfiguration object does not exist)", ("types", "generic")),
			]
		else:
			if formatting is None:
				formatting = ("types", "generic")
			field = [(f"{value}", formatting)]

	return field

def field_processor_timestamp(obj, path, extra_paths = None, fallback = None, formatting = None):
	timestamp = deep_get(obj, path)

	if timestamp is None:
		field = [("strings", "unset")]
	elif type(timestamp) == datetime:
		field = format_timestamp(timestamp)
	else:
		timestamp = timestamp_to_datetime(timestamp)
		field = format_timestamp(timestamp, localtimezone = True)

	return field

def field_processor_age(obj, path, extra_paths = None, fallback = None, formatting = None):
	timestamp = deep_get(obj, path)

	if timestamp is None:
		field = [("strings", "unset")]
	else:
		# XXX: this should highlight units
		duration = iktlib.seconds_to_age(timestamp)
		field = format_numerical_with_units(duration, "age", False)

	return field

def field_processor_default_storage_class(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	default = "false"

	annotations = deep_get(obj, "metadata#annotations")

	if annotations is not None:
		default = deep_get(annotations, "storageclass.kubernetes.io/is-default-class", "false")

	if default == "false":
		field = [("False", ("types", "generic"))]
	else:
		field = [("True", ("types", "generic"))]

	return field

def field_processor_csi_node(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	ref = kh.get_ref_by_kind_name_namespace(("CSINode", "storage.k8s.io"), deep_get(obj, "metadata#name"), "")
	csinodename = deep_get(ref, "metadata#name")
	if csinodename is None:
		field = fallback
	else:
		field = [(f"{csinodename}", ("types", "generic"))]

	return field

def field_processor_ingress_backend(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	# In ingress v1 backend is called defaultBackend
	path = "spec#backend"

	# FIXME: defaultBackend can also be a resource (apiGroup, kind, name) instead of (name:port)
	if "defaultBackend" in deep_get(obj, "spec"):
		path = "spec#defaultBackend"

	extra_paths = [f"{path}#serviceName", f"{path}#servicePort"]

	return field_processor_list(obj, path = path, extra_paths = extra_paths, fallback = fallback, formatting = formatting)

def field_processor_grace_period(obj, path, extra_paths = None, fallback = None, formatting = None):
	grace_period = deep_get(obj, path)

	if grace_period is None:
		field = [
			("Default (", ("types", "generic")),
			("30", ("types", "numerical")),
			("s", ("types", "unit")),
			(")", ("types", "generic")),
		]
	elif grace_period == 0:
		field = [("Immediately", ("types", "generic"))]
	else:
		field = [
			(f"{grace_period}", ("types", "numerical")),
			("s", ("types", "unit")),
		]
	return field

def field_processor_host_ports(obj, path, extra_paths = None, fallback = None, formatting = None):
	host_ports = deep_get(obj, "spec#hostPorts", [])

	if len(host_ports) == 0:
		field = [("strings", "none")]
	else:
		ports = []
		for portrange in host_ports:
			ports.append(f"{deep_get(portrange, 'min')}-{deep_get(portrange, 'max')}")
		field = __field_processor_list(ports, formatting = formatting)

	return field

def field_processor_host_paths(obj, path, extra_paths = None, fallback = None, formatting = None):
	host_paths = deep_get(obj, "spec#allowedHostPaths", [])

	if len(host_paths) == 0:
		field = [("All", ("types", "generic"))]
	else:
		paths = []
		for epath in host_paths:
			suffix = ""
			if deep_get(epath, "readyOnly", False) == True:
				suffix = " [Read Only]"
			paths.append(f"{deep_get(epath, 'pathPrefix')}{suffix}")
		field = __field_processor_list(paths, formatting = formatting)

	return field

def field_processor_conditions(obj, path, extra_paths = None, fallback = None, formatting = None):
	conditions = get_conditions(deep_get(obj, path))
	field = []

	completed = False

	for condition, state, reason, message in conditions:
		# These are inverted; False means that everything is fine, True means things are bad
		# Showing them when they're causing issues feels wrong
		if condition in ["NetworkUnavailable", "MemoryPressure", "DiskPressure", "PIDPressure", "Degraded", "OutOfDisk"]:
			if state == "False":
				continue
			elif state == "True":
				state = "False"
		# If we're not progressing because we're already done then this condition isn't important
		elif condition == "Progressing" and state == "False" and reason == "AllComponentsReady":
			continue
		elif state == "False":
			condition = f"!{condition}"

		# If the pod has completed we need no other conditions
		if reason == "PodCompleted":
			completed = True
			break

		if state == "False":
			status_group = stgroup.NOT_OK
		elif state == "True":
			status_group = stgroup.OK
		elif state == "Unknown":
			condition = f"?{condition}"
			status_group = stgroup.UNKNOWN

		field.append((f"{condition} ", color_status_group(status_group)))

	if completed == True:
		field = [("PodCompleted", color_status_group(stgroup.DONE))]

	return field

def field_processor_condition_ready(obj, path, extra_paths = None, fallback = None, formatting = None):
	ready, message, status_group = get_condition_ready(obj)

	if message is not None and len(message) > 0:
		message = f" ({message})"
	return [(ready, color_status_group(status_group, False)), (message, ("types", "generic"))]

def field_processor_conditions_certsignreq(obj, path, extra_paths = None, fallback = None, formatting = None):
	conditions = get_certsignreq_conditions(deep_get(obj, path))
	formatting = {
		"conditionals": {
			"Approved": {
				"field_color": ("main", "status_ok"),
			},
			"Issued": {
				"field_color": ("main", "status_ok"),
			},
			"Denied": {
				"field_color": ("main", "status_not_ok"),
			},
		},
	}

	return __field_processor_list(conditions, formatting = formatting)

def __get_issuer_info(obj):
	crl_distribution_points = []
	ca_secret_name = ""
	if deep_get(obj, "spec#selfSigned") is not None:
		itype = "Self-Signed"
		crl_distribution_points = deep_get(obj, "spec#selfSigned#crlDistributionPoints", [])
	elif deep_get(obj, "spec#ca") is not None:
		itype = "CA"
		ca_secret_name = deep_get(obj, "spec#ca#secretName", "")
		crl_distribution_points = deep_get(obj, "spec#ca#crlDistributionPoints", [])
	elif deep_get(obj, "spec#vault") is not None:
		itype = "Vault"
	elif deep_get(obj, "spec#venafi") is not None:
		itype = "Venafi"
	elif deep_get(obj, "spec#external") is not None:
		itype = "External"
	elif deep_get(obj, "spec#acme") is not None:
		itype = "ACME"
	else:
		itype = "Unknown"
	return itype, crl_distribution_points, ca_secret_name

def field_processor_issuer_type(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	itype, crl_distribution_points, ca_secret_name = __get_issuer_info(obj)
	field = [(f"{itype}", ("types", "generic"))]
	return field

def field_processor_crl_distribution_points(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	itype, crl_distribution_points, ca_secret_name = __get_issuer_info(obj)
	if crl_distribution_points is None or len(crl_distribution_points) == 0:
		field = fallback
	else:
		field = __field_processor_list(crl_distribution_points, formatting)
	return field

def field_processor_ca_secret_name(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	itype, crl_distribution_points, ca_secret_name = __get_issuer_info(obj)
	if ca_secret_name is None or len(ca_secret_name) == 0:
		field = fallback
	else:
		field = [(f"{ca_secret_name}", ("types", "generic"))]
	return field

# All fields except obj will be ignored
def field_processor_api_service_available(obj, path, extra_paths = None, fallback = None, formatting = None):
	available, service, status, status_group = get_as_status_and_service(deep_get(obj, "status#conditions"), deep_get(obj, "spec#service"))
	return [(available, ("types", "generic"))]

# All fields except obj and path will be ignored
def field_processor_namespace_status(obj, path, extra_paths = None, fallback = None, formatting = None):
	status = deep_get(obj, path)
	if status == "Active":
		status_group = stgroup.OK
	elif status == "Terminating":
		status_group = stgroup.PENDING
	else:
		status_group = stgroup.UNKNOWN
	return [(status, color_status_group(status_group, False))]

# All fields except obj will be ignored
def field_processor_api_service_status(obj, path, extra_paths = None, fallback = None, formatting = None):
	available, service, status, status_group = get_as_status_and_service(deep_get(obj, "status#conditions"), deep_get(obj, "spec#service"))
	return [(status, color_status_group(status_group, False))]

# All fields except obj will be ignored
def field_processor_node_status(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	status, status_group, taints, full_taints = get_node_status(obj)
	return [(f"{status}", color_status_group(status_group, False))]

def field_processor_key_value(obj, path, extra_paths = None, fallback = None, formatting = None):
	items = deep_get(obj, path, {})

	if len(items) == 0:
		field = [("strings", "none")]
	else:
		key_value = []
		for item in items:
			key_value.append((item, items[item]))
		field = format_list(key_value, 0, 0, False, False, field_colors = [("types", "key"), ("types", "value")], field_separators = [(("separators", "keyvalue"))])
	return field

def field_processor_taints(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	status, status_group, taints, full_taints = get_node_status(obj)
	if len(full_taints) == 0:
		field = [("strings", "none")]
	else:
		# FIXME
		field = format_list(full_taints, 0, 0, False, False, field_colors = [("types", "key"), ("types", "value")], field_separators = [(("separators", "keyvalue"))])
	return field

def field_processor_component_kinds(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	kinds = []

	for item in deep_get(obj, "spec#componentKinds", []):
		group = deep_get(item, "group", "")
		kind = deep_get(item, "kind", "")
		kinds.append((group, kind))

	if len(kinds) == 0:
		field = [("strings", "none")]
	else:
		field = format_list(kinds, 0, 0, False, False, field_colors = [("types", "kind"), ("types", "api_group")], field_separators = [("separators", "kind_apigroup")])
	return field

# fallback will be ignored
def field_processor_holder(obj, owr_path = None, holder_path = None, fallback = None, formatting = None):
	if owr_path is None:
		owr_path = "metadata#ownerReferences"
	if holder_path is None:
		holder_path = "spec#holderIdentity"

	holder_name = deep_get(obj, holder_path)
	owner_references = deep_get(obj, owr_path)
	holder_kind = get_holder_kind_from_owner_references(obj)
	field = format_list((holder_kind, holder_name), 0, 0, False, False, field_colors = [("types", "kind"), ("types", "generic")], field_separators = [("separators", "kind")])
	return field

# All fields except obj and owr_path will be ignored
def field_processor_controller(obj, owr_path = None, extra_paths = None, fallback = None, formatting = None):
	if owr_path is None:
		owr_path = "metadata#ownerReferences"

	owr = deep_get(obj, owr_path)
	show_kind = deep_get(formatting, "show_kind", "mixed")
	controller = get_controller_from_owner_references(owr)
	if controller is None or controller == (("", ""), ""):
		if fallback is not None:
			field = fallback
		else:
			field = [("strings", "none")]
	else:
		controller = format_controller(controller, show_kind)
		field = format_list(controller, 0, 0, False, False, field_colors = [("types", "kind"), ("types", "generic")], field_separators = [("separators", "kind")])
	return field

# All fields except obj and owr_path will be ignored
def field_processor_owr_node(obj, owr_path = None, extra_paths = None, fallback = None, formatting = None):
	if owr_path is None:
		owr_path = "metadata#ownerReferences"

	owner_references = deep_get(obj, owr_path)
	field = format_list(get_name_by_kind_from_owner_references(owner_references, "Node"), 0, 0, False, False, field_colors = [("types", "kind"), ("types", "generic")], field_separators = [("separators", "kind")])
	return field

known_pv_types = {
	"awsElasticBlockStore": {
		"type": "AWS Elastic Block Storage",
		"description": "Represents a Persistent Disk resource in AWS",
		"properties": {
			"Volume ID:": { "path": "volumeID" },
			"Partition #:": { "path": "partition" },
			"Filesystem Type:": { "path": "fsType", "default": "ext4" },
			"Read Only:": { "path": "readyOnly", "default": False },
		},
	},
	"azureDisk": {
		"type": "Azure Disk",
		"description": "Azure Data Disk mount on the host and bind mount to the pod",
		"properties": {
			"Disk Name:": { "path": "diskName" },
			"Disk URI:": { "path": "diskURI" },
			"Filesystem Type:": { "path": "fsType", "default": "ext4" },
			"Read Only:": { "path": "readyOnly", "default": False },
			"Caching Mode:": { "path": "cachingMode" },
			"Kind:": { "path": "kind", "default": "shared" },
		},
	},
	"azureFile": {
		"type": "Azure File",
		"description": "Azure File Service mount on the host and bind mount to the pod",
		"properties": {
			"Share Name:": { "path": "shareName" },
			"Read Only:": { "path": "readyOnly", "default": False },
			# These two should combine to a shortcut to a secret; needs formatting + helper
			"Secret Name:": { "path": "secretName" },
			"Secret Namespace:": { "path": "secretNamespace", "default": "<pod namespace>" },
		},
	},
	"cephfs": {
		"type": "Ceph",
		"properties": {
			"Path:": { "path": "path", "default": "/" },
			"Monitors:": { "path": "monitors", "processor": field_processor_list },
			"Read Only:": { "path": "readOnly", "default": "False" },
			"Rados User": { "path": "user", "default": "admin" },
			# Should be a shortcut to a secret; needs formatting
			#"Secret:": { "path": "secretRef" },
			"Secret File:": { "path": "secretFile", "default": "/etc/ceph/user.secret" },
		},
	},
	"cinder": {
		# Deprecated
		"type": "OpenStack Cinder Volume",
		"properties": {
			"Volume ID:": { "path": "volumeID" },
			"Filesystem Type:": { "path": "fsType", "default": "ext4" },
			"Read Only:": { "path": "readOnly", "default": "False" },
			# Should be a shortcut to a secret; needs formatting
			#"Secret:": { "path": "secretRef" },
		},
	},
	"csi": {
		"type": "External CSI Volume",
		"description": "Storage managed by an external CSI volume driver",
		"properties": {
			"Volume Handle:": { "path": "volumeHandle" },
			"Driver:": { "path": "driver" },
			"Filesystem Type:": { "path": "fsType" },
			#{ "path": "controllerExpandSecretRef" },
			#{ "path": "controllerPublishSecretRef" },
			#{ "path": "nodeExpandSecretRef" },
			#{ "path": "nodePublishSecretRef" },
			"Read Only:": { "path": "readOnly" },
			#{ "path": "volumeAttributes" }, #dict(str, str)
		},
	},
	"fc": {
		"type": "Fibre Channel Volume",
		"properties": {
			"Filesystem Type:": { "path": "fsType", "default": "ext4" },
			"WorldWide Identifiers:": { "path": "wwids", "processor": field_processor_list },
			"Target WorldWide Names:": { "path": "targetWWNs", "processor": field_processor_list },
			"Logical Unit Number:": { "path": "lun" },
			"Read Only:": { "path": "readOnly", "default": "False" },
		},
	},
	"flexVolume": {
		"type": "FlexPersistentVolumeSource",
		"description": "Generic persistent volume resource provisioned/attached using an exec based plugin",
		"properties": {
			"Driver:": { "path": "driver" },
			"Filesystem Type:": { "path": "fsType", "default": "<script dependent>" },
			"Read Only:": { "path": "readOnly", "default": "False" },
			# Should be a shortcut to a secret; needs formatting
			#"Secret:": { "path": "secretRef" },
			"Options": { "path": "options", "default": {} },
		},
	},
	"flocker": {
		"type": "Flocker Volume",
		"description": "Flocker Volume mounted by the Flocker agent",
		"properties": {
			"Dataset Name:": { "path": "datasetName" },
			"Dataset UUID:": { "path": "datasetUUID" },
		},
	},
	"gcePersistentDisk": {
		"type": "GCE Persistent Disk",
		"description": "Google Compute Engine Persistent Disk resource",
		"properties": {
			"PD Name:": { "path": "pdName" },
			"Partition:": { "path": "partition" },
			"Filesystem Type:": { "path": "fsType", "default": "ext4" },
			"Read Only:": { "path": "readOnly", "default": "False" },
		},
	},
	"glusterfs": {
		"type": "GlusterFS",
		"description": "Glusterfs mount that lasts the lifetime of a pod",
		"properties": {
			"Path:": { "path", "path" },
			"Endpoints:": { "path": "endpoints" },
			"Endpoints Namespace:": { "path": "endpoints", "default": "<PVC namespace>" },
			"Read Only:": { "path": "readOnly", "default": "False" },
		},
	},
	"hostPath": {
		# Only works in single-node clusters
		"type": "Host Path",
		"description": "Host path mapped into a pod",
		"properties": {
			"Path:": { "path": "path" },
			"Host Path Type:": { "path": "type", "default": "" },
		},
	},
	"iscsi": {
		"type": "iSCSI Disk",
		"properties": {
			"iSCSI Qualified Name:": { "path": "iqn" },
			"Logical Unit Number:": { "path": "lun" },
			"Target Portal:": { "path": "targetPortal" },
			"Target Portals:": { "path": "targetPortals", "processor": field_processor_list },
			"Filesystem Type:": { "path": "fsType", "default": "ext4" },
			"Chap Auth Discovery:": { "path": "chapAuthDiscovery" },
			"Chap Auth Session:": { "path": "chapAuthSession" },
			"iSCSI Initiator:": { "path": "initiatorName" },
			"iSCSI Interface:": { "path": "iscsiInterface", "default": "tcp" },
			"Read Only:": { "path": "readOnly", "default": "False" },
			# Should be a shortcut to a secret; needs formatting
			#"Secret:": { "path": "secretRef" },
		},
	},
	"local": {
		"type": "Local",
		"description": "Directly-attached storage with node affinity",
		"properties": {
			"Path:": { "path": "path" },
			"Filesystem Type:": { "path": "fsType", "default": "<auto-detect>" },
		},
	},
	"nfs": {
		"type": "NFS",
		"description": "NFS mount that lasts the lifetime of a pod",
		"properties": {
			"Server:": { "path": "server" },
			"Path:": { "path": "path" },
			"Read Only:": { "path": "readOnly", "default": "False" },
		},
	},
	"portworxVolume": {
		"type": "Portworx volume",
		"properties": {
			"Volume ID:": { "path": "volumeID" },
			"Filesystem Type:": { "path": "fsType", "default": "ext4" },
			"Read Only:": { "path": "readOnly", "default": "False" },
		},
	},
	"quobyte": {
		"type": "Quobyte Mount",
		"description": "Quobyte mount that lasts the lifetime of a pod",
		"properties": {
			"Volume Name:": { "path": "volume" },
			"Registry:": { "path": "registry", "processor": field_processor_str_to_list, "formatting": { "iskeyvalue": True, "field_separators": [("separators", "host")] } }, # str(host:port, host:port, ...)
			"Read Only:": { "path": "readOnly", "default": "False" },
			"Tenant:": { "path": "tenant" },
			"User:": { "path": "user", "default": "<service account user>" },
			"Group:": { "path": "group", "default": None },
		},
	},
	"rbd": {
		"type": "RBD",
		"description": "Rados Block Device mount that lasts the lifetime of a pod",
		"properties": {
			"Image:": { "path": "image" },
			"Pool:": { "path": "pool", "default": "rbd" },
			"Filesystem Type:": { "path": "fsType", "default": "ext4" },
			"Monitors:": { "path": "monitors", "processor": field_processor_list },
			"Read Only:": { "path": "readOnly" },
			"Rados User": { "path": "user", "default": "admin" },
			"Keyring:": { "path": "keyring", "default": "/etc/ceph/keyring" },
			# Should be a shortcut to a secret; needs formatting
			#"Secret:": { "path": "secretRef" },
		},
	},
	"scaleIO": {
		# Deprecated
		"type": "Persistent ScaleIO Volume",
		"properties": {
			"Volume Name:": { "path": "volumeName" },
			"Gateway:": { "path": "gateway" },
			"Storage Pool:": { "path": "storagePool" },
			"Storage System:": { "path": "system" },
			"Storage Mode:": { "path": "storageMode", "default": "ThinProvisioned" },
			"Filesystem Type:": { "path": "fsType", "default": "xfs" },
			"Protection Domain:": { "path": "protectionDomain" },
			"SSL Enabled:": { "path": "sslEnabled", "default": "False" },
			"Read Only:": { "path": "readOnly" },
			# Should be a shortcut to a secret; needs formatting
			#"Secret:": { "path": "secretRef" },
		},
	},
	"storageos": {
		"type": "Persistent StorageOS Volume",
		"properties": {
			"Volume Name:": { "path": "volumeName" },
			"Volume Namespace:": { "path": "volumeNamespace", "default": "<pod namespace>" },
			"Filesystem Type:": { "path": "fsType", "default": "ext4" },
			"Read Only:": { "path": "readOnly" },
			# Should be a shortcut to a secret; needs formatting
			#"Secret:": { "path": "secretRef" },
		},
	},
	"vsphereVolume": {
		"type": "vSphere Volume",
		"properties": {
			"Volume Path:": { "path": "volumePath" },
			"Filesystem Type:": { "path": "fsType", "default": "ext4" },
			"Storage Policy ID:": { "path": "storagePolicyID" },
			"Storage Policy Name:": { "path": "storagePolicyName" },
		},
	},
}

def get_pv_type(obj):
	for pv_type, pv_data in known_pv_types.items():
		if pv_type in deep_get(obj, "spec", []):
			return pv_type
	return None

# All fields except obj will be ignored
def field_processor_pv_type(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	pvtype = "<unknown>"

	pvtype = get_pv_type(obj)
	if pvtype is not None:
		pvtype = known_pv_types[pvtype]["type"]
	else:
		pvtype = "<unknown>"

	return [(f"{pvtype}", ("types", "generic"))]

# All fields except obj will be ignored
def field_processor_pv_status(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	status, status_group = get_pv_status(obj)
	return [(f"{status}", color_status_group(status_group, False))]

# All fields except obj will be ignored
def field_processor_pvc_status(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	status, status_group = get_pvc_status(obj)
	return [(f"{status}", color_status_group(status_group, False))]

# All fields except obj will be ignored
def field_processor_pod_status(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	status, status_group = get_pod_status(obj)
	msg = deep_get(obj, "status#message", "").rstrip()
	field = [(f"{status}", color_status_group(status_group, False))]
	if len(msg) > 0:
		field.append((f" ({msg})", color_status_group(status_group, False)))
	return field

# All fields except obj and path will be ignored
def field_processor_dep_condition(obj, path, extra_paths = None, fallback = None, formatting = None):
	conditions = deep_get(obj, path)
	status, status_group, reason = get_dep_status(conditions)
	field = [
		(f"{status}", color_status_group(status_group, False)),
		(f" ({reason})", ("types", "generic"))
	]
	return field

# extra_paths will be ignored
def field_processor_access_modes(obj, path, extra_paths = None, fallback = None, formatting = None):
	access_modes = []
	for mode in deep_get(obj, path, []):
		access_modes.append(abbreviate_access_mode(mode))

	if len(access_modes) == 0:
		if fallback is None:
			field = [("strings", "none")]
		else:
			field = fallback
	else:
		field = format_list(access_modes, 0, 0, False, False)
	return field

def field_processor_replica_status(obj, path, extra_paths, fallback = None, formatting = None):
	path1 = extra_paths[0]
	path2 = extra_paths[1]

	value1 = deep_get(obj, path1)
	value2 = deep_get(obj, path2)

	field = [
		(f"{value1}", ("types", "numerical")),
		(" Current ", ("types", "generic")),
		(("separators", "fraction")),
		(f" {value2}", ("types", "numerical")),
		(" Desired", ("types", "generic")),
	]

	return field

def field_processor_replication_controller_status(obj, path, extra_paths, fallback = None, formatting = None):
	current_replicas = deep_get(obj, "status#replicas")
	ready_replicas = deep_get(obj, "status#readyReplicas")
	desired_replicas = deep_get(obj, "spec#replicas")
	field = [
		(f"{current_replicas}", ("types", "numerical")),
		(f" Current ", ("types", "generic")),
		(("separators", "fraction")),
		(f" {ready_replicas}", ("types", "numerical")),
		(f" Ready ", ("types", "generic")),
		(("separators", "fraction")),
		(f" {desired_replicas}", ("types", "numerical")),
		(f" Desired", ("types", "generic")),
	]
	return field

def field_processor_replica_set_status(obj, path, extra_paths, fallback = None, formatting = None):
	current_replicas = deep_get(obj, "status#replicas")
	desired_replicas, max_replicas = get_desired_replicas(obj)
	field = [
		(f"{current_replicas}", ("types", "numerical")),
		(f" Current ", ("types", "generic")),
		(("separators", "fraction")),
		(f" {desired_replicas}", ("types", "numerical")),
		(f" Desired ", ("types", "generic")),
		(("separators", "fraction")),
		(f" {max_replicas}", ("types", "numerical")),
		(f" Max", ("types", "generic")),
	]
	return field

# All fields except obj and path are ignored
def field_processor_secret_type(obj, path, extra_paths, fallback = None, formatting = None):
	secret_type = deep_get(obj, "type", "")

	formatting = {
		"field_colors": [("types", "secret_type"), ("types", "generic")],
		"item_separators": [("separators", "secret_type")],
	}

	tmp = re.match(r"^(.*)/(.*)", secret_type)

	if tmp is not None:
		secret_type = [tmp[1], tmp[2]]
		field = __field_processor_list(secret_type, formatting = formatting)
	elif len(secret_type) > 0:
		field = [("", ("types", "generic")), (f"{secret_type}", ("types", "generic"))]
	else:
		field = [("", ("types", "generic")), ("strings", "unset")]
	return field

# All fields except obj and path are ignored
def field_processor_phase_status(obj, path, extra_paths, fallback = None, formatting = None):
	phase = deep_get(obj, path)
	if phase == "Error":
		status_group = stgroup.NOT_OK
	else:
		status_group = stgroup.OK
	field = [(f"{phase}", color_status_group(status_group, False))]
	return field

def field_processor_label_selector(obj, path, extra_paths, fallback = None, formatting = None):
	selector = deep_get(obj, path)
	if selector is not None:
		field = []
		selector = kh.make_selector(selector)
		split_selector = selector.split(",")
		for i in range(0, len(split_selector)):
			field.append((split_selector[i], ("types", "generic")))
			if i < len(split_selector) - 1:
				field.append(("separators", "selector_list"))
	else:
		field = [("strings", "none")]
	return field

def field_processor_match_expressions(obj, path, extra_paths, fallback = None, formatting = None):
	match_expressions = deep_get(obj, path)
	if match_expressions is not None:
		field = [(make_set_expression(match_expressions), ("types", "generic"))]
	else:
		field = [("strings", "none")]
	return field

# everything except obj and path will be ignored
def field_processor_tls(obj, path, extra_paths = None, fallback = None, formatting = None):
	tls = deep_get(obj, path)

	if tls is None:
		field = [("strings", "none")]
	else:
		secretname = deep_get(tls[0], "secretName")
		hosts = deep_get(tls[0], "hosts")

		if secretname is None or hosts is None:
			field = [("strings", "fixme")]
		else:
			field = [
				(secretname, ("main", "highlight")),
				(" terminates ", ("types", "generic")),
			]
			field += format_list(hosts, 0, 0, False, False, field_colors = [("types", "host")])

	return field

def field_processor_job_completion(obj, path, extra_paths = None, fallback = None, formatting = None):
	start_time = timestamp_to_datetime(deep_get(obj, "status#startTime"))
	completion_time = timestamp_to_datetime(deep_get(obj, "status#completionTime"))

	if completion_time is None:
		state, status_group = get_job_state(obj)
		field = [(state, color_status_group(status_group, False))]
	else:
		timediff = completion_time - start_time
		duration = timediff.days * 24 * 60 * 60 + timediff.seconds
		field = [("%s (Duration: %s)" % (completion_time.strftime("%Y-%m-%d %H:%M:%S"), iktlib.seconds_to_age(duration)), ("types", "generic"))]
	return field

def field_processor_svc_ports(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	ports = get_svc_ports(obj)
	field = format_list(ports, 0, 0, False, False, field_colors = [("types", "service"), ("types", "port"), ("types", "protocol")], field_separators = [("separators", "port"), ("separators", "service")])
	return field

def field_processor_endpoints(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	svcname = deep_get(obj, "metadata#name")
	svcnamespace = deep_get(obj, "metadata#namespace")
	ref = kh.get_ref_by_kind_name_namespace(("Endpoints", ""), svcname, svcnamespace)
	endpointsnamespace = deep_get(ref, "metadata#namespace")
	endpointsname = deep_get(ref, "metadata#name")
	field = [
		(f"{endpointsnamespace}", ("types", "namespace")),
		("separators", "namespace"),
		(f"{endpointsname}", ("types", "generic"))
	]
	return field

def field_processor_endpoint_slices(obj, path = None, extra_paths = None, fallback = None, formatting = None):
	tmp = get_endpoint_slices(obj)

	field = []
	for item in tmp:
		field += [
			(item[0], ("types", "namespace")),
			("separators", "namespace"),
			(item[1], ("types", "generic")),
			("separators", "list")
		]
	# Remove the trailing comma
	if len(field) > 0:
		field.pop(len(field) - 1)

	return field

def export_data(filename, value):
	vtype, value = decode_value(value)

	if vtype.startswith("string"):
		f = open(filename, "w")
		f.write(value)
	elif vtype.startswith("base64-binary") or vtype.startswith("gzip"):
		f = open(filename, "wb")
		f.write(base64.b64decode(value))
	elif vtype.startswith("base64-utf-8"):
		f = open(filename, "w")
		f.write(base64.b64decode(value).decode("utf-8"))
	else:
		raise Exception(f"Trying to export unknown vtype: {vtype}")
	f.close()

def callout_obj(uip, kind, obj, **kwargs):
	if "selection" in kwargs:
		_discard_field, namespace_field, name_field = deep_get(kwargs, "selection")
		namespace = namespace_field[0][0]
		name = name_field[0][0]
		obj = kh.get_ref_by_kind_name_namespace(kind, name, namespace)
		return resourceinfodispatch(uip.stdscr, obj, kind, **kwargs)
	# Add other ways to override obj here later on
	else:
		return resourceinfodispatch(uip.stdscr, obj, kind, **kwargs)

def callout_show_package_versions(uip, kind, obj, **kwargs):
	win = curses_helper.notice(uip.stdscr, uip.maxy // 2, uip.maxx // 2, message = "Fetching package versions...")
	host_name = deep_get(obj, "metadata#name")
	package_versions = get_package_versions(host_name)
	uip.refresh_window()
	uip.refresh_infopad()
	uip.refresh_listpad()
	uip.refresh_statusbar()
	curses.doupdate()
	if len(package_versions) == 0:
		return
	title = "Package Versions:"
	headers = ["Package:", "Version:"]
	curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, package_versions, title = title, headers = headers, cursor = False)

def generate_helptext(view, viewtype, additional_helptexts = []):
	helptext = []

	if viewtype == "infoview":
		helptext += helptexts.infoviewheader

		if view not in infoviews:
			helptext += helptexts.spacer
			helptext += additional_helptexts
			return helptext

		viewref = infoviews[view]

		if "listpad" in viewref:
			if deep_get(viewref, "activatedfun") is not None:
				helptext.append(("[Enter]", "Open info page for selected resource"))

		if len(additional_helptexts) > 0:
			helptext += helptexts.spacer
			helptext += additional_helptexts

		if "shortcuts" in viewref:
			for shortcut in deep_get(viewref, "shortcuts"):
				tmp = deep_get(viewref, f"shortcuts#{shortcut}#helptext")
				if tmp is not None:
					helptext.append(tmp)

		if "listpad" in viewref:
			helptext += helptexts.spacer

			if deep_get(viewref, "reversible", True) == False:
				helptext += helptexts.irreversiblelistmovement
			else:
				helptext += helptexts.listmovement
		elif "logpad" in viewref:
			helptext += helptexts.spacer
			helptext += helptexts.logmovement
	elif viewtype == "listview":
		helptext += helptexts.listviewheader

		viewref = views[view]

		if viewref["kind"] in infoviews:
			helptext += helptexts.openresource

		if "shortcuts" in viewref:
			for shortcut in deep_get(viewref, "shortcuts"):
				tmp = deep_get(viewref, f"shortcuts#{shortcut}#helptext")
				if tmp is not None:
					helptext.append(tmp)
			helptext += helptexts.spacer

		if deep_get(viewref, "is_taggable", True) == True:
			helptext += helptexts.tagactions
			helptext += helptexts.spacer
			if deep_get(viewref, "labels", True) == True:
				helptext += helptexts.selectoractions
				helptext += helptexts.spacer

		helptext += helptexts.listmovement

	return helptext

def genericinfoloop(stdscr, obj, view, **kwargs):
	global initial_container

	# If obj is a tuple we got the object as obj[0]
	# params as obj[1]
	if type(obj) is tuple:
		objparams = obj[1]
		obj = obj[0]
	else:
		objparams = None

	uip = UIProps(stdscr)

	if view not in infoviews:
		return curses_helper.retval.NOMATCH

	viewref = infoviews[view]

	objgetter = deep_get(viewref, "objgetter")
	if objgetter is not None:
		obj = objgetter(obj)
		if obj is None or len(obj) == 0:
			return curses_helper.retval.NOMATCH

	fields = deep_get(viewref, "fields")
	sortcolumn = deep_get(viewref, "sortcolumn")
	field_blacklist = deep_get(viewref, "field_blacklist", [])
	field_list, sortcolumn = fieldgenerator(None, fields = fields, sortcolumn = sortcolumn, blacklist = field_blacklist)

	# If no window header has been specified that overrides the default, just use the view name with " Info" tacked on at the end
	if "title" in kwargs:
		windowheader = deep_get(kwargs, "title")
	else:
		windowheader = deep_get(viewref, "windowheader", f"{view[0]} Info")
	sortorder_reverse = deep_get(viewref, "sortorder_reverse", False)
	reversible = deep_get(viewref, "reversible", True)
	activatedfun = deep_get(viewref, "activatedfun")
	labels = deep_get(viewref, "labels", "metadata#labels")
	annotations = deep_get(viewref, "annotations", "metadata#annotations")
	extraref = deep_get(viewref, "extraref")
	data = deep_get(viewref, "data", None)
	name_path = deep_get(viewref, "name_path", "metadata#name")
	namespace_path = deep_get(viewref, "namespace_path", "metadata#namespace")
	creation_timestamp_path = deep_get(viewref, "creation_timestamp_path", "metadata#creationTimestamp")
	if data is not None:
		data = obj
	viewoverride = deep_get(viewref, "viewoverride", view)
	infopadheight = 0
	if "infopad" in viewref:
		# Number of fields added conditionally
		if name_path is not None and len(name_path) > 0 and deep_get(obj, name_path) is not None:
			infopadheight += 1
		if namespace_path is not None and len(namespace_path) > 0 and deep_get(obj, namespace_path) is not None:
			infopadheight += 1
		if creation_timestamp_path is not None and len(creation_timestamp_path) > 0:
			infopadheight += 1
		# Number of custom fields
		infopadheight += len(deep_get(viewref, "infopad", {}))
		# If we *only* have an infopad, then it should cover at least the entire screen; if not it should be as big as needed;
		# it doesn't really matter if we allocate an infopad larger than the screen, but this should be correct.
		if "listpad" not in viewref and "logpad" not in viewref:
			maxyx = stdscr.getmaxyx()
			infopadheight = max(infopadheight, maxyx[0] - 3)

	shortcuts = deep_get(viewref, "shortcuts", {})

	# Shortcuts
	# Always include the shortcut for namespaces, unless overriden
	# or the namespace_path returns None
	#
	# An override is necessary if metadata#namespace isn't the correct path
	# or some other changes are necessary
	if deep_get(shortcuts, "Namespace") is None and deep_get(obj, namespace_path) is not None:
		shortcuts["Namespace"] = {
			"shortcut": ord("N"),
			"helptext": ("[Shift] + N", "Open info page for namespace"),
			"call": resourceinfodispatch,
			"kind": ("Namespace", ""),
			"name_path": "metadata#namespace",
		}

	# Always include the shortcut for security contexts,
	# unless the path returns None
	if "securityContext" in deep_get(obj, "spec", {}) or "securityContext" in deep_get(obj, "spec#template#spec", {}):
		shortcuts["Show Security Context"] = {
			"shortcut": ord("x"),
			"helptext": ("X", "Show security context information"),
			"widget": "windowwidget",
			"title": "Security Context Policies:",
			"headers": ["Policy:", "Value:"],
			"itemgetter": get_security_context,
		}

	# Always include the shortcut for conditions,
	# unless the path returns None
	if "conditions" in deep_get(obj, "status", {}):
		shortcuts["Show Resource Conditions"] = {
			"shortcut": ord("c"),
			"helptext": ("C", "Show resource conditions"),
			"widget": "windowwidget",
			"title": "Conditions:",
			"headers": ["Type:", "Status:", "Last Probe:", "Last Transition:", "Message:"],
			"itemgetter": __get_conditions,
		}

	# Always include the shortcut for events unless overriden
	if deep_get(viewref, "shortcuts#Show Events") is None:
		shortcuts["Show Events"] = {
			"shortcut": ord("e"),
			"helptext": ("E", "Show events"),
			"widget": "windowwidget",
			"title": "Events:",
			"headers": ["Namespace:", "Name:", "Last Seen:", "Status:", "Reason:", "Source:", "First Seen:", "Count:", "Message:"],
			"itemgetter": __get_events,
		}

	# Always include the shortcut for YAML dump, unless overridden
	if deep_get(viewref, "shortcuts#YAML") is None:
		shortcuts["View YAML dump of resource"] = {
			"shortcut": ord("y"),
			"helptext": ("Y", "View YAML dump of resource"),
			"widget": "callout",
			"title": "YAML dump",
			"callout": infoview_view_yaml,
		}

	# Always include the shortcut for last applied configuration, unless overridden
	if deep_get(viewref, "shortcuts#Last Applied Configuration") is None:
		shortcuts["Last Applied Configuration"] = {
			"shortcut": ord("L"),
			"helptext": ("[Shift] + L", "Show last applied configuration"),
			"widget": "callout",
			"title": "Last applied configuration",
			"callout": infoview_view_last_applied_configuration,
		}

	# Conditional helptexts
	additional_helptexts = []
	if deep_get(obj, labels) is not None:
		additional_helptexts += helptexts.labels
	if deep_get(obj, annotations) is not None:
		additional_helptexts += helptexts.annotations

	helptext = generate_helptext(view, "infoview", additional_helptexts)

	uip.init_window(field_list, windowheader = windowheader, view = viewoverride, sortcolumn = sortcolumn, sortorder_reverse = sortorder_reverse, reversible = reversible, helptext = helptext, activatedfun = activatedfun, extraref = extraref, data = obj)

	# XXX: Should we calculate y-pos based on needed space?

	# For generic information
	if infopadheight > 0:
		infopad = uip.init_infopad(height = infopadheight, width = -1, ypos = 1, xpos = 1, labels = deep_get(obj, labels), annotations = deep_get(obj, annotations))
	else:
		infopad = None

	# For the status bar; position is always at the bottom of the screen and the entire width of the screen
	statusbar = uip.init_statusbar()

	# For lists
	headerpad = None
	listpad = None
	if deep_get(viewref, "listpad") is not None:
		if infopadheight == 0:
			headerpad, listpad = uip.init_listpad(listheight = 1, width = -1, ypos = 1, xpos = 1)
		else:
			headerpad, listpad = uip.init_listpad(listheight = 1, width = -1, ypos = infopadheight + 2, xpos = 1)

	# For log pads; we cannot have both a logpad and a listpad simultaneously
	logpad = deep_get(viewref, "logpad")
	if logpad is not None:
		if listpad is not None:
			raise Exception("We cannot have listpad and logpad simultaneously")

		timestamps = deep_get(viewref, "timestamps", True)
		if infopadheight == 0:
			tspad, logpad = uip.init_logpad(width = -1, ypos = 1, xpos = 1, timestamps = timestamps)
		else:
			tspad, logpad = uip.init_logpad(width = -1, ypos = infopadheight + 2, xpos = 1, timestamps = timestamps)

	while True:
		uip.countdown_to_update()

		# Output listpad if we have one
		if uip.is_update_triggered():
			# The data in some fields might become shorter, so we need to trigger a clear
			if infopad is not None:
				uip.infopad.erase()
			uip.statusbar.erase()

			# Refresh obj whenever we reload, unless this is a special view
			if not view[0].startswith("__"):
				obj = kh.get_ref_by_kind_name_namespace(view, deep_get(obj, "metadata#name"), deep_get(obj, "metadata#namespace"))

				if obj is None:
					title = "Error!"
					errormsg = [(0,
						[("Resource not available; it may have been deleted", ("types", "generic"))])]
					curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, errormsg, title = title, cursor = False)
					return curses_helper.retval.RETURNDONE

			uip.update_window()

			if listpad is not None:
				listref = deep_get(viewref, "listpad#listref")
				listgetter = deep_get(viewref, "listpad#listgetter")
				infogetter = deep_get(viewref, "listpad#infogetter")
				infogetter_filters = deep_get(viewref, "listpad#infogetter_filters", None)
				filters = None

				if infogetter_filters is not None:
					filters = []
					if infogetter_filters is not None:
						for key, value in infogetter_filters:
							filters.append((eval(key), eval(value)))
				fields = deep_get(viewref, "fields", [])

				if listref is not None:
					vlist = deep_get(obj, listref)
				elif listgetter is not None:
					if type(listgetter) == str:
						vlist = eval(listgetter)
					else:
						vlist = listgetter(obj)
				else:
					vlist = obj

				if infogetter == generic_infogetter:
					info = infogetter(vlist, fields)
				else:
					if filters is not None:
						info = infogetter(vlist, filters = filters)
					else:
						info = infogetter(vlist)

				listlen = uip.update_info(info)
				linelen = update_field_widths(field_list, uip.info)
				uip.resize_listpad(listlen, linelen)
			elif logpad is not None:
				infogetter = deep_get(viewref, "logpad#infogetter")
				if infogetter is not None:
					messages = infogetter(obj)
					uip.update_log_info(None, None, None, messages)
					uip.loglen = len(messages)

			if infopad is not None:
				y = 0
				if name_path is not None and len(name_path) > 0:
					fieldarray = [
						("Name:", ("main", "infoheader")),
						(f" {deep_get(obj, name_path)}", ("types", "generic"))
					]
					uip.addthemearray(infopad, fieldarray, y = y, x = 0)
					y += 1
				if namespace_path is not None and len(namespace_path) > 0 and deep_get(obj, namespace_path) is not None:
					fieldarray = [
						("N", ("main", "infoheader_shortcut")), ("amespace:", ("main", "infoheader")),
						(f" {deep_get(obj, namespace_path)}", ("types", "generic"))
					]
					uip.addthemearray(infopad, fieldarray, y = y, x = 0)
					y += 1

				for key, value in deep_get(viewref, "infopad", {}).items():
					fieldarray = deep_get(value, "fieldname", [(key, ("main", "infoheader"))])
					uip.addthemearray(infopad, fieldarray, y = y, x = 0)

					path = deep_get(value, "path")
					extra_paths = deep_get(value, "extra_paths")
					fallback = deep_get(value, "fallback")
					if "processor" in value:
						field_processor = deep_get(value, "processor")
					else:
						field_processor = field_processor_empty
					formatting = deep_get(value, "formatting")

					# objparam will replace fallback; this is too ugly for words
					if objparams is not None:
						objparam = deep_get(value, "objparam")
						if objparam is not None and len(objparams) > objparam:
							tmp = objparams[objparam]
							fallback = [(f"{tmp}", ("types", "generic"))]

					# If we neither get a path nor a field processor to use on the object,
					# we only output the field array; to achieve this pass None as processor.
					# If processor is omitted field_processor_empty() will be used by default.
					if field_processor is None and path is None:
						y += 1
						continue

					if field_processor is None:
						val = deep_get(obj, path, "")
						fieldarray = [(" %s" % val, ("types", "generic"))]
					else:
						fieldarray = [(" ", ("types", "generic"))]
						fieldarray += field_processor(obj, path, extra_paths, fallback, formatting)
					uip.addthemearray(infopad, fieldarray, y = y)
					y += 1

				if creation_timestamp_path is not None and len(creation_timestamp_path) > 0:
					fieldarray = [
						("Created: ", ("main", "infoheader")),
					]
					fieldarray += field_processor_timestamp(obj, creation_timestamp_path)
					uip.addthemearray(infopad, fieldarray, y = y, x = 0)

		# Output listpad if we have one
		if listpad is not None:
			y = 0

			uip.update_sorted_list()
			generate_listheader(uip, headerpad, field_list)
			for item in uip.sorted_list:
				uip.select_if_y(y, item)
				generate_list(uip, listpad, item, field_list, y, uip.is_selected(item))
				y += 1

		# Output logpad if we have one
		if logpad is not None and uip.refresh:
			uip.logpad.erase()
			maxx = 0
			for y in range(0, min(uip.logpadheight, len(messages))):
				ypos, xpos = uip.addthemearray(logpad, messages[uip.yoffset + y], y = y, x = 0)
				maxx = max(maxx, xpos)
			uip.resize_logpad(uip.maxy - uip.logpadypos - 2, maxx)

		uip.refresh_window()
		uip.refresh_infopad()
		uip.refresh_listpad()
		uip.refresh_logpad()
		uip.refresh_statusbar()
		curses.doupdate()
		uip.refresh = False

		# XXX: Handle initial containers and configmaps
		if initial_container is not None:
			match = None

			if len(uip.info) > 0:
				match_count = 0

				for item in uip.info:
					# If we're dealing with a config map we're interested in rtype == "Text",
					# If we're dealing with a pod we're interested in containers
					if hasattr(item, "kind") and item.kind in [("Container", ""), ("InitContainer", "")]:
						name = item.ref["name"]
						ikind = item.kind
					elif hasattr(item, "rtype") and item.rtype in [rtype[4] for rtype in cmdata_format]:
						name = item.configmap
						ikind = ("ConfigMap", "")
					else:
						continue

					# If we have an exact match we don't care about partial matches
					if name == initial_container:
						match = item.ref
						match_count = 1
						break
					elif name.startswith(initial_container):
						# Since the exact match might occur later than the partial we cannot abort
						# on partial matches; instead just save the match and continue searching.
						# If we get more than one partial match we ignore the partial matches.
						if match_count == 0:
							match = item.ref
						match_count += 1

				initial_container = None

				if match is not None and match_count == 1:
					retval = uip.activatedfun(uip.stdscr, match, ikind, info = obj)
					if retval == curses_helper.retval.RETURNFULL:
						return retval
					uip.force_update()
					uip.refresh_all()
					continue

		uip.stdscr.timeout(100)
		c = uip.stdscr.getch()
		retval = uip.generic_keycheck(c)

		if retval == curses_helper.retval.MATCH:
			continue
		elif retval == curses_helper.retval.RETURNONE:
			return curses_helper.retval.RETURNDONE
		elif retval == curses_helper.retval.RETURNFULL:
			return retval

		if c == curses.KEY_F2:
			retval = selectwindow(stdscr, uip)
			if retval == curses_helper.retval.RETURNFULL:
				return retval
			uip.refresh_all()
		elif c == curses.KEY_F3:
			retval = selectwindow(stdscr, uip, refresh_apis = True)
			if retval == curses_helper.retval.RETURNFULL:
				return retval
			uip.refresh_all()
		elif c == ord("B"):
			uip.toggle_borders()
			uip.refresh_all()
			uip.force_update()

		for key, value in shortcuts.items():
			shortcut_keys = deep_get(value, "shortcut")
			if shortcut_keys is None:
				continue

			if type(shortcut_keys) != list:
				shortcut_keys = [shortcut_keys]

			if c not in shortcut_keys:
				continue

			widget = deep_get(value, "widget")
			force_update = deep_get(value, "force_update", True)
			changed = False
			if widget is not None:
				if widget == "windowwidget":
					w_title = deep_get(value, "title", "")
					w_headers = deep_get(value, "headers")
					w_itemgetter = deep_get(value, "itemgetter")
					w_itemgetter_args = deep_get(value, "itemgetter_args", {})
					w_selectable = deep_get(value, "selectable", False)
					w_callout = deep_get(value, "callout", None)
					w_kind = deep_get(value, "kind", view)
					w_items = w_itemgetter(obj, **w_itemgetter_args)

					# If the first element is an integer we assume that we've been provided
					# a pre-formatted list. Otherwise we apply formatting if available.
					# If not available we try to provide some sensible defaults.
					if w_items is not None and len(w_items) > 0 and type(w_items[0][0]) != int:
						tmp_items = []
						w_formatting = deep_get(value, "formatting", [("windowwidget", "default")])
						# w_item is a line
						for w_item in w_items:
							tmp = [widgetlineattrs.NORMAL]
							# w_item[i] is a column
							for i in range(0, len(w_item)):
								tmp.append([(w_item[i], w_formatting[min(i, len(w_formatting) - 1)])])

							tmp_items.append(tuple(tmp))
						w_items = tmp_items

					if w_items is not None and len(w_items) > 0:
						w_sortcolumn = deep_get(value, "sortcolumn")
						tmpselection = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, items = w_items, headers = w_headers, title = w_title, cursor = w_selectable)
						if w_selectable is True and w_callout is not None:
							retval = w_callout(uip, w_kind, obj, selection = tmpselection)
							if retval is not None and retval == curses_helper.retval.RETURNFULL:
								return retval
						changed = True
				elif widget == "inputbox":
					selected = uip.get_selected()
					w_title = deep_get(value, "inputtitle", "")
					w_result = curses_helper.inputbox(uip.stdscr, uip.maxy // 2, 1, uip.maxy - 1, uip.maxx - 1, w_title)
					if w_result is None or len(w_result) == 0 or w_title == "" or len(selected.value) == 0:
						continue
					# This is necessary because we never go through the normal update cycle for the listpad and infopad
					uip.refresh_infopad()
					uip.refresh_listpad()
					uip.refresh_logpad()
					uip.refresh_statusbar()
					curses.doupdate()
					# If there's no confirm function we default to False
					w_confirm = eval(deep_get(value, "confirm", True))
					if w_confirm is True:
						curses.doupdate()
						w_confirmtitle = deep_get(value, "confirmtitle")
						if curses_helper.confirmationbox(uip.stdscr, uip.maxy // 2, uip.maxx // 2, title = w_confirmtitle, default = False) == False:
							continue
					w_call = deep_get(value, "call")
					w_call(w_result, selected.value)
				elif widget == "executecommand":
					selected = uip.get_selected()
					w_kinds = deep_get(value, "kinds")
					if (w_kinds is None):
						continue
					if w_kinds != ["<native>"] and (selected is None or not selected.kind in w_kinds) and w_kinds != [("", "")]:
						continue

					w_inputtitle = deep_get(value, "inputtitle", "")
					if w_inputtitle is not None:
						w_input = curses_helper.inputbox(uip.stdscr, uip.maxy // 2, 1, uip.maxy - 1, uip.maxx - 1, title = w_inputtitle)
						w_command = w_input.split()
					else:
						w_command = deep_get(value, "command", [])
					if len(w_command) == 0:
						continue

					w_waitforkeypress = deep_get(value, "waitforkeypress", False)
					if type(selected) == ResourceInfo:
						# This needs to be generalised somehow; so does the message
						containername = selected.ref["name"]
					elif not (w_kinds == ["<native>"] and w_command == ["<dnsutils>"]):
						sys.exit(f"Cannot handle type {type(selected)} in executecommand();\nw_kinds={w_kinds}\nw_command={w_command}")
					if w_command == [ "<ephemeral>" ]:
						ephemeral_image = deep_get(iktconfig, "Debug#ephemeral_image", "busybox")
						msg = [(f"Creating ephemeral ", "action"), (ephemeral_image, "programname"), (" container sharing process namespace with ", "action"), (f"{containername}", "path")]
					elif w_command == [ "<dnsutils>" ]:
						node_name = deep_get(obj, "metadata#name")
						containername = None
						msg = [(f"Opening dnsutils container on ", "action"), (node_name, "hostname")]
					else:
						msg = [(f"Executing ", "action"), (w_command[0], "programname"), (" inside ", "action"), (containername, "path")]
					executecommand(uip.stdscr, obj, containername, msg, command = w_command, waitforkeypress = w_waitforkeypress)
					changed = True
				elif widget == "callout":
					selected = uip.get_selected()
					w_title = deep_get(value, "title", "")
					w_headers = deep_get(value, "headers")
					w_itemgetter = deep_get(value, "itemgetter")
					w_sortcolumn = deep_get(value, "sortcolumn")
					w_callout = deep_get(value, "callout")
					w_callout_args = deep_get(value, "callout_args", {})
					w_kind = deep_get(value, "kind", view)
					# Try to be as generic as we can;
					# pass all kinds of information and let the handler use whatever information it needs
					retval = w_callout(uip, w_kind, obj, selected = selected, title = w_title, headers = w_headers, itemgetter = w_itemgetter, sortcolumn = w_sortcolumn, **w_callout_args)
					if retval is not None and retval == curses_helper.retval.RETURNFULL:
						return retval
			else:
				call = deep_get(value, "call")
				call_name = None
				owner_references_path = deep_get(value, "owner_references_path")
				owner_references_kind = deep_get(value, "owner_references_kind")
				holder_identity_path = deep_get(value, "holder_identity_path")
				if owner_references_path is not None:
					owner_reference = deep_get(obj, owner_references_path, [])
					if holder_identity_path is not None:
						if len(owner_reference) == 0:
							continue
						call_name = deep_get(obj, holder_identity_path)
						kind = get_holder_kind_from_owner_references(owner_reference, call_name)
					elif owner_references_kind is not None:
						call_name = get_name_by_kind_from_owner_references(owner_reference, owner_references_kind)
						kind = owner_references_kind
					else:
						kind, call_name = get_controller_from_owner_references(owner_reference)
					if kind == ("", "") or len(call_name) == 0:
						continue
				else:
					call_name_path = deep_get(value, "name_path")
					kind = deep_get(value, "kind")
					if kind is None:
						kind_path = deep_get(value, "kind_path", "")
						kind = deep_get(obj, kind_path)
					if call_name_path is not None:
						call_name = deep_get(obj, call_name_path)

				call_namespace_path = deep_get(value, "namespace_path")
				call_namespace = ""
				if call_namespace_path is not None:
					call_namespace = deep_get(obj, call_namespace_path)

				call_kind_name_namespace_selection_path = deep_get(value, "call_kind_name_namespace_selection_path")
				if call_kind_name_namespace_selection_path is not None:
					kind_filter = deep_get(value, "kind_filter", None)
					selected = uip.get_selected()

					if kind_filter is None or selected.kind in kind_filter:
						knn_order = deep_get(value, "call_kind_name_namespace_selection_order")
						knn = eval(f"selected.{call_kind_name_namespace_selection_path}")
						if knn_order[0] != -1:
							kind = knn[knn_order[0]]
						if knn_order[1] != -1:
							call_name = knn[knn_order[1]]
						if knn_order[2] != -1:
							call_namespace = knn[knn_order[2]]

				if call is not None and call_name is not None:
					if kind is None or kind == ("", ""):
						retval = call(uip.stdscr, call_name)
						if retval is not None and retval == curses_helper.retval.RETURNFULL:
							return retval
					elif len(kind) > 0:
						if type(kind) == str:
							kind = kh.guess_kind(kind)
						ref = kh.get_ref_by_kind_name_namespace(kind, call_name, call_namespace)
						retval = call(uip.stdscr, ref, kind)
						if retval is not None and retval == curses_helper.retval.RETURNFULL:
							return retval
					changed = True
			if force_update == True:
				uip.force_update()

def eventdispatch(stdscr, ref, view):
	kind = deep_get(ref, "involvedObject#kind")
	kind = kh.guess_kind(kind)
	name = deep_get(ref, "involvedObject#name")
	namespace = deep_get(ref, "involvedObject#namespace")
	if namespace is None:
		namespace = deep_get(ref, "metadata#namespace")
	ref = kh.get_ref_by_kind_name_namespace(kind, name, namespace)
	return resourceinfodispatch(stdscr, ref, kind)

def log_add_line(timestamps, facilities, severities, messages, timestamp, facility, severity, message):
	if timestamp is not None:
		timestamps.append(timestamp.astimezone())
	else:
		timestamps.append("")
	facilities.append(facility)
	severities.append(severity)
	messages.append(message)

	return timestamps, facilities, severities, messages

def containerinfoloop(stdscr, container, kind, obj):
	global override_tail_lines

	uip = UIProps(stdscr)

	uip.init_window(field_list = None, windowheader = "Container Info", helptext = helptexts.containerinfo)

	# For generic information
	infopad = uip.init_infopad(height = 7, width = -1, ypos = 1, xpos = 1)

	# For the pod log
	tspad, logpad = uip.init_logpad(width = -1, ypos = 9, xpos = 1)

	# For the status bar; position is always at the bottom of the screen and the entire width of the screen
	statusbar = uip.init_statusbar()

	# Number of lines of log to show by default
	if override_tail_lines is None:
		override_tail_lines = deep_get(iktconfig, "Pods#logsize", default_tail_lines)

	tail_lines = override_tail_lines

	uip.continuous_log = False
	merge_repeats = deep_get(iktconfig, "Pods#merge_repeated_messages", False)
	saved_merge_repeats = merge_repeats
	raw_logs = False
	tmp_log_level = deep_get(iktconfig, "Pods#loglevel", "Info")
	log_level = name_to_loglevel(tmp_log_level)
	show_borders = deep_get(iktconfig, "Pods#show_borders", True)
	uip.toggle_borders(show_borders)
	show_timestamps = deep_get(iktconfig, "Pods#show_timestamps", True)
	uip.toggle_timestamps(show_timestamps)
	# Currently not working properly
	compact_timestamps = False
	# Currently not working properly
	only_new_dates = False
	# Currently not working properly
	highlight_new_dates = False
	wrap_lines = False
	# Show severity as text
	severity_prefix = deep_get(iktconfig, "Pods#severity_prefix", False)

	# This decides whether or not compound log messages,
	# such as Python dicts and JSON, should be expanded
	fold_msg = deep_get(iktconfig, "Pods#fold_msg", True)
	saved_fold_msg = fold_msg

	# This decides whether or not to show the facility,
	# and if so how it is to be displayed
	show_facility = deep_get(iktconfig, "Pods#show_facility", "Full")
	override_parser = None

	reload = False

	uip.update_window()
	uip.force_update()
	uip.refresh_window()
	uip.refresh_statusbar()
	curses.doupdate()

	while True:
		uip.countdown_to_update()

		if uip.is_update_triggered():
			# When following the log we update the log continuously, but tail lines is limited to the number of lines
			# that fits on the screen, and cursor movements are disabled; as soon as the user presses a key the log
			# will stop scrolling and the whole log (default number of tail lines) will be loaded
			if uip.continuous_log:
				tail_lines = uip.logpadheight

			pod_info = get_pod_info([obj])[0]
			podname = pod_info.name
			namespace = pod_info.namespace
			containername = deep_get(container, "name")

			if kind == ("InitContainer", ""):
				src_statuses = deep_get(pod_info.ref, "status#initContainerStatuses", [])
				container_type = "init_container"
			else:
				src_statuses = deep_get(pod_info.ref, "status#containerStatuses", [])
				container_type = "container"
			container_status = None
			for container_status in src_statuses:
				if deep_get(container_status, "name") == containername:
					break
			if container_status is not None:
				image = deep_get(container_status, "imageID", "")
			else:
				image = "<unavailable>"

			notice = curses_helper.notice(None, y = uip.maxy // 2, x = uip.maxx // 2, message = "Fetching log")
			rawmsg, internal_error = get_pod_log_by_name_namespace_container(podname, namespace, containername, tail_lines = tail_lines)
			del notice
			uip.refresh_window()
			uip.refresh_infopad()
			uip.refresh_logpad()
			uip.refresh_statusbar()
			curses.doupdate()

			splitmsg = split_msg(rawmsg)

			timestamps = []
			facilities = []
			severities = []
			messages = []
			parser = ""
			prev_timestamp = None
			prev_facility = ""
			prev_severity = ""
			prev_message = ""
			prev_remnants = None
			repeat_count = 0

			total_msgs = 0
			hidden_msgs = 0
			merged_lines = 0

			linecount = len(splitmsg)
			linepercent = int(linecount * 0.01)

			progressbar = None
			if linecount > 1000:
				progressbar = curses_helper.progressbar(None, y = uip.maxy // 2, minx = (uip.minx + 8), maxx = (uip.maxx - 8), progress = 0, title = "Parsing log")

			i = 0
			while i < len(splitmsg):
				line = splitmsg[i]

				# We probably need a progress bar once we reach this many lines
				if linecount > 1000 and (i % linepercent) == 0:
					curses_helper.progressbar(progressbar, y = uip.maxy // 2, minx = (uip.minx + 8), maxx = (uip.maxx - 8), progress = 100 - 100 * ((linecount - i) / linecount))
					progressbar.timeout(10)
					c = progressbar.getch()
					if c == 27:	# ESCAPE
						if reload == True:
							break
						else:
							return curses_helper.retval.RETURNDONE

				if internal_error == True:
					timestamp, facility, severity, message, remnants, parser = logparser("internal_error", "", "", line, container_type = container_type, line = i)
				elif raw_logs == True:
					timestamp, facility, severity, message, remnants, parser = logparser("raw", "", "", line, container_type = container_type, line = i)
				else:
					timestamp, facility, severity, message, remnants, parser = logparser(podname, containername, image, line, fold_msg = fold_msg, override_parser = override_parser, container_type = container_type, line = i)

				i += 1

				# In some cases rather than expanding a single line into multiple lines,
				# we want to parse multiple lines as a single block;
				# we signal this by returning message == ["start_block", processor],
				# then continue parsing until we either get ["end_block", *],
				# ["break", *], or reach the end of the file
				if type(message) == list and message[0] == "start_block" and raw_logs == False:
					_logentries = [(timestamp, facility, severity, remnants)]
					processor = message
					options = {}
					if len(processor) == 3:
						options = processor[2]
					for j in range(i, len(splitmsg)):
						processor, _logentry = processor[1](splitmsg[j], fold_msg = fold_msg, options = options)
						if processor[0] != "end_block_not_processed":
							_logentries.append(_logentry)

						if processor[0] in ["end_block", "end_block_not_processed"]:
							# OK, we've got a block; start by appending the first line
							if len(_logentries) > 0:
								timestamps, facilities, severities, messages = log_add_line(timestamps, facilities, severities, messages, timestamp, facility, severity, _logentries[0][3])
							if len(_logentries) > 1:
								for _timestamp, _facility, _severity, _message in _logentries[1:]:
									timestamps, facilities, severities, messages = log_add_line(timestamps, facilities, severities, messages, _timestamp, "".ljust(len(facility)), severity, _message)
								break
						elif processor[0] == "break":
							# We got something indicating that this isn't a valid block; abort
							break
					if processor[0] == "end_block":
						# We've got a block and it's been appended, so go on
						i = j + 1
						continue
					elif processor[0] == "end_block_not_processed":
						# We've got a block and it's been appended, but the last line needs processing again
						i = j
						continue
					else:
						message = remnants
						remnants = None

				# If this is a (themestr, severity) tuple, ignore the severity;
				# it's only necessary for remnants since the severity is passed separarely
				if type(message) == tuple:
					message, _ = message

				total_msgs += 1

				if severity > log_level:
					hidden_msgs += 1
					continue

				if prev_message == message and prev_facility == facility and prev_severity == severity and (prev_message != "" or prev_remnants is not None) and prev_remnants == remnants:
					repeat_count += 1
					prev_timestamp = timestamp
					if merge_repeats:
						merged_lines += 1
						continue
				else:
					if repeat_count > 0 and merge_repeats:
						timestamps, facilities, severities, messages = log_add_line(timestamps, facilities, severities, messages, prev_timestamp, prev_facility, prev_severity, [("[previous message repeated ", color_log_severity(prev_severity, False)), (f"{repeat_count}", ("logview", "repeat_count")), (" times]", color_log_severity(prev_severity, False))])
					repeat_count = 0
					prev_timestamp = timestamp
					prev_facility = facility
					prev_severity = severity
					prev_message = message
					prev_remnants = remnants

				timestamps, facilities, severities, messages = log_add_line(timestamps, facilities, severities, messages, timestamp, facility, severity, message)

				if remnants is not None and len(remnants) > 0:
					# Remnants are used for unfolding multi-line messages that have been folded into one,
					# such as YAML/JSON, etc.
					#
					# Remnants can, for the time being, be either:
					# (list of string, severity)
					# (string(newline separated strings), severity)
					# or
					# [(string, severity), ...]
					if type(remnants) == tuple:
						tmpmessages, severity = remnants

						if type(tmpmessages) == list:
							for message in tmpmessages:
								timestamps, facilities, severities, messages = log_add_line(timestamps, facilities, severities, messages, None, "".ljust(len(facility)), severity, message)
						else:
							for message in tmpmessages.split("\n"):
								timestamps, facilities, severities, messages = log_add_line(timestamps, facilities, severities, messages, None, "".ljust(len(facility)), severity, message)
					else:
						for message, severity in remnants:
							timestamps, facilities, severities, messages = log_add_line(timestamps, facilities, severities, messages, None, "".ljust(len(facility)), severity, message)

			reload = True

			# The data in some fields might become shorter, so we need to trigger a clear
			uip.infopad.erase()
			uip.statusbar.erase()

			del progressbar

			# If the last message in the log is a repeat we need to add the repeat signature
			if repeat_count > 0 and merge_repeats:
				timestamps, facilities, severities, messages = log_add_line(timestamps, facilities, severities, messages, prev_timestamp, prev_facility, prev_severity, [("[previous message repeated ", color_log_severity(prev_severity, False)), (f"{repeat_count}", ("logview", "repeat_count")), (" times]", color_log_severity(prev_severity, False))])

			uip.update_log_info(timestamps, facilities, severities, messages)
			uip.loglen = len(messages)
			uip.update_window()

			containertypearray = f" [Type: {kind[0]}]"
			if kind == ("InitContainer", ""):
				src_statuses = deep_get(pod_info.ref, "status#initContainerStatuses")
			else:
				src_statuses = deep_get(pod_info.ref, "status#containerStatuses")

			if src_statuses is None or len(src_statuses) == 0:
				break

			for container_status in src_statuses:
				if deep_get(container_status, "name") == containername:
					break

			status, status_group, restarts, message = get_container_status(src_statuses, containername, kind)
			containerarray = [
				("Container: ", ("main", "infoheader")),
				(f"{containername}{containertypearray}", ("types", "generic"))
			]
			statusarray = [
				("Status: ", ("main", "infoheader")),
				(f"{status}", color_status_group(status_group, False))
			]
			if message != "":
				statusarray.append((f" ({message})", ("types", "generic")))
			restartsarray = [
				("Restarts: ", ("main", "infoheader")),
				(f"{restarts}", ("types", "numerical"))
			]
			podarray = [
				("Pod: ", ("main", "infoheader")),
				(f"{pod_info.name}", ("types", "generic"))
			]
			containeridarray = [
				("Container ID: ", ("main", "infoheader")),
				(f"{deep_get(container_status, 'containerID')}", ("types", "generic"))
			]
			image_name, image_version = get_image_tuple(deep_get(container_status, "image"))
			imagearray = [
				("Image: ", ("main", "infoheader")),
				(f"{image_name}", ("types", "generic")),
				("separators", "version"),
				(f"{image_version}", ("types", "version")),
			]
			image_id = deep_get(container_status, "imageID")
			imageidarray = [
				("I", ("main", "infoheader_shortcut")),
				("mage ID: ", ("main", "infoheader")),
				(f"{image_id}", ("types", "generic"))
			]

			uip.addthemearray(infopad, containerarray, y = 0, x = 0)
			uip.addthemearray(infopad, statusarray, y = 1, x = 0)
			uip.addthemearray(infopad, restartsarray, y = 2, x = 0)
			uip.addthemearray(infopad, podarray, y = 3, x = 0)
			uip.addthemearray(infopad, containeridarray, y = 4, x = 0)
			uip.addthemearray(infopad, imagearray, y = 5, x = 0)
			uip.addthemearray(infopad, imageidarray, y = 6, x = 0)
			uip.refresh = True

		if uip.refresh:
			# FIXME: the status stuff should be done by curses_helper
			theme = get_theme_ref()
			_ltee = theme["boxdrawing"].get("ltee", curses.ACS_LTEE)
			_rtee = theme["boxdrawing"].get("rtee", curses.ACS_RTEE)
			_vline = theme["boxdrawing"].get("vline", curses.ACS_VLINE)

			uip.statusbar.erase()

			if uip.continuous_log == True:
				interval = "Follow"
			else:
				interval = "Manual"

			if tail_lines == sys.maxsize:
				loglimit = "Unlimited"
			else:
				loglimit = f"{tail_lines}"

			loglevel_str = loglevel_to_name(log_level)

			# We have two different widths of the statusbar
			if uip.maxx - uip.minx > 108:
				# Wide version
				statusarray1 = [
					("Updates: ", ("statusbar", "infoheader")), (f"{interval}", ("statusbar", "highlight")),
					("separators", "statusbar"),
					("Loglvl: ", ("statusbar", "infoheader")), (f"{loglevel_str}", ("statusbar", "highlight")),
					("separators", "statusbar"),
					("Log length: ", ("statusbar", "infoheader")), (f"{uip.loglen} ", ("statusbar", "highlight")), ("lines (", ("statusbar", "default")),
				]
				if hidden_msgs > 0:
					statusarray1 += [
						(f"{hidden_msgs}", ("statusbar", "highlight")), (" messages hidden)", ("statusbar", "dim")), ("; ", ("statusbar", "default")),
					]
				statusarray1 += [
					("limit: ", ("statusbar", "default")), (f"{loglimit}", ("statusbar", "highlight")), (")", ("statusbar", "default")),
				]
				statusarray2 = [
				]
				if fold_msg == False:
					statusarray2 += [
						("Unfolding messages", ("statusbar", "highlight")),
						("separators", "statusbar"),
					]
				if merge_repeats == True:
					statusarray2 += [
						("Repeats merged", ("statusbar", "highlight")),
						("separators", "statusbar"),
					]

				statusarray2 += [
					("Facility: ", ("statusbar", "infoheader")),
					(f"{show_facility}", ("statusbar", "highlight")),
					("separators", "statusbar"),
					("Format: ", ("statusbar", "infoheader")),
				]
			else:
				# Compact version
				statusarray1 = [
					("Updates: ", ("statusbar", "infoheader")), (f"{interval}", ("statusbar", "highlight")),
					("separators", "statusbar_compact"),
					("Loglvl: ", ("statusbar", "infoheader")), (f"{loglevel_str}", ("statusbar", "highlight")),
					("separators", "statusbar_compact"),
					(f"{uip.loglen} ", ("statusbar", "highlight")), ("lines (", ("statusbar", "default")),
				]
				if hidden_msgs > 0:
					statusarray1 += [
						(f"{hidden_msgs}", ("statusbar", "highlight")), (" hidden", ("statusbar", "dim")), ("; ", ("statusbar", "default")),
					]
				statusarray1 += [
					("max: ", ("statusbar", "default")), (f"{loglimit}", ("statusbar", "highlight")), (")", ("statusbar", "default")),
				]
				statusarray2 = [
				]
				if fold_msg == False:
					statusarray2 += [
						("Unfolding", ("statusbar", "highlight")),
						("separators", "statusbar_compact"),
					]
				if merge_repeats == True:
					statusarray2 += [
						("Repeats merged", ("statusbar", "highlight")),
						("separators", "statusbar_compact"),
					]

				statusarray2 += [
					("Facility: ", ("statusbar", "infoheader")),
					(f"{show_facility}", ("statusbar", "highlight")),
					("separators", "statusbar_compact"),
					("Format: ", ("statusbar", "infoheader")),
				]

			try:
				if len(rawmsg) == 0 or parser[0] == "":
					statusarray2 += [
						("Empty", ("main", "format_empty")),
					]
				elif parser[0] == "unknown":
					statusarray2 += [
						("Unknown", ("main", "format_unknown")),
					]
				elif raw_logs:
					statusarray2 += [
						("Raw", ("main", "format_raw")),
					]
				else:
					if uip.maxx - uip.minx > 118:
						statusarray2 += [
							(f"{parser[0]}:{parser[1]}", ("statusbar", "highlight")),
						]
					else:
						statusarray2 += [
							(f"{parser[0]}", ("statusbar", "highlight")),
						]
			except Exception as e:
				sys.exit(f"[{parser}]; Exception: {e}")

			uip.addthemearray(statusbar, statusarray1, y = 0, x = 0)
			uip.addthemearray(statusbar, statusarray2, y = 1, x = 0)

			maxlen = 0
			latest_facility_len = -1

			uip.tspad.erase()
			uip.logpad.erase()
			# This is needed in case we get a resize event
			uip.resize_logpad(uip.maxy - uip.logpadypos - 2, 0)
			new_date = False
			if timestamps is not None and len(timestamps) > 0:
				current_datestamp = ("%s" % timestamps[0])[0:len("YYYY-MM-DD")]
				new_date = True
			yadd = 0
			for y in range(0, min(uip.logpadheight, uip.loglen)):
				if uip.yoffset + y >= len(timestamps):
					timestamp = "".ljust(uip.tspadwidth)
				else:
					timestamp = ("%s" % timestamps[uip.yoffset + y]).ljust(uip.tspadwidth)
					datestamp = timestamp[0:len("YYYY-MM-DD ")]
					if current_datestamp is not None and type(timestamps[uip.yoffset + y]) == datetime and datestamp != current_datestamp:
						current_datestamp = datestamp
						new_date = True
				hourstamp = timestamp[len("YYYY-MM-DD "):]

				if new_date == True and highlight_new_dates == True:
					datetype = "timestamp_newdate"
				else:
					datetype = "timestamp"

				if compact_timestamps == False:
					hourxpos = len("YYYY-MM-DD ")
				else:
					hourxpos = 0

				if y + yadd > uip.logpadheight:
					continue

				tsstrarray = None
				# FIXME
				if only_new_dates == True:
					if new_date == True:
						tsthemearray = [(datestamp, ("types", datetype))]
						new_date = False
						# If we do empty lines for new dates we need this
						# uip.addstr(logpad, "", y = y + yadd, x = 0)
						# yadd += 1
				else:
					tsthemearray = [(datestamp, ("types", datetype))]

				tsthemearray += [(hourstamp, ("types", "timestamp"))]

				new_date = False

				if show_facility == "None" or uip.yoffset + y >= len(facilities):
					facility = ""
				elif show_facility == "Short":
					tmp = re.match(r"^.*/(.*)", facilities[uip.yoffset + y])
					if tmp is not None:
						facility = tmp[1]
						latest_facility_len = len(tmp[1])
					elif len(facilities[uip.yoffset + y].strip()) != len(facilities[uip.yoffset + y]) and len(facilities[uip.yoffset + y].strip()) == 0:
						if latest_facility_len == -1:
							facility = facilities[uip.yoffset + y]
						else:
							facility = facilities[uip.yoffset + y][0:latest_facility_len]
					else:
						facility = facilities[uip.yoffset + y]
				else:
					facility = facilities[uip.yoffset + y]

				if uip.yoffset + y >= len(severities):
					severity = loglevel.INFO
				else:
					severity = severities[uip.yoffset + y]

				msgstrarray = []

				if severity_prefix == True:
					if len(timestamp.strip()) != 0 or y >= len(timestamps):
						msgstrarray.append((f"[{lvl_to_4letter_severity(severity)}] ", color_log_severity(severity, False)))
					else:
						msgstrarray.append(("".ljust(len(f"[{lvl_to_4letter_severity(severity)}] ")), color_log_severity(severity, False)))

				if facility != "":
					if facility.rstrip() != "":
						facilitystr = "<%s> " % (facility)
					else:
						facilitystr = " %s  " % (facility)
					msgstrarray.append((facilitystr, color_log_severity(loglevel.DEBUG, False)))

				msg = messages[uip.yoffset + y]
				if type(msg) != list:
					msgstrarray.append((msg, color_log_severity(severity, False)))
				else:
					msgstrarray += msg
				if wrap_lines == True:
					sideadjust = 0
					if uip.borders == False:
						sideadjust = 2
					maxwidth = uip.maxx - uip.logpadxpos + sideadjust
				else:
					maxwidth = -1
				msgstrarrays = strarray_wrap_line(msgstrarray, maxwidth, wrap_marker = (uip.borders or get_mousemask() != 0))
				for i in range(0, len(msgstrarrays)):
					if y + yadd + i > uip.logpadheight:
						continue
					if i == 0:
						uip.addthemearray(tspad, tsthemearray, y = y + yadd, x = 0)
					cury, curx = uip.addthemearray(logpad, msgstrarrays[i], y = y + yadd + i, x = 0)
					maxlen = max(maxlen, len(themearray_extract_string(msgstrarrays[i])))
				yadd += i
			uip.resize_logpad(-1, maxlen)

			uip.refresh_window()
			uip.refresh_infopad()
			uip.refresh_logpad()
			uip.refresh_statusbar()
			curses.doupdate()
			uip.refresh = False

		uip.stdscr.timeout(100)
		c = uip.stdscr.getch()
		retval = uip.generic_keycheck(c)

		if retval == curses_helper.retval.MATCH:
			continue
		elif retval == curses_helper.retval.RETURNONE:
			return curses_helper.retval.RETURNDONE
		elif retval == curses_helper.retval.RETURNFULL:
			return retval

		if c == curses.KEY_F2:
			retval = selectwindow(stdscr, uip)
			if retval == curses_helper.retval.RETURNFULL:
				return retval
			uip.refresh_all()
		elif c == curses.KEY_F3:
			retval = selectwindow(stdscr, uip, refresh_apis = True)
			if retval == curses_helper.retval.RETURNFULL:
				return retval
			uip.refresh_all()
		elif c == ord("B"):
			uip.toggle_borders()
			uip.refresh_all()
			uip.force_update()
		elif c == curses.KEY_F4:
			if uip.continuous_log:
				# Disable log tailing
				uip.continuous_log = False
				tail_lines = override_tail_lines
				uip.set_update_delay(-1)
				# When we disable tailing we would ideally want to be at the end, but we don't know where the end is
				# so, stay on top...
			else:
				# Enable log tailing
				uip.continuous_log = True
				tail_lines = uip.logpadheight
				uip.set_update_delay(10)
			uip.yoffset = 0
			uip.xoffset = 0
			uip.force_update()
		elif c == curses.KEY_F8:
			if uip.continuous_log:
				continue

			# Toggle full logs; this might be VERY slow
			uip.continuous_log = False

			if tail_lines == override_tail_lines:
				if curses_helper.confirmationbox(uip.stdscr, uip.maxy // 2, uip.maxx // 2, title = "Show full log (Potentially very slow):", default = False):
					tail_lines = sys.maxsize
			else:
				tail_lines = override_tail_lines

			uip.refresh_infopad()
			uip.refresh_logpad()
			uip.refresh_statusbar()
			curses.doupdate()
			uip.force_update()
		elif c == ord("I"):
			# XXX: This is very inefficient
			_vlist = get_container_info()
			retval = None
			for _obj in _vlist:
				if _obj.name == containername and _obj.image_id == image_id:
					retval = resourceinfodispatch(stdscr, _obj.ref, ("__Container", ""))
					uip.force_update()
					break
			if retval is not None and retval == curses_helper.retval.RETURNFULL:
				return retval
		elif c == ord("R"):
			if uip.continuous_log:
				continue

			if raw_logs == False:
				saved_fold_msg = fold_msg
				saved_merge_repeats = merge_repeats
				fold_msg = True
				merge_repeats = False
			else:
				fold_msg = saved_fold_msg
				merge_repeats = saved_merge_repeats
			raw_logs = not raw_logs
			uip.yoffset = 0
			uip.xoffset = 0
			uip.force_update()
		elif c == ord("P"):
			parserlist = [
				(widgetlineattrs.NORMAL, [(f"<autodetect>", ("windowwidget", "default"))]),
			]
			for lvl in natsorted(get_parser_list()):
				parserlist.append((widgetlineattrs.NORMAL, [(f"{lvl}", ("windowwidget", "default"))]))

			if override_parser is None:
				preselection = "<autodetect>"
			else:
				preselection = override_parser
			tmpselection = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, parserlist, title = "Override Logparser", cursor = True, preselection = preselection)
			selection = None
			if tmpselection is not None and type(tmpselection[0]) == int:
				selection = tmpselection[1][0][0]
			uip.refresh_all()
			if selection is not None and len(selection) > 0:
				if selection == "<autodetect>":
					override_parser = None
				else:
					override_parser = selection
			uip.yoffset = 0
			uip.xoffset = 0
			uip.force_update()
		elif c == ord("L"):
			loglevellist = []
			for lvl in get_loglevel_names():
				if lvl.startswith("Diff"):
					continue
				loglevellist.append((widgetlineattrs.NORMAL, [(f"{lvl}", ("windowwidget", "default"))]))

			tmpselection = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, loglevellist, title = "Select Loglevel", cursor = True, preselection = loglevel_to_name(log_level))
			selection = None
			if tmpselection is not None and type(tmpselection[0]) == int:
				selection = tmpselection[1][0][0]
			uip.refresh_all()
			if selection is not None and len(selection) > 0:
				log_level = name_to_loglevel(selection)
			uip.yoffset = 0
			uip.xoffset = 0
			uip.force_update()
		elif c == ord("W"):
			wrap_lines = not wrap_lines
			uip.yoffset = 0
			uip.xoffset = 0
			uip.force_update()
		elif c == ord("F"):
			if uip.continuous_log or raw_logs:
				continue

			fold_msg = not fold_msg
			uip.yoffset = 0
			uip.xoffset = 0
			uip.force_update()
		elif c == ord("D"):
			if uip.continuous_log or raw_logs:
				continue

			merge_repeats = not merge_repeats
			uip.yoffset = 0
			uip.xoffset = 0
			uip.force_update()
		elif c == ord("T"):
			uip.toggle_timestamps()
			uip.refresh_all()
			uip.force_update()
		elif c == ord("V"):
			facilitylist = [
				(widgetlineattrs.NORMAL, [("Full", ("windowwidget", "default"))]),
				(widgetlineattrs.NORMAL, [("Short", ("windowwidget", "default"))]),
				(widgetlineattrs.NORMAL, [("None", ("windowwidget", "default"))]),
			]

			tmpselection = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, facilitylist, title = "Select Facility Level", cursor = True, preselection = show_facility)
			selection = None
			if tmpselection is not None and type(tmpselection[0]) == int:
				selection = tmpselection[1][0][0]
			uip.refresh_all()
			if selection is not None and len(selection) > 0:
				show_facility = selection
			uip.yoffset = 0
			uip.xoffset = 0
			uip.force_update()
		elif c == ord("E"):
			if uip.continuous_log:
				continue

			uip.refresh = True
			if raw_logs:
				rawprefix = "Raw "
			else:
				rawprefix = ""
			filename = curses_helper.inputbox(uip.stdscr, uip.maxy // 2, 1, uip.maxy - 1, uip.maxx - 1, "Export %slog to file: " % (rawprefix))
			if filename is None or filename == "":
				continue

			# This is necessary because we never go through the normal update cycle for the logpad and infopad
			uip.refresh_infopad()
			uip.refresh_logpad()
			curses.doupdate()

			if os.path.exists(filename):
				curses.doupdate()
				if curses_helper.confirmationbox(uip.stdscr, uip.maxy // 2, uip.maxx // 2, title = "File '%s' already exists; overwrite?:" % (filename), default = False) == False:
					continue

			f = open(filename, "w")
			for y in range(0, uip.loglen):
				# Even when the log is raw we want the first timestamp
				if y >= len(timestamps):
					timestamp = "".ljust(uip.tspadwidth)
				else:
					timestamp = "%s" % (timestamps[y])

				if y >= len(facilities) or facilities[y] == "":
					facility = ""
				else:
					facility = "<%s> " % (facilities[y])

				if uip.yoffset + y >= len(severities):
					severity = f"{lvl_to_4letter_severity(loglevel.INFO)}: "
				else:
					severity = f"{lvl_to_4letter_severity(severities[y])}: "

				if raw_logs:
					f.write("%s  %s\n" % (timestamp, messages[y]))
				else:
					f.write("%s  %s%s%s\n" % (timestamp, severity, facility, messages[y]))
			f.close()

def executecommand(stdscr, obj, container, msg, command, waitforkeypress):
	# This should probably be done using connect_get_namespaced_pod_exec()
	curses.endwin()
	os.system("clear")
	iktprint(msg)
	print()
	if obj is not None:
		obj_name = deep_get(obj, "metadata#name")
		obj_namespace = deep_get(obj, "metadata#namespace")
		# This is a request to create a new ephemeral container
		if command == ["<ephemeral>"]:
			# Should we perhaps use subprocess.run here?
			ephemeral_image = deep_get(iktconfig, "Debug#ephemeral_image", "busybox")
			args = ["kubectl", "debug", obj_name, "--target", container, "-n", obj_namespace, "-i", "-t", "--image", ephemeral_image, "--attach=false"]
			result = subprocess.run(args, stdout = PIPE, stderr = STDOUT, check = False)
			if result.returncode != 0:
				iktprint([(f"Error: ", "error"), ("Failed to create debug image", "default")], stderr = True)
			else:
				output = result.stdout.decode('utf-8')
				tmp = re.match(r"Creating debugging pod (\S+) with container (\S+) .*", output)
				if tmp is not None:
					pod_name = tmp[1]
					container_name = tmp[2]
				else:
					sys.exit(f"executecommand failed to parse output; {output}; aborting.")

				time.sleep(5)
				# Attach to the image
				# Should we perhaps use subprocess.run here?
				args = ["kubectl", "attach", pod_name, "-c", container_name, "-it", "--pod-running-timeout=1m0s"]
				subprocess.call(args)

				# Delete the image
				# Should we perhaps use subprocess.run here?
				args = ["kubectl", "delete",  "pod", pod_name]
				subprocess.call(args)
		elif command == ["<dnsutils>"]:
			dnsutils_image = deep_get(iktconfig, "Debug#network_image", "gcr.io/kubernetes-e2e-test-images/dnsutils:1.3")
			args = ["kubectl", "debug", f"node/{obj_name}", "-i", "-t", "--image", dnsutils_image, "--attach=false"]
			result = subprocess.run(args, stdout = PIPE, stderr = STDOUT, check = False)
			if result.returncode != 0:
				iktprint([(f"Error: ", "error"), ("Failed to create debug image", "default")], stderr = True)
			else:
				output = result.stdout.decode('utf-8')
				tmp = re.match(r"Creating debugging pod (\S+) with container (\S+) .*", output)
				if tmp is not None:
					pod_name = tmp[1]
					container_name = tmp[2]
				else:
					sys.exit(f"executecommand failed to parse output; {output}; aborting.")

				time.sleep(2)
				# Attach to the image
				# Should we perhaps use subprocess.run here?
				args = ["kubectl", "attach", pod_name, "-c", container_name, "-it", "--pod-running-timeout=1m0s"]
				subprocess.call(args)

				# Delete the image
				# Should we perhaps use subprocess.run here?
				args = ["kubectl", "delete",  "pod", pod_name]
				subprocess.call(args)
		else:
			# Should we perhaps use subprocess.run here?
			args = ["kubectl", "exec", obj_name, "-n", obj_namespace, "-i", "-t", "--container", container, "--"] + command
			subprocess.call(args)
	else:
		sys.exit("Called execute command with obj=None")

	if waitforkeypress:
		input("\nPress Enter to continue...")
	stdscr.refresh()

def resourcelistdispatch(stdscr, ref, kind):
	plural = deep_get(ref, "status#acceptedNames#plural")

	view = checkforview(plural)

	if view is not None:
		if "viewfunc" in views[view]:
			viewfunc = views[view]["viewfunc"]
		else:
			viewfunc = genericlistloop
		viewfunc(stdscr, view)

	return curses_helper.retval.RETURNDONE

def csi_driver_dispatch(stdscr, obj, kind):
	ref = kh.get_ref_by_kind_name_namespace(kind, deep_get(obj, "name"), "")
	return resourceinfodispatch(stdscr, ref, kind)

# This dispatches to info views; as soon as one is added this function will
# dispatch to that view.
def resourceinfodispatch(stdscr, obj, kind, info = None, **kwargs):
	if obj is None:
		return None

	if kind in infoviews:
		return genericinfoloop(stdscr, obj, kind, **kwargs)
	# Exceptions
	elif kind in [("Container", ""), ("InitContainer", "")]:
		# Do not try to open pending containers
		if deep_get(info, "status#phase") in ["Pending", "Failed"]:
			return None
		return containerinfoloop(stdscr, obj, kind, info)
	else:
		return None

def get_all_namespaced_resources(obj):
	namespace = deep_get(obj, "metadata#name")
	info = get_resource_info_by_namespace(namespace, kh.get_list_of_namespaced_resources())
	return info

# Iterate over the list of items and execute command on all of them
def executecommand_multiple(stdscr, msg, command, items, waitforkeypress):
	item = None

	curses.endwin()
	os.system("clear")
	print(msg + "\n")

	for item in items:
		args = command + [item]
		# Should we perhaps use subprocess.run here?
		retval = subprocess.call(args)
		if retval != 0:
			break

	if waitforkeypress or retval > 0:
		if retval > 0:
			iktprint([(f"Error: ", "error")] + format_commandline(args) + [(" returned ", "default"), (f"{retval}", "errorvalue")], stderr = True)
		input("\nPress Enter to continue...")
	stdscr.refresh()

def print_ansible_results(data, host, task, retval):
	if task.startswith("TASK: "):
		task = task[len("TASK: "):]
	elif task.startswith("output"):
		tmp = re.match(r"output.*?_(.*)", task)
		if tmp:
			task = tmp[1].replace("_", " ")
	elif task == "":
		task = "<unnamed>"

	tmp = re.match(r"(.*)#\d*$", task)
	if tmp is not None:
		task = tmp[1]

	if type(data) is str:
		stdout_lines = ""
		stderr_lines = ""
		msg = data
	else:
		stdout_lines = data.get("stdout", "").rstrip(" \t\v\r\n").splitlines()
		stderr_lines = data.get("stderr", "").rstrip(" \t\v\r\n").splitlines()
		msg = data.get("msg", "")
		retval = data.get("rc", retval)

	# If this is an empty debug message we don't need to show it
	if task.startswith("debug") and len(stdout_lines) == 0 and len(stderr_lines) == 0 and len(msg) == 0 and retval == 0:
		return

	# Header for this section
	if data.get("skipped", False) == True or data.get("skip_reason", "") == "Conditional result was False":
		iktprint([("• ", "separator"), (f"{task}", "skip")])
		iktprint([(data.get("skip_reason", "<skipped>"), "default")])
		iktprint([("", "default")])
		return

	if retval != 0:
		iktprint([("• ", "separator"), (f"{task}", "error"), (" (retval: ", "default"), (retval, "errorvalue"), (")", "default")], stderr = True)
	else:
		iktprint([("• ", "separator"), (f"{task}", "success")])

	if len(msg) > 0:
		iktprint([("msg:", "header")])
		for line in msg.splitlines():
			iktprint([(line, "default")])
		iktprint([("", "default")])

	if type(data) is str:
		iktprint([(f"{data}", "default")])
	else:
		# If we have module_stdout or module_stderr,
		# and stderr and stderr are empty, don't show either
		failed_modules = data.get("failed_modules")
		module_stdout_lines = str(data.get("module_stdout", "")).rstrip(" \t\v\r\n").splitlines()
		module_stderr_lines = str(data.get("module_stderr", "")).rstrip(" \t\v\r\n").splitlines()

		if len(stdout_lines) > 0 or (failed_modules is None and len(module_stdout_lines) == 0 and len(module_stderr_lines) == 0):
			iktprint([("stdout:", "header")])
			for line in stdout_lines:
				iktprint([(f"{line}", "default")])
			if len(stdout_lines) == 0 and (failed_modules is None and len(module_stdout_lines) == 0 and len(module_stderr_lines) == 0):
				iktprint([("<no output>\n", "none")])
			else:
				iktprint([("", "default")])

		# If retval isn't 0 we don't really care if stderr is empty
		if len(stderr_lines) > 0 or (retval != 0 and failed_modules is None and len(module_stdout_lines) == 0 and len(module_stderr_lines) == 0):
			iktprint([("stderr:", "header")])
			for line in stderr_lines:
				iktprint([(f"{line}", "default")], stderr = True)
			if len(stderr_lines) == 0 and (failed_modules is None and len(module_stdout_lines) == 0 and len(module_stderr_lines) == 0):
				iktprint([("<no output>\n", "none")])
			else:
				iktprint([("", "default")])

		if len(module_stdout_lines) > 0:
			iktprint([("module_stdout:", "header")])
			for line in module_stdout_lines:
				iktprint([(f"{line}", "default")])
			iktprint([("", "default")])

		if len(module_stderr_lines) > 0:
			iktprint([("module_stderr:", "header")])
			for line in module_stderr_lines:
				iktprint([(f"{line}", "default")])
			iktprint([("", "default")])

		if failed_modules is not None and len(failed_modules) > 0:
			for module in failed_modules:
				iktprint([(f"{module}", "default")])
				iktprint([("module_stdout:", "header")])

				module_stdout_lines = str(failed_modules[module].get("module_stdout", "")).rstrip(" \t\v\r\n").splitlines()
				module_stderr_lines = str(failed_modules[module].get("module_stderr", "")).rstrip(" \t\v\r\n").splitlines()

				if len(module_stdout_lines) > 0:
					for line in module_stdout_lines:
						iktprint([(f"{line}", "default")])
					iktprint([("", "default")])
				else:
					iktprint([("<no output>\n", "none")])

				iktprint([("module_stderr:", "header")])
				if len(module_stderr_lines) > 0:
					for line in module_stderr_lines:
						iktprint([(f"{line}", "default")], stderr = True)
					iktprint([("\n", "default")])
				else:
					iktprint([("<no output>\n", "none")])

def __run_playbook(playbookpath, nodes, values = None):
	http_proxy = deep_get(iktconfig, "Network#http_proxy", "")
	https_proxy = deep_get(iktconfig, "Network#https_proxy", "")
	use_proxy = "no"
	if (http_proxy is not None and len(http_proxy) > 0) or (https_proxy is not None and len(https_proxy) > 0):
		use_proxy = "yes"
	values["use_proxy"] = use_proxy

	retval, ansible_results = ansible_run_playbook_on_selection(playbookpath, selection = nodes, values = values)

	# FIXME: ensure that we tell apart fail from skip
	if retval != 0 and len(ansible_results) == 0:
		iktprint([("Failed to execute playbook; retval: ", "error"), (f"{retval}", "errorvalue")], stderr = True)
	else:
		for host in ansible_results:
			# Start by trying to discern the overall result of the run,
			# to use as the colour for the hostname
			data = ansible_results[host]

			# This task does not log using register/debug;
			# users of ansible-playbook will be unhappy
			if data.get("stdout") or data.get("stderr") or data.get("msg"):
				skipped = False

				retval = data.get("rc", retval)
				if data.get("stdout", "").startswith("[SKIP]") and len(data.get("stderr", "")) == 0 and len(data.get("msg", "")) == 0:
					skipped = True
			else:
				tmpskipped = -1

				for task in ansible_results[host]:
					# Messages (should) always succeed
					if task.startswith("TASK: ") or task.startswith("output"):
						retval = data[task].get("rc", retval)
						if ansible_results[host][task].get("stdout", "").startswith("[SKIP]") and len(ansible_results[host][task].get("stderr", "")) == 0 and len(ansible_results[host][task].get("msg", "")) == 0 and tmpskipped != 0:
							tmpskipped = 1
						elif (len(ansible_results[host][task].get("stdout", "")) > 0 and ansible_results[host][task].get("stdout", "").startswith("[SKIP]") == False) or len(ansible_results[host][task].get("stderr", "")) > 0 or len(ansible_results[host][task].get("msg", "")) > 0:
							tmpskipped = 0

					if retval != 0:
						break

			skipped = bool(tmpskipped)

			# Time to output the messages:
			if retval == 0:
				if skipped == False:
					iktprint([(f"[{host}]", "success")])
				else:
					iktprint([(f"[{host}] N/A (SKIPPED)\n", "skip")])
			else:
				iktprint([(f"[{host}]", "error")])

			if skipped == False:
				if "quiet" in values and values["quiet"] == True:
					print("See Logs for output")
					continue

				# This task does not log using register/debug;
				# users of ansible-playbook will be unhappy
				if data.get("stdout") or data.get("stderr") or data.get("msg"):
					print_ansible_results(data, host, "", retval)
				else:
					for task in ansible_results[host]:
						if task.startswith("TASK: ") or task.startswith("output") or task.startswith("msg"):
							print_ansible_results(data[task], host, task, retval)
					print()

	return retval

def run_playbook(playbook, nodes, values = None):
	return __run_playbook(playbook.get("playbook"), nodes, values = values)

def view_yaml_dump(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	title = deep_get(kwargs, "title")
	objects = []

	for item in items:
		if type(item) == tuple:
			namespace, name = item
		else:
			name = item
			namespace = ""
		obj = kh.get_ref_by_kind_name_namespace(kind, name, namespace)
		objects.append(obj)

	themearrays = iktlib.format_yaml(objects)
	return resourceinfodispatch(uip.stdscr, themearrays, ("__ResourceView", ""), title = title)

def infoview_view_yaml(uip, kind, obj, **kwargs):
	if "path" in kwargs:
		path = deep_get(kwargs, "path")
		if deep_get(kwargs, "include_final_path", False) == True:
			_obj = [{f"{path.rsplit('#')[-1]}": deep_get(obj, path)}]
		else:
			_obj = [deep_get(obj, path)]
	else:
		_obj = [obj]
	title = deep_get(kwargs, "title")
	themearrays = iktlib.format_yaml(_obj)
	return resourceinfodispatch(uip.stdscr, themearrays, ("__ResourceView", ""), title = title)

def infoview_view_last_applied_configuration(uip, kind, obj, **kwargs):
	title = deep_get(kwargs, "title")
	last_applied_configuration = deep_get(obj, "metadata#annotations#kubectl.kubernetes.io/last-applied-configuration", {})
	if last_applied_configuration is not None and len(last_applied_configuration) > 0:
		data = json.loads(last_applied_configuration)
	else:
		return
	themearrays = iktlib.format_yaml([data])
	return resourceinfodispatch(uip.stdscr, themearrays, ("__ResourceView", ""), title = title)

def delete_resource(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	success = True

	for item in items:
		namespace, name = item
		message, status = kh.delete_obj_by_kind_name_namespace(kind, name, namespace)
		if status != 200:
			win = curses_helper.alert(uip.stdscr, uip.maxy // 2, uip.maxx // 2, message = message)
			success = False
			break

	# We successfully deleted everything
	if success == True:
		win = curses_helper.notice(uip.stdscr, uip.maxy // 2, uip.maxx // 2, message = "Successfully deleted all specified resources")

	# Wait for a keypress
	while True:
		uip.stdscr.timeout(100)
		c = uip.stdscr.getch()
		if c != -1:
			del win
			break

def force_delete_resource(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	values["__force"] = True
	return delete_resource(uip, items = items, action = action, values = values, kind = kind)

def __create_namespace(name):
	args = [ "kubectl", "create", "namespace", f"{name}" ]
	result = subprocess.run(args, stdout = PIPE, stderr = PIPE, universal_newlines = True)
	return result

def create_namespace(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	curses.endwin()
	os.system("clear")

	name = values["namespace"]

	# Verify that the name of the new namespace is valid
	if kh.validate_name("dns-label", name) == False:
		msg = [("Error: ", "error"), ("“", "default"), (name, "argument"), ("“ is not a valid name", "default")]
		iktprint(msg, stderr = True)
	elif kh.get_ref_by_kind_name_namespace(kind, name, "") is not None:
		msg = [("Error: ", "error"), ("namespace “", "default"), (name, "argument"), ("“ already exists", "default")]
		iktprint(msg, stderr = True)
	else:
		msg = [("Creating namespace “", "default"), (name, "argument"), ("“", "default")]
		iktprint(msg)
		__create_namespace(name)

	print("\n")

	waitforkeypress = True
	if waitforkeypress:
		input("\nPress Enter to continue...")
	uip.stdscr.refresh()

# Valid kinds:
# ("Deployment", "apps")
def __pause_resource(kind, namespace, name):
	args = ["kubectl", "rollout", "pause", f"{kind[0]}.{kind[1]}/{name}", f"--namespace={namespace}"]
	result = subprocess.run(args, stdout = PIPE, stderr = PIPE, universal_newlines = True)
	return result

# Valid kinds:
# ("Deployment", "apps")
def __resume_resource(kind, namespace, name):
	args = ["kubectl", "rollout", "resume", f"{kind[0]}.{kind[1]}/{name}", f"--namespace={namespace}"]
	result = subprocess.run(args, stdout = PIPE, stderr = PIPE, universal_newlines = True)
	return result

# Valid kinds:
# ("DaemonSet", "apps")
# ("Deployment", "apps")
# ("StatefulSet", apps")
def __restart_resource(kind, namespace, name):
	args = ["kubectl", "rollout", "restart", f"{kind[0]}.{kind[1]}/{name}", f"--namespace={namespace}"]
	result = subprocess.run(args, stdout = PIPE, stderr = PIPE, universal_newlines = True)
	return result

def __get_resource_scale(kind, namespace, name):
	obj = kh.get_ref_by_kind_name_namespace(kind, name, namespace)
	return deep_get(obj, "status#replicas")

# Valid kinds:
# ("Deployment", "apps")
# ("ReplicaSet", "apps")
# ("ReplicationController", "")
# ("StatefulSet", apps")
def __scale_replicas(kind, namespace, name, scale):
	args = ["kubectl", "scale", f"{kind[0]}.{kind[1]}/{name}", f"--namespace={namespace}", f"--replicas={scale}"]
	result = subprocess.run(args, stdout = PIPE, stderr = PIPE, universal_newlines = True)
	return result

def __edit_resource(kind, namespace, name):
	if namespace is None:
		args = ["kubectl", "edit", f"{kind[0]}.{kind[1]}/{name}"]
	else:
		args = ["kubectl", "edit", f"{kind[0]}.{kind[1]}/{name}", f"--namespace={namespace}"]
	result = subprocess.run(args, universal_newlines = True)
	return result.returncode, args

def apply_configuration_from_file(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	curses.endwin()
	os.system("clear")
	resource_path, rtype = values["resource_path"]

	if rtype == "Configuration File":
		args = ["kubectl", "apply", "-f", resource_path]
	elif rtype == "Kustomization":
		args = ["kubectl", "apply", "-k", resource_path]
	else:
		raise Exception(f"Unknown resource type {rtype}; this is a programming error.")

	result = subprocess.run(args, universal_newlines = True)
	return result.returncode, args

def apply_configuration_from_url(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	curses.endwin()
	os.system("clear")
	resource_path = values["resource_url"]

	args = ["kubectl", "apply", "-f", resource_path]
	result = subprocess.run(args, universal_newlines = True)
	return result.returncode, args

# We should probably download and apply locally instead
def create_resource_from_url(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	curses.endwin()
	os.system("clear")
	resource_path, rtype = values["resource_url"]

	args = ["kubectl", "create", "-f", resource_path]
	result = subprocess.run(args, universal_newlines = True)
	return result.returncode, args

def __summarise_resources(path):
	resources = []
	with open(path, "r") as f:
		dicts = yaml.safe_load_all(f)
		for d in dicts:
			kind, api_group = kh.kind_api_version_to_kind(deep_get(d, "kind"), deep_get(d, "apiVersion"))
			if api_group == "":
				merged_kind = kind
			else:
				merged_kind = f"{kind}.{api_group}"
			name = deep_get(d, "metadata#name")
			namespace = deep_get(d, "metadata#namespace", "")
			resources.append((merged_kind, namespace, name))
	return resources

def create_resource_from_file(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	resource_path, rtype = values["resource_path"]
	resource_list = []

	result = 0
	args = []

	if rtype == "Configuration File":
		resources = __summarise_resources(resource_path)

		for kind, namespace, name in resources:
			resource_list.append((widgetlineattrs.NORMAL, [(namespace, ("windowwidget", "default"))], [(name, ("windowwidget", "default"))], [(kind, ("windowwidget", "default"))]))

		headers = ["Namespace:", "Name:", "Kind:"]
		title = "The following resources will be created; accept?"

		uip.refresh_all()
		confirm_buttons = [[ord("y"), ord("Y"), ord("n"), ord("N")], [("[", ("windowwidget", "default")), ("Y", ("windowwidget", "highlight")), ("es", ("windowwidget", "default")), ("]", ("windowwidget", "default"))], [("[", ("windowwidget", "default")), ("N", ("windowwidget", "highlight")), ("o", ("windowwidget", "default")), ("]", ("windowwidget", "default"))]]
		confirm_press, tmpselection = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, resource_list, headers = headers, title = title, confirm = True, confirm_buttons = confirm_buttons, cursor = False)

		if confirm_press in [ord("y"), ord("Y")]:
			args = ["kubectl", "create", "-f", resource_path]
	elif rtype == "Kustomization":
		args = ["kubectl", "create", "-k", resource_path]
	else:
		raise Exception(f"Unknown resource type {rtype}; this is a programming error.")

	if len(args) > 0:
		result = subprocess.run(args, universal_newlines = True).returncode

	return result, args

def diff_resource_configuration(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	resource_path, rtype = values["resource_path"]

	if rtype == "Configuration File":
		args = ["kubectl", "diff", "-f", resource_path]
	elif rtype == "Kustomization":
		args = ["kubectl", "diff", "-k", resource_path]
	else:
		raise Exception(f"Unknown resource type {rtype}; this is a programming error.")

	result = subprocess.run(args, stdout = PIPE, stderr = PIPE, universal_newlines = True)

	indent = deep_get(iktconfig, "Global#indent", 4)

	diff = []

	if len(result.stdout) == 0:
		return

	for line in result.stdout.splitlines():
		if line.startswith("+"):
			formatting = ("windowwidget", "diffplus")
		elif line.startswith("-"):
			formatting = ("windowwidget", "diffminus")
		elif line.startswith("@@"):
			formatting = ("windowwidget", "diffatat")
		elif line.startswith("diff "):
			formatting = ("windowwidget", "diffheader")
		else:
			formatting = ("windowwidget", "diffsame")

		# kubectl shows the diff with a 2-space indent;
		# if a different indent is configured we have to modify this
		if indent != 2:
			prefix = line[0]
			tmp = re.match(r"^(\s*)(.*)", line[1:])
			if tmp is not None and len(tmp[1]) % 2 == 0:
				line = prefix + "".ljust(int(len(tmp[1]) / 2) * indent) + tmp[2]

		diff.append((widgetlineattrs.NORMAL, [(line, formatting)]))

	tmpselection = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, diff, title = "Diff between current and would be configuration:", cursor = False)

def format_commandline(args, implicit_command = True):
	strarray = []

	command = implicit_command

	for i in range(0, len(args)):
		# The first argument is the program name
		if i == 0:
			strarray += [(args[i], "programname")]
		# This is an option
		elif args[i].startswith(("-")):
			strarray += [(f" {args[i]}", "option")]
		# The first non-option argument is the command
		elif command == False:
			strarray += [(f" {args[i]}", "command")]
			command = True
		else:
			strarray += [(f" {args[i]}", "argument")]

	return strarray

def edit_resource(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	curses.endwin()
	os.system("clear")
	if type(items[0]) == tuple:
		namespace, name = items[0]
	else:
		namespace = None
		name = items[0]
	retval, args = __edit_resource(kind, namespace, name)

	waitforkeypress = False
	if waitforkeypress or retval > 0:
		if retval > 0:
			iktprint([("\nError: ", "error")] + format_commandline(args, implicit_command = False) + [(" returned ", "default"), (retval, "errorvalue")], stderr = True)
		input("\nPress Enter to continue...")
	uip.stdscr.refresh()

def restart_resource_rescale(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	curses.endwin()
	os.system("clear")

	for item in items:
		namespace, name = item
		old_scale = __get_resource_scale(kind, namespace, name)
		__scale_replicas(kind, namespace, name, scale = 0)
		__scale_replicas(kind, namespace, name, scale = old_scale)
		msg = [(f"Restarting {kind[0]}.{kind[1]} “", "default"), (namespace, "argument"), ("::", "separator"), (name, "argument"), ("“ scale: ", "default"), (old_scale, "emphasis")]
		iktprint(msg)

	print("\n")

	waitforkeypress = True
	if waitforkeypress:
		input("\nPress Enter to continue...")
	uip.stdscr.refresh()

def restart_resource_rollout(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	curses.endwin()
	os.system("clear")

	for item in items:
		namespace, name = item
		__restart_resource(kind, namespace, name)
		msg = [(f"Restarting {kind[0]}.{kind[1]} “", "default"), (namespace, "argument"), ("::", "separator"), (name, "argument")]
		iktprint(msg)

	print("\n")

	waitforkeypress = True
	if waitforkeypress:
		input("\nPress Enter to continue...")
	uip.stdscr.refresh()

def rescale_resource(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	curses.endwin()
	os.system("clear")

	for item in items:
		namespace, name = item
		scale = values["scale"]
		old_scale = __get_resource_scale(kind, namespace, name)
		__scale_replicas(kind, namespace, name, scale)
		msg = [(f"Rescaling {kind[0]}.{kind[1]} “", "default"), (namespace, "argument"), ("::", "separator"), (name, "argument"), ("“ scale: ", "default"), (old_scale, "emphasis"), (" => ", "default"), (scale, "emphasis")]
		iktprint(msg)

	print("\n")

	waitforkeypress = True
	if waitforkeypress:
		input("\nPress Enter to continue...")
	uip.stdscr.refresh()

# Valid kinds:
# ("Deployment", "apps")
def pause_resource_rollout(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	curses.endwin()
	os.system("clear")

	for item in items:
		namespace, name = item
		__pause_resource(kind, namespace, name)
		msg = [(f"Pausing {kind[0]}.{kind[1]} “", "default"), (namespace, "argument"), ("::", "separator"), (name, "argument")]
		print(msg)

	print("\n")

	waitforkeypress = True
	if waitforkeypress:
		input("\nPress Enter to continue...")
	uip.stdscr.refresh()

# Valid kinds:
# ("Deployment", "apps")
def resume_resource_rollout(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	curses.endwin()
	os.system("clear")

	for item in items:
		namespace, name = item
		__resume_resource(kind, namespace, name)
		msg = [(f"Resuming {kind[0]}.{kind[1]} “", "default"), (namespace, "argument"), ("::", "separator"), (name, "argument")]
		print(msg)

	print("\n")

	waitforkeypress = True
	if waitforkeypress:
		input("\nPress Enter to continue...")
	uip.stdscr.refresh()

# Valid kinds:
# ("Deployment", "apps")
def stop_resource_rescale(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	curses.endwin()
	os.system("clear")

	for item in items:
		namespace, name = item
		old_scale = __get_resource_scale(kind, namespace, name)
		__scale_replicas(kind, namespace, name, 0)
		msg = [(f"Stopping {kind[0]}.{kind[1]} “", "default"), (namespace, "argument"), ("::", "separator"), (name, "argument"), ("“ scale: ", "default"), (old_scale, "emphasis"), (" => ", "default"), ("0", "emphasis")]
		print(msg)

	print("\n")

	waitforkeypress = True
	if waitforkeypress:
		input("\nPress Enter to continue...")
	uip.stdscr.refresh()

def delete_logs(uip, items = [], action = {}, values = {}, kind = None):
	for item in items:
		ansible_delete_log(item)

def command_hosts(uip, items = [], action = {}, values = {}, kind = None, **kwargs):
	hosts = iktlib.join_tuple_list(items, _tuple = "hostname", separator = (", ", "separator"))
	msg = [("Executing playbook “", "action"), (action.get("description"), "argument"), ("“ on the following hosts: ", "action")] + hosts + [("\n", "action")]

	curses.endwin()
	os.system("clear")

	if action.get("requires_cluster_info") == True:
		gather_cluster_info()

	iktprint(msg)

	# If run_before is set we have one or several playbooks that need to be run before this one
	runbefore = action.get("run_before", [])

	retval = 0

	for preplaybook in runbefore:
		preplaybookpath = get_playbook_path(f"{preplaybook}.yaml")
		tmpretval = __run_playbook(preplaybookpath, items, values = values)
		if retval == 0:
			retval = tmpretval
		else:
			break

	# If all pre-requisites completed successfully we perform the main task
	if retval == 0:
		retval = run_playbook(action, items, values = values)

	# If everything was successful it's time for the run_after playbooks
	if retval == 0:
		runafter = action.get("run_after", [])

		for postplaybook in runafter:
			postplaybookpath = get_playbook_path(f"{postplaybook}.yaml")
			tmpretval = __run_playbook(postplaybookpath, items, values = values)
			if retval == 0:
				retval = tmpretval
			else:
				break

	# If everything was successful we execute the add_to_groups/remove_from_groups actions
	if retval == 0:
		for group in action.get("add_to_groups", []):
			ansible_add_hosts(ANSIBLE_INVENTORY, items, group = group, skip_all = True)

		for group in action.get("remove_from_groups", []):
			ansible_remove_hosts(ANSIBLE_INVENTORY, items, group = group)

	waitforkeypress = True
	if waitforkeypress or retval > 0:
		input("\nPress Enter to continue...")
	uip.stdscr.refresh()

nodeplaybooks = {
}

def ssh_to_host(stdscr, host):
	if host is None:
		return

	msg = [("SSH:ing to ", "action"), (host, "hostname"), (":\n", "action")]

	inventory = ansible_get_inventory_dict()

	# ansible_user in the inventory overrides that defined in ikt.yaml
	# thus we get these in ascending over of priority
        # if no user is specified we assume that we're to use the user we're running as;
	# but really, ansible
	sshuser = deep_get(iktconfig, "Ansible#ansible_user")
	sshuser = deep_get(inventory, "all#vars#ansible_user", sshuser)
	sshuser = deep_get(inventory, f"all#hosts#{host}#ansible_user", sshuser)

	curses.endwin()
	os.system("clear")
	iktprint(msg)

	args = [ "/usr/bin/ssh" ]

	if sshuser is not None:
		host = "%s@%s" % (sshuser, host)
	args.append(host)

	subprocess.run(args)

	stdscr.refresh()

def list_configuration_files(basedir):
	# Always provide a means to navigate up in the directory tree
	plist = []
	directories = []
	kustomizations = []
	files = []

	if os.path.isdir(basedir):
		for filename in os.listdir(basedir):
			if filename.startswith("~") or filename.startswith("."):
				continue
			# real path, display name, type
			if filename.endswith(".yaml") or filename.endswith(".yml"):
				files.append((f"{basedir}/{filename}", filename, "Configuration File"))
				ptype = "Configuration File"
			elif os.path.isdir(f"{basedir}/{filename}"):
				if os.path.isfile(f"{basedir}/{filename}/kustomization.yaml"):
					files.append((f"{basedir}/{filename}", filename, "Kustomization"))
				else:
					files.append((f"{basedir}/{filename}", filename, "<dir>"))
			else:
				continue

	plist = [(f"{os.path.dirname(basedir)}", "..", "<dir>")]
	plist += natsorted(directories)
	plist += natsorted(kustomizations)
	plist += natsorted(files)

	return plist

def populate_actionlist(context = None, action_list = None, control_plane_selected = False, single_item = False, cluster_available = True):
	actions = []
	order = []
	for item in action_list:
		context_types = deep_get(action_list, f"{item}#context_types")
		if context is not None and context_types != [] and context not in context_types:
			continue

		description = deep_get(action_list, f"{item}#description")
		category = deep_get(action_list, f"{item}#category", "")
		name = item
		order.append((description, category, name))

	currentcategory = ""
	for item in sorted(order, key = itemgetter(1, 0)):
		action = item[0]
		category = item[1]
		allowoncontrolplane = deep_get(action_list, f"{item[2]}#allow_on_controlplane", True)
		singleoncontrolplane = deep_get(action_list, f"{item[2]}#single_on_controlplane", False)
		singleonly = deep_get(action_list, f"{item[2]}#single_only", False)
		readonly = deep_get(action_list, f"{item[2]}#read_only", False)
		metadata = deep_get(action_list, f"{item[2]}#metadata", [])
		if singleonly == True and single_item == False:
			continue
		if singleoncontrolplane == True and control_plane_selected == True and single_item == False:
			continue
		elif allowoncontrolplane == False and control_plane_selected == True:
			continue
		if readonly == False and read_only_mode == True:
			continue

		if currentcategory != category:
			actions.append((widgetlineattrs.SEPARATOR, [(f" {category} ", ("windowwidget", "default"))], [("", ("windowwidget", "default"))]))
			currentcategory = category
		actions.append((widgetlineattrs.NORMAL, [(f"{action}", ("windowwidget", "default"))], metadata))

	return actions, action_list

def __populate_playbooklist(path, action_list = {}):
	if not os.path.isdir(path):
		return

	if action_list is None:
		action_list = {}

	for filename in os.listdir(path):
		if filename.startswith("~") or filename.startswith("."):
			continue

		tmp = re.match(r"(.*)\.ya?ml$", filename)
		if tmp is None:
			continue

		playbookname = tmp[1]

		# Check if the playbook already exists in the action list,
		if playbookname in action_list:
			continue

		playbookpath = f"{path}/{filename}"
		description = None

		with open(playbookpath) as f:
			try:
				d = yaml.safe_load(f)
			except Exception as e:
				# This entry could not be parsed; add a dummy entry
				action_list[playbookname] = {
					"description": playbookpath,
					"playbook": playbookpath,
					"category": "__INVALID__",
					"comments": "Failed to parse (Not valid YAML)",
				}
				continue

			# Empty files are used to disable playbooks completely
			if d is None or len(d) == 0:
				action_list[playbookname] = {
					"description": playbookpath,
					"playbook": playbookpath,
					"category": "__DISABLED__",
				}
				continue

			if type(d) != list:
				# This entry could not be parsed; add a dummy entry
				action_list[playbookname] = {
					"description": playbookpath,
					"playbook": playbookpath,
					"category": "__INVALID__",
					"comments": "Failed to parse (Not a list of plays)",
				}
				continue

			description = deep_get(d[0], "vars#metadata#description")

			# Ignore all playbooks that lack a description;
			# typically they are internal playbooks
			if description is None:
				continue

			query = deep_get(d[0], "vars#metadata#query#string")
			queryval = deep_get(d[0], "vars#metadata#query#variable")
			queryfunc = deep_get(d[0], "vars#metadata#query#function")

			# Sanity check
			if queryfunc is not None:
				if queryfunc not in ["string", "yesno", "filechooser"]:
					raise Exception(f"unknown queryfunc “{queryfunc}“ provided")

			confirm = deep_get(d[0], "vars#metadata#confirm", False)
			tmpallowoncontrolplane = deep_get(d[0], "vars#metadata#allow_on_control_plane")
			if tmpallowoncontrolplane is None or tmpallowoncontrolplane == "always":
				allowoncontrolplane = True
				singleoncontrolplane = False
			elif tmpallowoncontrolplane.lower() == "single":
				allowoncontrolplane = True
				singleoncontrolplane = True
			elif tmpallowoncontrolplane.lower() == "never":
				allowoncontrolplane = False
				singleoncontrolplane = False
			else:
				raise Exception(f"{playbookpath}: Invalid values for allow-on-control-plane")

			requiresclusterinfo = deep_get_with_fallback(d[0], ["vars#metadata#requires_cluster_info", "vars#metadata#requires-cluster-info"], False)
			runbefore = deep_get_with_fallback(d[0], ["vars#metadata#run_before", "vars#metadata#run-before"], [])
			runafter = deep_get_with_fallback(d[0], ["vars#metadata#run_after", "vars#metadata#run-after"], [])
			addtogroups = deep_get_with_fallback(d[0], ["vars#metadata#add_to_groups", "vars#metadata#add-to-groups"], [])
			removefromgroups = deep_get_with_fallback(d[0], ["vars#metadata#remove_from_groups", "vars#metadata#remove-from-groups"], [])
			category = deep_get(d[0], "vars#metadata#category", "Uncategorized")
			playbooktypes = deep_get_with_fallback(d[0], ["vars#metadata#playbook_types", "vars#metadata#playbook-types"], [])
			comments = deep_get(d[0], "vars#metadata#comments", "")
			readonly = deep_get_with_fallback(d[0], ["vars#metadata#read_only", "vars#metadata#read-only"], False)
			extravars = {
				"quiet": deep_get(d[0], "vars#metadata#quiet", False),
			}

			action_list[playbookname] = {
				"description": description,
				"playbook": playbookpath,
				"query": query,
				"queryval": queryval,
				"queryfunc": queryfunc,
				"confirm": confirm,
				"allow_on_control_plane": allowoncontrolplane,
				"single_on_control_plane": singleoncontrolplane,
				"requires_cluster_info": requiresclusterinfo,
				"run_before": runbefore,
				"run_after": runafter,
				"add_to_groups": addtogroups,
				"remove_from_groups": removefromgroups,
				"category": category,
				"playbook_types": playbooktypes,
				"comments": comments,
				"read_only": readonly,
				"extravars": extravars,
			}

	return action_list

def populate_playbooklist(context = None, actions = [], action_list = {}, control_plane_selected = False, single_item = False, cluster_available = True):
	# Only populate the list of playbooks if we have ansible support
	if ansible_support == True:
		local_playbooks = deep_get(iktconfig, "Ansible#local_playbooks", [])
		for playbook_path in local_playbooks:
			# Substitute {HOME}/ for {HOMEDIR}
			if playbook_path.startswith("{HOME}/"):
				playbook_path = f"{HOMEDIR}/{playbook_path[len('{HOME}/'):]}"
			# Skip non-existing playbook paths
			if not os.path.isdir(playbook_path):
				continue
			action_list = __populate_playbooklist(playbook_path, action_list = action_list)
		action_list = __populate_playbooklist(ANSIBLE_PLAYBOOK_DIR, action_list = action_list)

		order = []
		for item in action_list:
			playbooktypes = action_list[item].get("playbook_types", [])
			if context is not None and playbooktypes != [] and context not in playbooktypes:
				continue

			description = action_list[item].get("description")
			category = action_list[item].get("category", "")
			name = item
			order.append((description, category, name))

		currentcategory = ""
		for item in sorted(order, key = itemgetter(1, 0)):
			action = item[0]
			category = item[1]
			allowoncontrolplane = action_list[item[2]].get("allow_on_control_plane", True)
			singleoncontrolplane = action_list[item[2]].get("single_on_control_plane", True)
			readonly = action_list[item[2]].get("read_only", False)
			comments = action_list[item[2]].get("comments", "")
			lineattrs = widgetlineattrs.NORMAL
			if read_only_mode == True and readonly == False:
				continue
			if singleoncontrolplane == True and control_plane_selected == True and single_item == False:
				continue
			elif allowoncontrolplane == False and control_plane_selected == True:
				continue
			elif action_list[item[2]].get("requires_cluster_info", False) == True and cluster_available == False:
				metadata = []
				if len(comments) > 0:
					metadata.append((f"{comments},", ("windowwidget", "highlight")))
				metadata.append(("<cluster not available>", ("windowwidget", "alert")))
				lineattrs = widgetlineattrs.UNSELECTABLE
			elif category == "__INVALID__":
				lineattrs = widgetlineattrs.UNSELECTABLE & widgetlineattrs.INVALID
				metadata = [(comments, ("windowwidget", "alert"))]
			elif category == "__DISABLED__":
				continue
			else:
				metadata = [(comments, ("windowwidget", "highlight"))]

			if currentcategory != category:
				if category == "__INVALID__":
					actions.append((widgetlineattrs.SEPARATOR, [(f" INVALID ", ("windowwidget", "bright"))], [("", ("windowwidget", "default"))]))
					currentcategory = category
				else:
					actions.append((widgetlineattrs.SEPARATOR, [(f" {category} ", ("windowwidget", "default"))], [("", ("windowwidget", "default"))]))
					currentcategory = category
			if currentcategory == "__INVALID__":
				actions.append((lineattrs, [(f"{action}", ("windowwidget", "alert"))], metadata))
			else:
				actions.append((lineattrs, [(f"{action}", ("windowwidget", "default"))], metadata))

	return actions, action_list

available_api_families = []

def selectwindow(stdscr, uip, refresh_apis = False):
	global available_api_families
	global defaultview
	hide_unavailable = deep_get_with_fallback(iktconfig, ["Selector#hide_unavailable_apis", "Global#hide_unavailable_apis"], True)
	sortcolumn = deep_get(iktconfig, "Selector#sortcolumn", "kind")

	if sortcolumn == "kind":
		sortkey1 = 1
		sortkey2 = 2
	elif sortcolumn == "name":
		sortkey1 = 1
		sortkey2 = 0
	else:
		sys.exit(f"Invalid sortcolumn {sortcolumn} for Selector; aborting.")

	if len(available_api_families) == 0 or refresh_apis == True:
		notice = curses_helper.notice(None, y = uip.maxy // 2, x = uip.maxx // 2, message = "Refreshing list of available APIs")
		available_api_families, status = kh.get_available_api_families()
		del notice
		curses.doupdate()
	items = []
	order = []

	# Find the correct sort order
	for item in views:
		kind = None
		viewref = views[item]
		if deep_get(viewref, "skip", False) == True:
			continue
		if hide_unavailable == True:
			kind = deep_get(viewref, "kind", ("", ""))
			if kind is not None and kind != ("", "") and not kind[0].startswith("__") and kind not in available_api_families:
				continue
		group = views[item].get("group")
		# XXX: Do override in a nicer manner; perhaps we want all "built-in" groups first?
		if group == "Administration":
			group = "0"
		elif group == "Core":
			group = "1"
		elif group == "Workloads":
			group = "2"
		elif "(Deprecated)" in group:
			# This should ensure that all deprecated groups end up last, but still sorted
			group = "ZZZZZ" + group
		if kind is not None:
			api_group = kind[1]
		else:
			api_group = ""

		if group is None:
			raise Exception(f"group shouldn't be None unless skip=True; view: {item}")

		order.append((item, group, api_group))

	currentgroup = ""
	for item in sorted(order, key = itemgetter(sortkey1, sortkey2)):
		view = item[0]
		viewref = views[view]

		# This allows us to override the sort order
		group = deep_get(viewref, "group", "")
		kind = deep_get(viewref, "kind", ("", ""))
		if kind is not None:
			api_group = kind[1]
		else:
			api_group = ""

		if currentgroup != group:
			items.append((widgetlineattrs.SEPARATOR, [(f" {group} ", ("windowwidget", "highlight"))], [("", ("windowwidget", "default"))]))
			currentgroup = group
		lineattrs = widgetlineattrs.NORMAL
		if kind is not None and kind != ("", "") and not kind[0].startswith("__") and kind not in available_api_families:
			lineattrs = widgetlineattrs.UNSELECTABLE
		if api_group == "":
			items.append((lineattrs, [(f"{view}", ("windowwidget", "default"))], [(f"", ("windowwidget", "dim"))]))
		else:
			items.append((lineattrs, [(f"{view}", ("windowwidget", "default"))], [(f"<{api_group}>", ("windowwidget", "dim"))]))

	selection = None

	# Ideally we want to return to the same selection
	while selection is None:
		title = "Choose view"
		tmpselection = curses_helper.windowwidget(uip.stdscr, uip.maxy, uip.maxx, uip.maxy // 2, uip.maxx // 2, items, title = title, preselection = defaultview)
		if tmpselection is not None:
			selection = tmpselection[1][0][0]

	if selection != "":
		defaultview = selection
		return curses_helper.retval.RETURNFULL
	else:
		return curses_helper.retval.NOMATCH

def selectorloop(stdscr, view):
	uip = UIProps(stdscr)

	field_list, sortcolumn = fieldgenerator(view)
	windowheader = views[view].get("windowheader", view)

	uip.init_window(field_list = field_list, windowheader = windowheader, sortcolumn = sortcolumn)

	while True:
		retval = selectwindow(stdscr, uip)
		if retval == curses_helper.retval.RETURNFULL:
			return retval

listviewactions = {
	"Apply configuration from file": {
		"description": "Apply configuration file or Kustomization",
		"confirm": True,
		"actionfunc": apply_configuration_from_file,
		"query": "Choose configuration/Kustomization to apply:",
		"queryval": "resource_path",
		"queryfunc": "filechooser",
		"extravars": {
			"listgetter": list_configuration_files,
			"basedir": DEPLOYMENTDIR,
		},
		#"confirmfunc":
		#"confirmstring": "The following changes will be performed:",
	},
	"Create resources from file": {
		"description": "Create resource from configuration file/Kustomization",
		"actionfunc": create_resource_from_file,
		"query": "Choose configuration/Kustomization to create:",
		"queryval": "resource_path",
		"queryfunc": "filechooser",
		"extravars": {
			"listgetter": list_configuration_files,
			"basedir": DEPLOYMENTDIR,
		},
	},
	"Apply configuration from URL": {
		"description": "Apply configuration URL",
		"confirm": True,
		"actionfunc": apply_configuration_from_url,
		"query": "URL to apply",
		"queryval": "resource_url",
		"queryfunc": "string",
		#"confirmfunc":
		#"confirmstring": "The following changes will be performed:",
	},
	"Create resources from URL": {
		"description": "Create resource from URL",
		"actionfunc": create_resource_from_url,
		"query": "URL to create resource from",
		"queryval": "resource_url",
		"queryfunc": "string",
	},
	#"Delete resources from configuration": {
	#	"description": "Delete resource by reverting configuration file/Kustomization",
	#},
	"Diff resource configuration": {
		"description": "Show what modifications applying configuration file or Kustomization would make",
		"actionfunc": diff_resource_configuration,
		"query": "Choose configuration/Kustomization to show difference against:",
		"queryval": "resource_path",
		"queryfunc": "filechooser",
		"extravars": {
			"listgetter": list_configuration_files,
			"basedir": DEPLOYMENTDIR,
		},
	},
}

# Default infopad fields:
#
# Name		Shortcut	Path				Special
# ---		---		---				---
# Name:				metadata#name			Always first
# Namespace:	N		metadata#namespace		Always second; optional
# Created:			metadata#creationTimestamp	Always last; uses field_processor_timestamp()
#
# Brief overview of infopad:
#
# "Fieldname:":
#	{
#		"fieldname": formatlist		# Overrides "Fieldname:" as field name; this is a format list
#						# This is mainly intended for highlighting shortcut keys
#		"path": "path",			# Get the value from this path using deep_get(obj, "path")
#	        "processor": "processor",	# Use this function to postprocess the value;
#						# returns a list of (string, (color, attribute)) tuples
#						# The default processor turns None, "", and -1 into
#						# [("strings, "none")]
#						# Example processors:
#						#	field_processor_timestamp()
#						#	field_processor_status()
#						#	field_processor_list()
#						#	field_processor_empty() (the default)
#						#	None (use this to only output the fieldname)
#		"extra_paths": [ "path", ... ]	# Extra paths to pass to the processor
#		"fallback": formatlist		# Fallback format list
#       }
# Note: For complex data it might be easier to write a custom processor and just pass in the object
#
# The most versatile, but also complex, processor is field_processor_list();
# it can provide formatting, substitutions, and (limited) flow control
infoviews = {
	("Addon", "k3s.cattle.io"): {
		"infopad": {
			"Source:": {
				"path": "spec#source",
			},
			"Checksum": {
				"path": "spec#checksum",
			},
		},
	},
	("Alertmanager", "monitoring.coreos.com"): {
		"infopad": {
			"Image:": {
				"processor": field_processor_images,
			},
			"Node Selector:": {
				"path": "spec#nodeSelector",
				"processor": field_processor_label_selector,
			},
			"Replicas:": {
				"path": "spec#replicas",
			},
			"Service Account Name:": {
				"fieldname": [("S", ("main", "infoheader_shortcut")), ("ervice Account Name:", ("main", "infoheader"))],
				"path": "spec#serviceAccountName",
			},
		},
		"shortcuts": {
			"Service Account": {
				"shortcut": ord("S"),
				"helptext": ("[Shift] + S", "Open info page for service account"),
				"call": resourceinfodispatch,
				"kind": ("ServiceAccount", ""),
				"namespace_path": "metadata#namespace",
				"name_path": "spec#serviceAccountName",
			},
		},
	},
	# apiregistration.k8s.io
	("APIService", "apiregistration.k8s.io"): {
		"windowheader": "API Service Info",
		"infopad": {
			"Group:": {
				"path": "spec#group",
			},
			"API Version Priority:": {
				"path": "spec#versionPriority",
			},
			"Group Priority Minimum:": {
				"path": "spec#groupPriorityMinimum",
			},
			"Insecure Skip TLS Verify:": {
				"path": "spec#insecureSkipTLSVerify",
				"fallback": [("False", ("types", "generic"))],
			},
			"Service:": {
				"path": "spec#service",
				"fieldname": [("S", ("main", "infoheader_shortcut")), ("ervice:", ("main", "infoheader"))],
				"extra_paths": [ "spec#service#namespace", "spec#service#name" ],
				"formatting": {
					"field_colors": [("types", "namespace"), ("types", "generic")],
					"item_separators": [("separators", "namespace")],
				},
				"fallback": [("Local", ("types", "generic"))],
				"processor": field_processor_list,
			},
			"Available:": {
				"processor": field_processor_api_service_available,
			},
			"Status:": {
				"processor": field_processor_api_service_status,
			},
		},
		"shortcuts": {
			"Service": {
				"shortcut": ord("S"),
				"helptext": ("[Shift] + S", "Open info page for service"),
				"call": resourceinfodispatch,
				"kind": ("Service", ""),
				"namespace_path": "spec#service#namespace",
				"name_path": "spec#service#name",
			},
		},
	},
	("Application", "app.k8s.io"): {
		"fields": ["name", "status", "link"],
		"sortcolumn": "status",
		"infopad": {
			"Add ownerRef:": {
				"path": "spec#addOwnerRef",
				"fallback": [("False", ("types", "generic"))],
			},
			"Label Selector:": {
				"path": "spec#selector#matchLabels",
				"processor": field_processor_label_selector,
			},
			"Kinds:": {
				"processor": field_processor_component_kinds,
			},
			"Description:": {
				"path": "spec#descriptor#description",
				"processor": field_processor_description,
			},
			"Keywords:": {
				"extra_paths": "spec#descriptor#keywords",
				"processor": field_processor_list,
			},
		},
		"listpad": {
			"infogetter": get_app_status_info,
		},
	},
	("AppRepository", "kubeapps.com"): {
		"infopad": {
			"Type:": {
				"path": "spec#type",
			},
			"URL:": {
				"path": "spec#url",
			},
		},
	},
	("AuthorizationPolicy", "security.istio.io"): {
		"fields": ["source", "operation", "condition"],
		"sortcolumn": "source",
		"windowheader": "Authorization Policy Info",
		"infopad": {
			"Workload Selector:": {
				"path": "spec#selector#labels",
				"processor": field_processor_label_selector,
			},
			"Action:": {
				"path": "spec#action",
				"fallback": [("ALLOW", ("types", "generic"))],
			},
		},
		"listpad": {
			"infogetter": get_auth_rule_info,
		},
	},
	("CatalogSource", "operators.coreos.com"): {
		"windowheader": "Catalog Source",
		"infopad": {
			"Display Name:": {
				"path": "spec#displayName",
			},
			"Image:": {
				"path": "spec#image",
			},
			"Publisher:": {
				"path": "spec#publisher",
			},
			"Source Type:": {
				"path": "spec#sourceType",
			},
		},
	},
	("Certificate", "cert-manager.io"): {
		"infopad": {
			"Common Name:": {
				"path": "spec#commonName",
			},
			"DNS Names:": {
				"extra_paths": "spec#dnsNames",
				"processor": field_processor_list,
			},
			"Issuer:": {
				"fieldname": [("I", ("main", "infoheader_shortcut")), ("ssuer:", ("main", "infoheader"))],
				"extra_paths": ["spec#issuerRef#kind", "spec#issuerRef#name"],
				"formatting": {
					"field_colors": [("types", "kind"), ("types", "generic")],
					"item_separators": [("separators", "kind")],
				},
				"processor": field_processor_list,
			},
			"Secret Name:": {
				"fieldname": [("S", ("main", "infoheader_shortcut")), ("ecret Name:", ("main", "infoheader"))],
				"path": "spec#secretName",
			},
			"Status:": {
				"processor": field_processor_condition_ready,
			},
		},
		"shortcuts": {
			"Issuer": {
				"shortcut": ord("I"),
				"helptext": ("[Shift] + I", "Open info page for issuer"),
				"call": resourceinfodispatch,
				"kind_path": "spec#issuerRef#kind",
				"name_path": "spec#issuerRef#name",
				"namespace_path": "metadata#namespace",
			},
			"Secret Name": {
				"shortcut": ord("S"),
				"helptext": ("[Shift] + S", "Open info page for secret"),
				"call": resourceinfodispatch,
				"kind": ("Secret", ""),
				"name_path": "spec#secretName",
				"namespace_path": "metadata#namespace",
			},
		},
	},
	("CertificateRequest", "cert-manager.io"): {
		"windowheader": "Certificate Request Info",
		"infopad": {
			"Issuer:": {
				"fieldname": [("I", ("main", "infoheader_shortcut")), ("ssuer:", ("main", "infoheader"))],
				"extra_paths": ["spec#issuerRef#kind", "spec#issuerRef#name"],
				"formatting": {
					"field_colors": [("types", "kind"), ("types", "generic")],
					"item_separators": [("separators", "kind")],
				},
				"processor": field_processor_list,
			},
			"Status:": {
				"processor": field_processor_condition_ready,
			},
		},
		"shortcuts": {
			"Issuer": {
				"shortcut": ord("I"),
				"helptext": ("[Shift] + I", "Open info page for issuer"),
				"call": resourceinfodispatch,
				"kind_path": "spec#issuerRef#kind",
				"name_path": "spec#issuerRef#name",
				"namespace_path": "metadata#namespace",
			},
		},
	},
	("CertificateSigningRequest", "certificates.k8s.io"): {
		"windowheader": "Certificate Signing Request Info",
		"infopad": {
			"Signer:": {
				"path": "spec#signerName",
			},
			"Requesting User:": {
				#"fieldname": [("R", ("main", "infoheader_shortcut")), ("equesting User:", ("main", "infoheader"))],
				"path": "spec#username",
			},
			"Groups:": {
				"extra_paths": "spec#groups",
				"processor": field_processor_list,
			},
			"Conditions:": {
				"path": "status",
				# Add highlighting for Approved, Denied, Issued
				"processor": field_processor_conditions_certsignreq,
			},
			"Usages:": {
				"extra_paths": "spec#usages",
				"processor": field_processor_list,
			}
		},
		#"shortcuts": {
		#	"Requesting User": {
		#		"shortcut": ord("R"),
		#		"helptext": ("[Shift] + R", "Open info page for requesting user"),
		#		"call": resourceinfodispatch,
		#		"kind": ("???", "???"),
		#		"name_path": "spec#username",
		#	},
		#},
	},
	("CiliumIdentity", "cilium.io"): {
		"windowheader": "Cilium Identity Info",
		"infopad": {},
		# Should we have a link to the Cilium Endpoint here?
	},
	("ClusterInformation", ""): {
		"windowheader": "Cluster Information Info",
		"infopad": {
			"Calico Version:": {
				"path": "spec#calicoVersion",
			},
			"Cluster GUID:": {
				"path": "spec#clusterGUID",
			},
			"Cluster Type:": {
				"path": "spec#clusterType",
			},
			"Datastore Ready:": {
				"path": "spec#datastoreReady",
			},
		},
	},
	("ClusterIssuer", "cert-manager.io"): {
		"windowheader": "Cluster Issuer Info",
		"infopad": {
			"Status:": {
				"processor": field_processor_condition_ready,
			},
			"Type:": {
				"processor": field_processor_issuer_type,
			},
			"CRL Distribution Points:": {
				"fallback": [("strings", "none")],
				"processor": field_processor_crl_distribution_points,
			},
			"CA Secret Name:": {
				"fieldname": [("C", ("main", "infoheader_shortcut")), ("A Secret Name:", ("main", "infoheader"))],
				"fallback": [("strings", "none")],
				"processor": field_processor_ca_secret_name,
			},
		},
		"shortcuts": {
			"CA Secret Name": {
				"shortcut": ord("C"),
				"helptext": ("[Shift] + C", "Open info page for CA Secret Name"),
				"call": resourceinfodispatch,
				"name_path": "spec#ca#secretName",
				# XXX: This can be reconfigured; how can that be queried?
				"namespace": "cert-manager",
				"kind": ("Secret", ""),
			},
		},
	},
	("ClusterRole", "rbac.authorization.k8s.io"): {
		"windowheader": "Cluster Role Info",
		"fields": ["api_group", "resource", "non_resource_urls", "resource_names", "verbs"],
		"sortcolumn": "resource",
		"infopad": {},
		"listpad": {
			"infogetter": get_policy_rule_info,
		},
	},
	("ClusterRoleBinding", "rbac.authorization.k8s.io"): {
		"windowheader": "Cluster Role Binding Info",
		"fields": ["namespace", "name", "kind", "rtype"],
		"sortcolumn": "name",
		"activatedfun": resourceinfodispatch,
		"extraref": "kind",
		"infopad": {
			"Cluster Role:": {
				"processor": None,
			},
			"  API Group:": {
				"path": "roleRef#apiGroup",
			},
			"  Name:": {
				"path": "roleRef#name",
			},
			"  Kind:": {
				"path": "roleRef#kind",
			},
		},
		"listpad": {
			"infogetter": get_subject_info,
		},
	},
	("ClusterServiceVersion", "operators.coreos.com"): {
		"windowheader": "Cluster Service Version",
		"infopad": {
			"Display Name:": {
				"path": "spec#displayName",
			},
			"Description:": {
				"path": "spec#description",
			},
			"Keywords:": {
				"extra_paths": "spec#keywords",
				"processor": field_processor_list,
			},
			"Maturity:": {
				"path": "spec#maturity",
			},
			"Version:": {
				"path": "spec#version",
			},
			"Minimum Kubernetes Version:": {
				"path": "spec#minKubeVersion",
			},
		},
		"shortcuts": {
			"Maintainers": {
				"shortcut": ord("m"),
				"helptext": ("M", "Show maintainers"),
				"widget": "windowwidget",
				"title": "Maintainers:",
				"headers": ["Name:", "E-mail:"],
				"itemgetter": get_list_fields,
				"itemgetter_args": {
					"path": "spec#maintainers",
					"fields": [
						"name",
						"email",
					],
				},
				# This isn't supported for now
				"sortcolumn": "name",
			},
			"Installation Spec": {
				"shortcut": ord("i"),
				"helptext": ("I", "Show installation specification"),
				"widget": "callout",
				"title": "Installation specification",
				"callout": infoview_view_yaml,
				"callout_args": {
					"path": "spec#install",
					"include_final_path": True,
				},
			},
			"Supported Installation Modes": {
				"shortcut": ord("I"),
				"helptext": ("[Shift] + I", "Show supported installation modes"),
				"widget": "windowwidget",
				"title": "Supported Installation Modes:",
				"headers": ["Mode:", "Supported:"],
				"itemgetter": get_list_fields,
				"itemgetter_args": {
					"path": "spec#installModes",
					"fields": [
						"type",
						"supported",
					],
				},
				# This isn't supported for now
				"sortcolumn": "type",
			},
			"Custom Resource Definitions": {
				"shortcut": ord("C"),
				"helptext": ("[Shift] + C", "Show custom resource definitions"),
				"widget": "callout",
				"title": "Custom Resource Definitions",
				"callout": infoview_view_yaml,
				"callout_args": {
					"path": "spec#customresourcedefinitions",
					"include_final_path": True,
				},
			},
		},
	},
	("__Container", ""): {
		"windowheader": "Container Info",
		"name_path": "name",
		"creation_timestamp_path": None,
		"fields": ["namespace", "name", "status", "node", "pod_ip", "age", "restarts"],
		"sortcolumn": "status",
		"activatedfun": resourceinfodispatch,
		"viewoverride": ("Pod", ""),
		"infopad": {
			"Type:": {
				"path": "container_type",
			},
			"Version:": {
				"path": "image_version",
			},
			"Image ID:": {
				"path": "image_id",
			},
		},
		"listpad": {
			"listref": "pod_references",
			"infogetter": get_pod_info,
		},
	},
	("ConfigMap", ""): {
		"windowheader": "Config Map Info",
		"fields": ["configmap", "rtype"],
		"sortcolumn": "configmap",
		"activatedfun": cmdataloop,
		"infopad": {},
		"listpad": {
			"infogetter": get_cm_data_info,
		},
	},
	("ControllerRevision", "apps"): {
		"windowheader": "Controller Revision Info",
		"infopad": {
			"Controller:": {
				"fieldname": [("C", ("main", "infoheader_shortcut")), ("ontroller:", ("main", "infoheader"))],
				"processor": field_processor_controller,
			},
		},
		"shortcuts": {
			"Controller": {
				"shortcut": ord("C"),
				"helptext": ("[Shift] + C", "Open info page for controller"),
				"call": resourceinfodispatch,
				"namespace_path": "metadata#namespace",
				"owner_references_path": "metadata#ownerReferences",
			},
		},
	},
	("CronJob", "batch"): {
		"windowheader": "Cron Job Info",
		"infopad": {
			"Concurrency Policy:": {
				"path": "spec#concurrencyPolicy",
			},
			"Parallelism:": {
				"path": "spec#jobTemplate#spec#parallelism",
			},
			"Completions:": {
				"path": "spec#jobTemplate#spec#completions",
			},
			"Starting Deadline:": {
				"extra_paths": ["spe#startingDeadlines"],
				"formatting": {
					"field_colors": [("types", "numerical")],
					"conditionals": {
						"<<<none>>>": {
							"substitute": "0",
						},
					},
					"suffix": ("s", ("types", "generic")),
				},
				"processor": field_processor_list,
			},
			"Suspend:": {
				"path": "spec#suspend",
			},
			"Schedule:": {
				"path": "spec#schedule",
			},
		},
	},
	("CSIDriver", "storage.k8s.io"): {
		"windowheader": "CSI Driver Info",
		"infopad": {
			"Attach Required:": {
				"path": "spec#attachRequired",
			},
			"Pod Info on Mount:": {
				"path": "spec#podInfoOnMount",
			},
			"Volume Lifecycle Modes:": {
				"extra_paths": "spec#volumeLifecycleModes",
				"processor": field_processor_list,
			},
		},
	},
	("CSINode", "storage.k8s.io"): {
		"windowheader": "CSI Node Info",
		"fields": ["name", "node_id", "topology_keys", "allocatable_count"],
		"sortcolumn": "name",
		"activatedfun": csi_driver_dispatch,
		"viewoverride": ("CSIDriver", "storage.k8s.io"),
		"infopad": {
			"Node:": {
				"fieldname": [("N", ("main", "infoheader_shortcut")), ("ode:", ("main", "infoheader"))],
				"processor": field_processor_owr_node,
			},
		},
		"listpad": {
			"infogetter": get_csi_node_driver_info,
		},
		"shortcuts": {
			"Node": {
				"shortcut": ord("N"),
				"helptext": ("[Shift] + N", "Open info page for node"),
				"call": resourceinfodispatch,
				"owner_references_path": "metadata#ownerReferences",
				# Note: This cannot be (kind, api_group)
				"owner_references_kind": "Node",
			},
		},
	},
	("DaemonSet", "apps"): {
		"windowheader": "Daemon Set Info",
		"fields": ["namespace", "name", "status", "node", "pod_ip", "age", "containers", "restarts"],
		"sortcolumn": "status",
		"activatedfun": resourceinfodispatch,
		"viewoverride": ("Pod", ""),
		"infopad": {
			"Controller:": {
				"fieldname": [("C", ("main", "infoheader_shortcut")), ("ontroller:", ("main", "infoheader"))],
				"processor": field_processor_controller,
				"fallback": [("strings", "none")],
			},
			"Label Selector:": {
				"path": "spec#selector#matchLabels",
				"processor": field_processor_label_selector,
			},
			"Set-based Selector:": {
				"path": "spec#selector#matchExpressions",
				"processor": field_processor_match_expressions,
			},
			"Priority Class:": {
				"fieldname": [("P", ("main", "infoheader_shortcut")), ("riority Class:", ("main", "infoheader"))],
				"path": "spec#template#spec#priorityClassName",
			},
			"Scheduler:": {
				"path": "spec#template#spec#schedulerName",
			},
			"DNS Policy:": {
				"path": "spec#template#spec#dnsPolicy",
			},
			"Restart Policy:": {
				"path": "spec#template#spec#restartPolicy",
			},
			"Host IPC:": {
				"path": "spec#template#spec#hostIPC",
				"fallback": [("False", ("types", "none"))],
			},
			"Host PID:": {
				"path": "spec#template#spec#hostPID",
				"fallback": [("False", ("types", "none"))],
			},
			"Host Network:": {
				"path": "spec#template#spec#hostNetwork",
				"fallback": [("False", ("types", "none"))],
			},
			"Nodes:": {
				"extra_paths": ["status#currentNumberScheduled", "", "status#desiredNumberScheduled", "", "status#updatedNumberScheduled", "", "status#numberAvailable", "", "status#numberMisscheduled"],
				"formatting": {
					"field_colors": [("types", "numerical")],
					"item_separators": [
						(" Current", ("types", "generic")),
						("separators", "fraction_spaced"),
						(" Desired", ("types", "generic")),
						("separators", "fraction_spaced"),
						(" Up to Date", ("types", "generic")),
						("separators", "fraction_spaced"),
						(" Available", ("types", "generic")),
						("separators", "fraction_spaced"),
					],
					"suffix": (" Misscheduled", ("types", "generic")),
					"conditionals": {
						"<<<none>>>": {
							"substitute": "0",
						},
					},
				},
				"processor": field_processor_list,
			},
			"Node Selector:": {
				"path": "spec#template#spec#nodeSelector",
				"processor": field_processor_label_selector,
			},
			"Service Account:": {
				"fieldname": [("S", ("main", "infoheader_shortcut")), ("ervice Account:", ("main", "infoheader"))],
				"path": "spec#template#spec#serviceAccountName",
			},
			"Update Strategy:": {
				"extra_paths": ["spec#updateStrategy#type", "", "spec#updateStrategy#rollingUpdate#maxSurge", "", "spec#updateStrategy#rollingUpdate#maxUnavailable", ""],
				"formatting": {
					"field_colors": [("types", "generic"), ("types", "numerical"), ("types", "numerical")],
					"item_separators": [
						(" (", ("types", "generic")),
						("Max Surge: ", ("main", "infoheader")),
						("separators", "list"),
						("Max Unavailable: ", ("main", "infoheader")),
						(")", ("types", "generic")),
					],
					"conditionals": {
						"RollingUpdate:<<<0>>>": {
							"substitute": "Rolling Update",
						},
						"<<<none>>>:<<<0>>>": {
							"substitute": "Rolling Update",
						},
						"Recreate": {
							"stop": True,
						},
					},
				},
				"processor": field_processor_list,
			},
		},
		"listpad": {
			"listgetter": "kh.get_list_by_kind_namespace(('Pod', ''), '', label_selector = kh.make_selector(deep_get(obj, 'spec#selector#matchLabels')))",
			"infogetter": get_pod_info,
		},
		"shortcuts": {
			"Controller": {
				"shortcut": ord("C"),
				"helptext": ("[Shift] + C", "Open info page for controller"),
				"call": resourceinfodispatch,
				"namespace_path": "metadata#namespace",
				"owner_references_path": "metadata#ownerReferences",
			},
			"Priority Class": {
				"shortcut": ord("P"),
				"helptext": ("[Shift] + P", "Open info page for priority class"),
				"call": resourceinfodispatch,
				"kind": ("PriorityClass", "scheduling.k8s.io"),
				"name_path": "spec#template#spec#priorityClassName",
			},
			"Service Account": {
				"shortcut": ord("S"),
				"helptext": ("[Shift] + S", "Open info page for service account"),
				"call": resourceinfodispatch,
				"kind": ("ServiceAccount", ""),
				"name_path": "spec#template#spec#serviceAccountName",
				"namespace_path": "metadata#namespace",
			},
		},
	},
	("Deployment", "apps"): {
		"fields": ["namespace", "name", "controller", "status", "node", "pod_ip", "age", "containers", "restarts"],
		"sortcolumn": "status",
		"activatedfun": resourceinfodispatch,
		"viewoverride": ("Pod", ""),
		"infopad": {
			"Label Selector:": {
				"path": "spec#selector#matchLabels",
				"processor": field_processor_label_selector,
			},
			"Set-based Selector:": {
				"path": "spec#selector#matchExpressions",
				"processor": field_processor_match_expressions,
			},
			"Node Selector:": {
				"path": "spec#template#spec#nodeSelector",
				"processor": field_processor_label_selector,
			},
			"Priority Class:": {
				"fieldname": [("P", ("main", "infoheader_shortcut")), ("riority Class:", ("main", "infoheader"))],
				"path": "spec#template#spec#priorityClassName",
			},
			"Update Strategy:": {
				"extra_paths": ["spec#strategy#type", "", "spec#strategy#rollingUpdate#maxSurge", "", "spec#strategy#rollingUpdate#maxUnavailable", ""],
				"formatting": {
					"field_colors": [("types", "generic"), ("types", "numerical"), ("types", "numerical")],
					"item_separators": [
						(" (", ("types", "generic")),
						("Max Surge: ", ("main", "infoheader")),
						("separators", "list"),
						("Max Unavailable: ", ("main", "infoheader")),
						(")", ("types", "generic")),
					],
					"conditionals": {
						"RollingUpdate:<<<0>>>": {
							"substitute": "Rolling Update",
						},
						"<<<none>>>:<<<0>>>": {
							"substitute": "Rolling Update",
						},
						"Recreate": {
							"stop": True,
						},
					},
				},
				"processor": field_processor_list,
			},
			"Condition:": {
				"path": "status#conditions",
				"processor": field_processor_dep_condition,
			},
		},
		"listpad": {
			"listgetter": "kh.get_list_by_kind_namespace(('Pod', ''), '', label_selector = kh.make_selector(deep_get(obj, 'spec#selector#matchLabels')))",
			"infogetter": get_pod_info,
		},
		"shortcuts": {
			"Priority Class": {
				"shortcut": ord("P"),
				"helptext": ("[Shift] + P", "Open info page for priority class"),
				"call": resourceinfodispatch,
				"kind": ("PriorityClass", "scheduling.k8s.io"),
				"name_path": "spec#template#spec#priorityClassName",
			},
		},
	},
	("__Deprecation", ""): {
		"windowheader": "Deprecation Info",
		"fields": ["kind", "namespace", "name"],
		"sortcolumn": "namespace",
		"extraref": "kind",
		"activatedfun": resourceinfodispatch,
		"name_path": "",
		"namespace_path": "",
		"creation_timestamp_path": "",
		"infopad": {
			"Deprecated API:": {
				"path": ["fields#resource", "fields#group"],
				"processor": field_processor_kind,
				"formatting": {
					"field_colors": [("types", "kind"), ("types", "api_group")],
					"item_separators": [("separators", "kind_apigroup")],
				},
			},
			"Deprecated Version:": {
				"processor": field_processor_list,
				"formatting": {
					"field_colors": [("types", "generic"), ("types", "generic")],
					"item_separators": [("separators", "kind")],
				},
				"extra_paths": ["fields#group", "fields#version"]
			},
			"Latest Version:": {
				"processor": field_processor_latest_api_version,
				"formatting": {
					"field_colors": [("types", "generic"), ("types", "generic")],
					"item_separators": [("separators", "kind")],
				},
				"path": ["fields#resource", "fields#group", "fields#version"],
			},
			"Planned removal:": {
				"path": "fields#removed_release",
			},
		},
		"listpad": {
			"listgetter": "get_resource_info_by_last_applied_configuration(configuration = {'apiVersion': deep_get(obj, 'fields#group') + '/' + deep_get(obj, 'fields#version', ''), 'kind': kh.guess_kind((deep_get(obj, 'fields#resource'), deep_get(obj, 'fields#group', '')))[0]}, resources = [(deep_get(obj, 'fields#resource'), deep_get(obj, 'fields#group', ''))])",
			"infogetter": generic_infogetter,
		},
	},
	("DestinationRule", "networking.istio.io"): {
		"windowheader": "Destination Rule Info",
		"infopad": {
			"Host:": {
				"path": "spec#host",
			},
			# Subsets essentially have same kind of DestinationRules as their parent (except they cannot have subsets, TTBOMK)
			# Since we need to list subsets anyway, treat all Traffic Policies for the parent as yet another subset, but
			# with a different (fake) type
			#
			# Show the information in the logpad, not in the infopad, that way it won't cause crash on resize,
			# especially once we had the remaining information
			"Traffic Policy:": {
				"processor": None,
			},
			"  Load Balancer:": {
				"path": "spec#trafficPolicy#loadBalancer",
				"extra_paths": ["spec#trafficPolicy#loadBalancer#simple", "spec#trafficPolicy#loadBalancer#consistentHash"],
				"formatting": {
					# Only one load balancer can be specified at any time;
					# by substituting None for "" we achieve a one-or-the-other behaviour
					"field_colors": [("types", "generic"), ("types", "generic")],
					"item_separators": [("", curses.A_NORMAL)],
					"conditionals": {
						"<<<none>>>": {
							"substitute": "",
						},
					},
					"suffix": ("s", curses.A_NORMAL),
				},
				"fallback": [("strings", "none")],
				"processor": field_processor_list,
			},
			"  TCP » Max Connections:": {
				"path": "spec#trafficPolicy#connectionPool#tcp#maxConnections",
				"fallback": [("2³¹-1", ("types", "numerical"))],
				"formatting": ("types", "numerical"),
			},
			"      » Connection Timeout:": {
				"path": "spec#trafficPolicy#connectionPool#tcp#connectionTimeout",
				"fallback": [("10", ("types", "numerical")), ("s", ("types", "unit"))],
				"formatting": ("types", "numerical"),
			},
			"      » Keepalive:": {
				"path": "spec#trafficPolicy#connectionPool#tcp#tcpKeepalive",
				"fallback": [("False", ("types", "none"))],
				"formatting": ("types", "numerical"),
			},
			"  HTTP » Max HTTP1 Pending Requests:": {
				"path": "spec#trafficPolicy#connectionPool#http#http1MaxPendingRequests",
				"fallback": [("2³¹-1", ("types", "numerical"))],
				"formatting": ("types", "numerical"),
			},
			"       » Max HTTP2 Requests:": {
				"path": "spec#trafficPolicy#connectionPool#http#http2MaxRequests",
				"fallback": [("2³¹-1", ("types", "numerical"))],
				"formatting": ("types", "numerical"),
			},
			"       » Max Requests Per Connection:": {
				"path": "spec#trafficPolicy#connectionPool#http#maxRequestsPerConnection",
				"fallback": [("2²⁹", ("types", "numerical"))],
				"formatting": ("types", "numerical"),
			},
			"       » Max Retries:": {
				"path": "spec#trafficPolicy#connectionPool#http#maxRetries",
				"fallback": [("2³¹-1", ("types", "numerical"))],
				"formatting": ("types", "numerical"),
			},
			"       » Idle Timeout:": {
				"path": "spec#trafficPolicy#connectionPool#http#idleTimeout",
				"fallback": [("1", ("types", "numerical")), ("h", ("types", "unit"))],
				"formatting": ("types", "numerical"),
			},
			"       » Upgrade HTTP1.1 to HTTP2:": {
				"path": "spec#trafficPolicy#connectionPool#http#h2UpgradePolicy",
				"fallback": [("Default", ("types", "none"))],
				"formatting": ("types", "numerical"),
			},
			"  Outlier Detection » Consecutive Gateway Errors:": {
				"path": "spec#trafficPolicy#outlierDetection#consecutiveGatewayErrors",
				"fallback": [("Disabled", ("types", "none"))],
				"formatting": ("types", "numerical"),
			},
			"                    » Consecutive 5xx Errors:": {
				"path": "spec#trafficPolicy#outlierDetection#consecutive5xxErrors",
				"fallback": [("5", ("types", "numerical"))],
				"formatting": ("types", "numerical"),
			},
			"                    » Ejection Sweep Interval:": {
				"path": "spec#trafficPolicy#outlierDetection#interval",
				"fallback": [("10", ("types", "numerical")), ("s", ("types", "unit"))],
				"formatting": ("types", "numerical"),
			},
			"                    » Base Ejection Time:": {
				"path": "spec#trafficPolicy#outlierDetection#baseEjectionTime",
				"fallback": [("30", ("types", "numerical")), ("s", ("types", "unit"))],
				"formatting": ("types", "numerical"),
			},
			"                    » Max Ejection Percent": {
				"path": "spec#trafficPolicy#outlierDetection#maxEjectionPercent",
				"fallback": [("10", ("types", "numerical")), ("%", ("types", "unit"))],
				"formatting": ("types", "numerical"),
				"processor": field_processor_percentage,
			},
			"                    » Min Health Percent": {
				"path": "spec#trafficPolicy#outlierDetection#minHealthPercent",
				"fallback": [("0", ("types", "numerical")), ("%", ("types", "unit"))],
				"formatting": ("types", "numerical"),
				"processor": field_processor_percentage,
			},
			# XXX: TLS
			# XXX: PortLevelSettings
			"Export To:": {
				"extra_paths": "spec#exportTo",
				"processor": field_processor_list,
				"formatting": {
					"field_colors": [("types", "generic")],
					"conditionals": {
						"<<<none>>>": {
							"substitute": "All",
						},
						"*": {
							"substitute": "All",
						},
						".": {
							"substitute": "Same",
						},
					},
				},
				"fallback": [("All", ("types", "generic"))],
			},
		},
		#listpad: { SUBSETS }
	},
	("Endpoints", ""): {
		"fields": ["addresses", "ports_ep", "status"],
		"sortcolumn": "status",
		"infopad": {},
		"listpad": {
			"infogetter": get_subsets_info,
		},
	},
	("EndpointSlice", "discovery.k8s.io"): {
		"windowheader": "Endpoint Slices Info",
		"fields": ["addresstype", "addresses", "ports_eps", "status", "target_ref", "topology"],
		"sortcolumn": "addresses",
		"infopad": {
			"Controller:": {
				"fieldname": [("C", ("main", "infoheader_shortcut")), ("ontroller:", ("main", "infoheader"))],
				"processor": field_processor_controller,
				"formatting": {"show_kind": "mixed"},
			},
		},
		"listpad": {
			"infogetter": get_eps_subsets_info,
		},
		"shortcuts": {
			"Controller": {
				"shortcut": ord("C"),
				"helptext": ("[Shift] + C", "Open info page for controller"),
				"call": resourceinfodispatch,
				"namespace_path": "metadata#namespace",
				"owner_references_path": "metadata#ownerReferences",
			},
			"Target Reference": {
				"shortcut": [ curses.KEY_ENTER, 10, 13 ],
				"helptext": ("[Enter]", "Open info page for target reference"),
				"call": resourceinfodispatch,
				"call_kind_name_namespace_selection_path": "target_ref",
				"call_kind_name_namespace_selection_order": (0, 2, 1),
			},
		},
	},
	("EnvoyFilter", "networking.istio.io"): {
		"windowheader": "Envoy Filter Info",
		"fields": ["name", "ftype"],
		"sortcolumn": "name",
		"infopad": {},
		"listpad": {
			"infogetter": get_filter_info,
		},
		"shortcuts": {
			"Workload Labels": {
				"shortcut": ord("w"),
				"helptext": ("W", "Show workload labels"),
				"widget": "windowwidget",
				"title": "Workload Labels:",
				"itemgetter": get_workload_labels,
			},
		},
	},
	("FlowSchema", "flowcontrol.apiserver.k8s.io"): {
		"windowheader": "Flow Schema Info",
		"infopad": {
			"Priority Level:": {
				"fieldname": [("P", ("main", "infoheader_shortcut")), ("riority Level:", ("main", "infoheader"))],
				"path": "spec#priorityLevelConfiguration#name",
				"processor": field_processor_priority_level,
			},
			"Matching Precedence:": {
				# FIXME: This should be numerical
				"path": "spec#matchingPrecedence",
			},
			"Distinguisher Method:": {
				"path": "spec#distinguisherMethod#type",
			},
		},
		"shortcuts": {
			"Priority Level Configuration": {
				"shortcut": ord("P"),
				"helptext": ("[Shift] + P", "Open info page for priority level configuration"),
				"call": resourceinfodispatch,
				"kind": ("PriorityLevelConfiguration", "flowcontrol.apiserver.k8s.io"),
				"name_path": "spec#priorityLevelConfiguration#name",
			},
		},
	},
	("Gateway", "networking.istio.io"): {
		"fields": ["hosts", "port"],
		"sortcolumn": "hosts",
		"infopad": {
			"Selector:": {
				"path": "spec#selector",
				"processor": field_processor_label_selector,
			},
		},
		"listpad": {
			"infogetter": get_gw_server_info,
		},
	},
	("GpuDevicePlugin", "deviceplugin.intel.com"): {
		"infopad": {
			"Image:": {
				"processor": field_processor_images,
			},
			"Init Image:": {
				"path": "spec#initImage",
				"processor": field_processor_images,
			},
			"Node Selector:": {
				"path": "spec#nodeSelector",
				"processor": field_processor_label_selector,
			},
			"Enable Monitoring:": {
				"path": "spec#enableMonitoring",
			},
			"Loglevel:": {
				"path": "spec#logLevel",
			},
			"Resource Manager:": {
				"path": "spec#resourceManager",
			},
			"Shared Device Number:": {
				"path": "spec#sharedDevNum",
			},
		},
	},
	("Group", "user.openshift.io"): {
		"infopad": {
			"Users": {
				"path": "users",
				"extra_path": "users",
				"processor": field_processor_list,
				"fallback": [("strings", "none")],
			},
		},
	},
	("HorizontalPodAutoscaler", "autoscaling"): {
		"windowheader": "Horizontal Pod Autoscaler Info",
		"infopad": {
			"Min Replicas:": {
				"path": "spec#minReplicas",
			},
			"Max Replicas:": {
				"path": "spec#maxReplicas",
			},
			"Target CPU Utilization:": {
				"path": "spec#targetCPUUtilizationPercentage",
				"processor": field_processor_percentage,
			},
			"Scale Target Reference:": {
				"fieldname": [("S", ("main", "infoheader_shortcut")), ("cale Target Reference:", ("main", "infoheader"))],
				"extra_paths": ["spec#scaleTargetRef#kind", "spec#scaleTargetRef#name"],
				"formatting": {
					"field_colors": [("types", "kind"), ("types", "generic")],
					"item_separators": [("separators", "kind")],
				},
				"processor": field_processor_list,
			},
			"Status:": {
				"path": "metadata#name",
				"extra_paths": [ "status#currentReplicas", "status#desiredReplicas" ],
				"processor": field_processor_replica_status,
			},
		},
		"shortcuts": {
			"Scale Target Reference": {
				"shortcut": ord("S"),
				"helptext": ("[Shift] + S", "Open info page for scale target reference"),
				"call": resourceinfodispatch,
				"kind_path": "spec#scaleTargetRef#kind",
				"name_path": "spec#scaleTargetRef#name",
				"namespace_path": "metadata#namespace",
			},
		},
	},
	("Identity", "user.openshift.io"): {
		"infopad": {
			"Provider:": {
				"path": "providerName",
			},
			"Provider User Name:": {
				"path": "providerUserName",
			},
			"User:": {
				"fieldname": [("U", ("main", "infoheader_shortcut")), ("ser:", ("main", "infoheader"))],
				"path": "user",
				"extra_paths": ["user#name", "user#uid"],
				"formatting": {
					"field_colors": [("types", "user"), ("types", "uid")],
					"item_separators": [("separators", "user_uid")],
				},
				"processor": field_processor_list,
			},
		},
		"shortcuts": {
			"User": {
				"shortcut": ord("U"),
				"helptext": ("[Shift] + U", "Open info page for User"),
				"call": resourceinfodispatch,
				"kind": ("User", "user.openshift.io"),
				"name_path": "user#name",
			},
		},
	},
	("Image", "caching.internal.knative.dev"): {
		"infopad": {
			"Image:": {
				"path": "spec#image",
			},
		},
	},
	("Ingress", "networking.k8s.io"): {
		"fields": ["host", "path", "backends"],
		"sortcolumn": "host",
		"infopad": {
			"Backend:": {
				# Note: path and extra_paths will be overriden
				# by field_processor_ingress_backend to handle
				# networking.k8s.io/v1 renames
				"path": "spec#backend",
				"extra_paths": ["spec#backend#serviceName", "spec#backend#servicePort"],
				"fallback": [("default-http-backend", ("types", "generic")), ("separators", "port"), ("80", ("types", "port"))],
				"formatting": {
					"field_colors": [("types", "generic"), ("types", "port")],
					"item_separators": [("separators", "port")],
				},
				"processor": field_processor_ingress_backend,
			},
			"TLS:": {
				"path": "spec#tls",
				"processor": field_processor_tls,
			}
		},
		"listpad": {
			"infogetter": get_ingress_rule_info,
		},
	},
	("__Inventory", ""): {
		"windowheader": "Inventory Info",
		"fields": ["mountpoint", "device", "fstype", "model", "disk_usage_partition", "options"],
		"sortcolumn": "mountpoint",
		"objgetter": objgetter_ansible_facts,
		"name_path": "ansible_hostname",
		"creation_timestamp_path": None,
		"infopad": {
			"Kernel Version:": {
				"extra_paths": ["ansible_kernel", "ansible_kernel_version"],
				"processor": field_processor_ansible_facts,
				"formatting": {
					"item_separators": [(" ", ("types", "generic"))],
				},
			},
			"Product Name:": {
				"extra_paths": ["ansible_product_name", "ansible_board_name"],
				"processor": field_processor_ansible_facts,
				"formatting": {
					"item_separators": [("", curses.A_NORMAL)],
					"conditionals": {
						"NA": {
							"substitute": "",
						},
						"<<<matchany>>>": {
							"stop": True,
						},
					},
				},
			},
			"BIOS version:": {
				"extra_paths": ["ansible_bios_version"],
				"processor": field_processor_ansible_facts,
			},
			"BIOS date:": {
				"extra_paths": ["ansible_bios_date"],
				"processor": field_processor_ansible_facts,
			},
			"Processor:": {
				"processor": field_processor_ansible_processor,
			},
			"Cores:": {
				"extra_paths": ["ansible_processor_cores"],
				"processor": field_processor_ansible_facts,
			},
			"Threads per Core:": {
				"extra_paths": ["ansible_processor_threads_per_core"],
				"processor": field_processor_ansible_facts,
			},
			"Sockets:": {
				"extra_paths": ["ansible_processor_count"],
				"processor": field_processor_ansible_facts,
			},
			"RAM:": {
				"extra_paths": ["ansible_memtotal_mb"],
				"processor": field_processor_ansible_facts,
				"formatting": {
					"suffix": ("MB", ("types", "unit")),
				},
			},
			"Default IPv4 Address:": {
				"extra_paths": ["ansible_default_ipv4#address"],
				"processor": field_processor_ansible_facts,
			},
			"Default IPv6 Address:": {
				"extra_paths": ["ansible_default_ipv6#address"],
				"processor": field_processor_ansible_facts,
			},
		},
		"listpad": {
			"listgetter": listgetter_ansible_volumes,
			"infogetter": generic_infogetter,
		},
	},
	("IPAMHandle", "crd.projectcalico.org"): {
		"infopad": {
			"Handle ID:": {
				"path": "spec#handleID",
			},
			"Blocks:": {
				"path": "spec#block",
				"processor": field_processor_ipam_block,
			},
		},
	},
	("IPPool", "crd.projectcalico.org"): {
		"infopad": {
			"Block Size:": {
				"path": "spec#blockSize",
			},
			"CIDR:": {
				"path": "spec#cidr",
				"processor": field_processor_cidr,
			},
			"IPIP Mode:": {
				"path": "spec#ipipMode",
			},
			"NAT Outgoing:": {
				"path": "spec#natOutgoing",
			},
			"VXLAN Mode:": {
				"path": "spec#vxlanMode",
			},
		},
	},
	("Issuer", "cert-manager.io"): {
		"infopad": {
			"Status:": {
				"processor": field_processor_condition_ready,
			},
			"Type:": {
				"processor": field_processor_issuer_type,
			},
			"CRL Distribution Points:": {
				"fallback": [("strings", "none")],
				"processor": field_processor_crl_distribution_points,
			},
			"CA Secret Name:": {
				"fieldname": [("C", ("main", "infoheader_shortcut")), ("A Secret Name:", ("main", "infoheader"))],
				"fallback": [("strings", "none")],
				"processor": field_processor_ca_secret_name,
			},
		},
		# XXX: List all certificates issued by this issuer here
		#listpad: {
		#},
		"shortcuts": {
			"CA Secret Name": {
				"shortcut": ord("C"),
				"helptext": ("[Shift] + C", "Open info page for CA Secret"),
				"call": resourceinfodispatch,
				"name_path": "spec#ca#secretName",
				"namespace_path": "metadata#namespace",
				"kind": ("Secret", ""),
			},
		},
	},
	("Job", "batch"): {
		"fields": ["namespace", "name", "status", "node", "pod_ip", "age", "containers", "restarts"],
		"sortcolumn": "name",
		"activatedfun": resourceinfodispatch,
		"viewoverride": ("Pod", ""),
		"infopad": {
			"Controller:": {
				"fieldname": [("C", ("main", "infoheader_shortcut")), ("ontroller:", ("main", "infoheader"))],
				"processor": field_processor_controller,
			},
			"Completions:": {
				"extra_paths": ["spec#completions"],
				"formatting": {
					"field_colors": [("types", "numerical")],
				},
				"processor": field_processor_list,
			},
			"Parallelism:": {
				"extra_paths": ["spec#parallelism"],
				"formatting": {
					"field_colors": [("types", "numerical")],
				},
				"processor": field_processor_list,
			},
			"Completed:": {
				"processor": field_processor_job_completion,
			}
		},
		"listpad": {
			"listgetter": "kh.get_list_by_kind_namespace(('Pod', ''), '', label_selector = 'job-name=%s' % deep_get(obj, 'spec#template#metadata#labels')['job-name'])",
			"infogetter": get_pod_info,
		},
		"shortcuts": {
			"Controller": {
				"shortcut": ord("C"),
				"helptext": ("[Shift] + C", "Open info page for controller"),
				"call": resourceinfodispatch,
				"namespace_path": "metadata#namespace",
				"owner_references_path": "metadata#ownerReferences",
			},
		},
	},
	("KubeVirt", "kubevirt.io"): {
		"infopad": {
			"Image Pull Policy:": {
				"path": "spec#imagePullPolicy",
			},
			"Observed KubeVirt Registry:": {
				"path": "status#observedKubeVirtRegistry",
			},
			"Observed Kubevirt Version:": {
				"path": "status#observedKubeVirtVersion",
			},
			"Target KubeVirt Registry:": {
				"path": "status#targetKubeVirtRegistry",
			},
			"Target Kubevirt Version:": {
				"path": "status#targetKubeVirtVersion",
			},
			"Conditions:": {
				"path": "status#conditions",
				"processor": field_processor_conditions,
			},
		},
	},
	("Lease", "coordination.k8s.io"): {
		"infopad": {
			"Holder Identity:": {
				"fieldname": [("Holder ", ("main", "infoheader")), ("I", ("main", "infoheader_shortcut")), ("dentity:", ("main", "infoheader"))],
				"processor": field_processor_holder,
			},
			"Acquire Time:": {
				"path": "spec#acquireTime",
				"processor": field_processor_timestamp,
			},
			"Lease Duration:": {
				"path": "spec#leaseDurationSeconds",
				"processor": field_processor_age,
			},
			"Lease Transitions:": {
				"path": "spec#leaseTransitions",
			},
			"Renew Time:": {
				"path": "spec#renewTime",
				"processor": field_processor_timestamp,
			},
		},
		"shortcuts": {
			"Holder": {
				"shortcut": ord("I"),
				"helptext": ("[Shift] + I", "Open info page for lease holder"),
				"call": resourceinfodispatch,
				"namespace_path": "metadata#namespace",
				"holder_identity_path": "spec#holderIdentity",
				"owner_references_path": "metadata#ownerReferences",
			},
		},
	},
	("LimitRange", ""): {
		"windowheader": "Limit Range Info",
		"fields": ["ltype", "name", "lmin", "lmax", "default_request", "default_limit", "max_lr_ratio"],
		"sortcolumn": "name",
		"infopad": {},
		"listpad": {
			"infogetter": get_limit_info,
		},
	},
	("__Log", ""): {
		"windowheader": "Playbook Log Info",
		"fields": ["index", ("name", "Task:"), "host", ("start_time", "Started At:"), ("completion_time", "Finished at:"), "retval"],
		"sortcolumn": "index",
		"objgetter": objgetter_ansible_log,
		# We're not getting the information the normal way
		"name_path": "name",
		"creation_timestamp_path": "created_at",
		"activatedfun": resourceinfodispatch,
		"viewoverride": ("__LogView", ""),
		"infopad": {
			"Path:": {
				"path": "playbook_path",
			},
			"Category:": {
				"path": "category",
			},
			"Playbook Types:": {
				"extra_paths": "playbook_types",
				"processor": field_processor_list,
			},
		},
		"listpad": {
			"infogetter": get_ansible_log_info,
		},
	},
	("__LogView", ""): {
		"windowheader": "Task Log View",
		"objgetter": objgetter_ansible_task_log,
		# We're not getting the information the normal way
		"name_path": "task",
		"namespace_path": None,
		"creation_timestamp_path": None,
		"timestamps": False,
		"infopad": {
			"Host:": {
				"path": "host",
			},
			"Return Value:": {
				"path": "retval",
			},
		},
		"logpad": {
			"infogetter": get_task_log,
		},
	},
	("MPIJob", "kubeflow.org"): {
		"windowheader": "MPI Job Info",
		"infopad": {
			"Status:": {
				"path": "spec#cleanPodPolicy",
			},
			"Clean Pod Policy:": {
				"path": "spec#cleanPodPolicy",
			},
			"Slots per Worker:": {
				"path": "spec#slotsPerWorker",
			},
			"Launcher:": {
				"processor": None,
			},
			"Launcher Replicas:": {
				"fieldname": [("  Replicas:", ("main", "infoheader"))],
				"path": "spec#mpiReplicaSpecs#Launcher#replicas",
			},
			"Worker:": {
				"processor": None,
			},
			"Worker Replicas:": {
				"fieldname": [("  Replicas:", ("main", "infoheader"))],
				"path": "spec#mpiReplicaSpecs#Worker#replicas",
			},
		},
		# FIXME: Add the containers here, maybe?
	},
	("MutatingWebhookConfiguration", "admissionregistration.k8s.io"): {
		"windowheader": "Mutating Webhook Configuration Info",
		"fields": ["name"],
		"sortcolumn": "name",
		"infopad": {},
		"listpad": {
			"infogetter": get_webhook_info,
		},
	},
	("Namespace", ""): {
		"fields": ["resource_tuple", "kind"],
		"sortcolumn": "kind",
		"activatedfun": resourceinfodispatch,
		"extraref": "kind",
		"infopad": {
			"Status:": {
				"path": "status#phase",
				"processor": field_processor_namespace_status,
			}
		},
		"listpad": {
			"infogetter": get_all_namespaced_resources,
		},
	},
	("NetworkPolicy", "networking.k8s.io"): {
		"fields": ["policy_type", "ports", "ipblock", "ipblock_exceptions", "pod_selector", "namespace_selector"],
		"sortcolumn": "policy_type",
		"infopad": {
			"Pod Selector:": {
				"path": "spec#podSelector#matchLabels",
				"processor": field_processor_label_selector,
			},
			"Policy Types:": {
				"extra_paths": "spec#policyTypes",
				"processor": field_processor_list,
			},
		},
		"listpad": {
			"infogetter": get_netpol_rule_info,
		},
	},
	("Node", ""): {
		"fields": ["namespace", "name", "controller", "status", "pod_ip", "age", "containers", "restarts"],
		"sortcolumn": "name",
		"activatedfun": resourceinfodispatch,
		"viewoverride": ("Pod", ""),
		"infopad": {
			"CSI Node:": {
				"fieldname": [("C", ("main", "infoheader_shortcut")), ("SI Node:", ("main", "infoheader"))],
				"processor": field_processor_csi_node,
				"fallback": [("strings", "unset")],
			},
			"Status:": {
				"processor": field_processor_node_status,
			},
			"Conditions:": {
				"path": "status#conditions",
				"processor": field_processor_conditions,
			},
			"Internal-IPs:": {
				#"fieldname": [("I", ("main", "infoheader_shortcut")), ("nternal-IPs:", ("main", "infoheader"))],
				# XXX: maybe we should ellipsise?
				"processor": field_processor_internal_ips,
			},
			"External-IPs:": {
				#"fieldname": [("E", ("main", "infoheader_shortcut")), ("xternal-IPs:", ("main", "infoheader"))],
				# XXX: maybe we should ellipsise?
				"processor": field_processor_external_ips,
			},
			"Architecture:": {
				"path": "status#nodeInfo#architecture",
			},
			"Kernel Version:": {
				"path": "status#nodeInfo#kernelVersion",
			},
			"kube-proxy Version:": {
				"path": "status#nodeInfo#kubeProxyVersion",
			},
			"Taints:": {
				"processor": field_processor_taints,
			},
		},
		"listpad": {
			"listgetter": "get_pods_by_node(deep_get(obj, 'metadata#name'))",
			"infogetter": get_pod_info_with_kind,
		},
		"shortcuts": {
			"CSI Node": {
				"shortcut": ord("C"),
				"helptext": ("[Shift] + C", "Open info page for the corresponding CSI Node"),
				"kind": ("CSINode", "storage.k8s.io"),
				"call": resourceinfodispatch,
				"name_path": "metadata#name",
			},
			"SSH to Node": {
				"shortcut": ord("S"),
				"helptext": ("[Shift] + S", "SSH to node"),
				"call": ssh_to_host,
				"name_path": "metadata#name",
			},
			"Open dnsutils pod on Node": {
				"shortcut": ord(""),
				"helptext": ("[Ctrl] + N", "Open a dnsutils pod on Node"),
				"widget": "executecommand",
				"command": ["<dnsutils>"],
				"kinds": ["<native>"],
			},
			"Show Package Versions": {
				"shortcut": ord("V"),
				"helptext": ("[Shift] + V", "Show package versions"),
				"widget": "callout",
				"callout": callout_show_package_versions,
			},
		},
	},
	("NodeMetrics", "metrics.k8s.io"): {
		"infopad": {
			"CPU Usage:": {
				"path": "usage#cpu",
			},
			"Memory Usage:": {
				"path": "usage#memory",
			},
			"Window:": {
				"path": "window",
			},
		},
	},
	("Peer", "kilo.squat.ai"): {
		"infopad": {
			"Persistent Keepalive:": {
				"path": "spec#persistentKeepalive",
			},
			"Allowed IPs:": {
				"fieldname": [("Allowed ", ("main", "infoheader")), ("I", ("main", "infoheader_shortcut")), ("Ps:", ("main", "infoheader"))],
				"extra_paths": "spec#allowedIPs",
				"processor": field_processor_list,
			},
		},
		"shortcuts": {
			"Allowed IPs": {
				"shortcut": ord("I"),
				"helptext": ("[Shift] + I", "Show list of allowed IPs"),
				"widget": "windowwidget",
				"title": "Allowed IPs:",
				"itemgetter": get_allowed_ips,
				# This isn't supported for now
				"sortcolumn": "key",
			},
		},
	},
	("PersistentVolume", ""): {
		"windowheader": "Persistent Volume Info",
		"fields": ["namespace", "name"],
		"sortcolumn": "name",
		"activatedfun": genericinfoloop,
		"viewoverride": ("Pod", ""),
		"infopad": {
			"Status:": {
				"processor": field_processor_pv_status,
			},
			"Storage Class:": {
				"path": "spec#storageClassName",
			},
			"Persistent Volume Claim:": {
				"fieldname": [("P", ("main", "infoheader_shortcut")), ("ersistent Volume Claim:", ("main", "infoheader"))],
				"path": "spec#claimRef",
				"extra_paths": ["spec#claimRef#namespace", "spec#claimRef#name"],
				"formatting": {
					"field_colors": [("types", "namespace"), ("types", "generic")],
					"item_separators": [("separators", "namespace")],
				},
				"processor": field_processor_list,
				"fallback": [("strings", "none")],
			},
			"Reclaim Policy:": {
				"path": "spec#persistentVolumeReclaimPolicy",
			},
			"Access Modes:": {
				"path": "spec#accessModes",
				"processor": field_processor_access_modes,
			},
			"Volume Mode:": {
				"path": "spec#volumeMode",
				"fallback": [("Filesystem", ("types", "generic"))],
			},
			"Volume Node Affinity:": {
				"path": "spec#volumeNodeAffinity",
				"processor": field_processor_label_selector,
			},
			"Capacity:": {
				"path": "spec#capacity#storage",
				# FIXME: maybe a processor here to allow for unit highlighting
				"fallback": [("", ("types", "generic"))],
			},
			"Source:": {
				"fieldname": [("S", ("main", "infoheader_shortcut")), ("ource:", ("main", "infoheader"))],
				"processor": None,
			},
			"  Type:": {
				"processor": field_processor_pv_type,
			}
		},
		"listpad": {
			"listgetter": get_pods_by_pv,
			"infogetter": get_pod_info,
		},
		"shortcuts": {
			"Persistent Volume Claim": {
				"shortcut": ord("P"),
				"helptext": ("[Shift] + P", "Open info page for Persistent Volume Claim"),
				"call": resourceinfodispatch,
				"kind": ("PersistentVolumeClaim", ""),
				"name_path": "spec#claimRef#name",
				"namespace_path": "spec#claimRef#namespace",
			},
			"Source": {
				"shortcut": ord("S"),
				"helptext": ("[Shift] + S", "Show properties for volume source"),
				"widget": "windowwidget",
				"title": "Properties:",
				"itemgetter": get_volume_properties,
				"formatting": [("windowwidget", "default"), ("windowwidget", "highlight")],
				# This isn't supported for now
				"sortcolumn": "key",
			},
		},
	},
	("PersistentVolumeClaim", ""): {
		"windowheader": "Persistent Volume Claim Info",
		"infopad": {
			"Status:": {
				"processor": field_processor_pvc_status,
			},
			"Persistent Volume:": {
				"fieldname": [("P", ("main", "infoheader_shortcut")), ("ersistent Volume:", ("main", "infoheader"))],
				"path": "spec#volumeName",
			},
			"Data Source:": {
				"fieldname": [("D", ("main", "infoheader_shortcut")), ("ata Source:", ("main", "infoheader"))],
				"path": "spec#dataSource",
				"extra_paths": ["spec#dataSource#kind", "spec#dataSource#name"],
				"fallback": [("strings", "none")],
				"formatting": {
					"item_separators": [("separators", "kind")],
				},
				"processor": field_processor_list,
			},
			"Storage Class:": {
				"path": "spec#storageClassName",
			},
			"Access Modes:": {
				"path": "status#accessModes",
				"processor": field_processor_access_modes,
			},
			"Volume Mode:": {
				"path": "spec#volumeMode",
				"fallback": [("Filesystem", curses.A_NORMAL)],
			},
			"Capacity:": {
				"path": "status#capacity#storage",
				"fallback": [("", curses.A_NORMAL)],
			},
		},
		"shortcuts": {
			"Persistent Volume": {
				"shortcut": ord("P"),
				"helptext": ("[Shift] + P", "Open info page for Persistent Volume"),
				"call": resourceinfodispatch,
				"kind": ("PersistentVolume", ""),
				"name_path": "spec#volumeName",
			},
		},
	},
	("Pod", ""): {
		"fields": ["resource_tuple", "rtype", "status", "restarts", "age", "message"],
		"sortcolumn": "rtype",
		"extraref": "kind",
		"data": True,
		"activatedfun": resourceinfodispatch,
		"infopad": {
			"Status:": {
				"processor": field_processor_pod_status,
			},
			"Conditions:": {
				"path": "status#conditions",
				"processor": field_processor_conditions,
			},
			"Priority:": {
				"extra_paths": ["spec#priority"],
				"formatting": {
					"field_colors": [("main", "highlight")],
				},
				"processor": field_processor_list
			},
			"Priority Class:": {
				"fieldname": [("P", ("main", "infoheader_shortcut")), ("riority Class:", ("main", "infoheader"))],
				"path": "spec#priorityClassName",
			},
			"QoS Class:": {
				"path": "status#qosClass",
			},
			"Pod IP:": {
				"path": "status#podIP",
			},
			"Termination Grace Period:": {
				"path": "spec#terminationGracePeriodSeconds",
				"processor": field_processor_grace_period,
			},
		},
		"listpad": {
			"infogetter": get_resource_info,
		},
		"shortcuts": {
			"Show affinities": {
				"shortcut": ord("A"),
				"helptext": ("[Shift] + A", "Show affinities"),
				"widget": "windowwidget",
				"headers": ["Type:", "Scheduling:", "Execution:", "Selector:", "Topology:"],
				"itemgetter": get_pod_affinity,
				# This isn't supported for now
				"sortcolumn": "key",
			},
			"Execute command in container": {
				"shortcut": ord(""),
				"helptext": ("[Ctrl] + E", "Execute command inside container"),
				"widget": "executecommand",
				"kinds": [("Container", ""), ("InitContainer", "")],
				"inputtitle": "Command to execute:",
				"waitforkeypress": True,
			},
			# This is only supported in Kubernetes v1.23+; do we need to add some sort of conditional support for this?
			"Open ephemeral shell in container": {
				"shortcut": ord(""),
				"helptext": ("[Ctrl] + D", "Open a debug (ephemeral) shell inside container"),
				"widget": "executecommand",
				"kinds": [("Container", ""), ("InitContainer", "")],
				"command": ["<ephemeral>"],
			},
			"Open shell in container": {
				"shortcut": ord(""),
				"helptext": ("[Ctrl] + O", "Open a shell inside container"),
				"widget": "executecommand",
				"kinds": [("Container", ""), ("InitContainer", ""), ("EphemeralContainer", "")],
				"command": [ "/bin/sh" ],
			},
			"Show tolerations": {
				"shortcut": ord("t"),
				"helptext": ("T", "Show tolerations"),
				"widget": "windowwidget",
				"headers": ["Key:", "Operator:", "Value:", "Effect:", "Timeout:"],
				"itemgetter": get_pod_tolerations,
				# This isn't supported for now
				"sortcolumn": "key",
			},
			"Priority Class": {
				"shortcut": ord("P"),
				"helptext": ("[Shift] + P", "Open info page for priority class"),
				"call": resourceinfodispatch,
				"kind": ("PriorityClass", "scheduling.k8s.io"),
				"name_path": "spec#priorityClassName",
			},
			"SSH to Node": {
				"shortcut": ord("S"),
				"helptext": ("[Shift] + S", "SSH to selected node"),
				"call": ssh_to_host,
				"call_kind_name_namespace_selection_path": "resource_tuple",
				"call_kind_name_namespace_selection_order": (-1, 1, -1),
				"kind_filter": [("Node", "")],
			},
		},
	},
	("PodDisruptionBudget", "policy"): {
		"windowheader": "Pod Disruption Budget Info",
		"infopad": {
			"Min Available:": {
				"path": "spec#minAvailable",
				"fallback": [("N/A", curses.A_NORMAL)],
			},
			"Label Selector:": {
				"path": "spec#selector#matchLabels",
				"processor": field_processor_label_selector,
			},
			"Set-based Selector:": {
				"path": "spec#selector#matchExpressions",
				"processor": field_processor_match_expressions,
			},
			"Status:": {
				"processor": None,
			},
			" Allowed Disruptions:": {
				"path": "status#disruptionsAllowed",
			},
			"     Current Healthy:": {
				"path": "status#currentHealthy",
			},
			"     Desired Healthy:": {
				"path": "status#desiredHealthy",
			},
			"               Total:": {
				"path": "status#expectedPods",
			},
		},
	},
	("PodMetrics", "metrics.k8s.io"): {
		"fields": [("name", "Container:"), "cpu_millicores", "mem_bytes"],
		"infopad": {
			"Window:": {
				"path": "window",
			},
		},
		"listpad": {
			"infogetter": get_pod_metrics_info,
		},
	},
	("PodSecurityPolicy", "policy"): {
		"windowheader": "Pod Security Policy Info",
		"infopad": {
			"Allow Privilege Escalation:": {
				"path": "spec#allowPrivilegeEscalation",
			},
			"Capabilities:": {
				"processor": None,
			},
			"  Allowed:": {
				"path": "spec#allowedCapabilities",
				"extra_paths": "spec#allowedCapabilities",
				"processor": field_processor_list,
				"fallback": [("strings", "none")],
			},
			"  Default Add:": {
				"path": "spec#defaultAddCapabilities",
				"extra_paths": "spec#defaultAddCapabilities",
				"processor": field_processor_list,
				"fallback": [("strings", "none")],
			},
			"  Required Drop:": {
				"path": "spec#requiredDropCapabilities",
				"extra_paths": "spec#requiredDropCapabilities",
				"processor": field_processor_list,
				"fallback": [("strings", "none")],
			},
			"Host Resources:": {
				"processor": None,
			},
			"  Allowed Paths:": {
				"path": "spec#allowedHostPaths",
				"extra_paths": "spec#allowedHostPaths",
				"processor": field_processor_host_paths,
				"fallback": [("strings", "none")],
			},
			"  Allow Network Use:": {
				"path": "spec#hostNetwork",
			},
			"  Permitted Network Ports:": {
				"processor": field_processor_host_ports,
			},
			"  Allow Host IPC:": {
				"path": "spec#hostIpc",
				"fallback": [("strings", "unset")],
			},
			"  Allow Host PID:": {
				"path": "spec#hostPid",
				"fallback": [("strings", "unset")],
			},
			"sysctls:": {
				"processor": None,
			},
			"  Allowed unsafe:": {
				"path": "spec#allowedUnsafeSysctls",
				"extra_paths": "spec#forbiddenSysctls",
				"processor": field_processor_list,
				"fallback": [("strings", "unset")],
			},
			"  Forbidden:": {
				"path": "spec#forbiddenSysctls",
				"extra_paths": "spec#forbiddenSysctls",
				"processor": field_processor_list,
				"fallback": [("strings", "unset")],
			},
			"SELinux:": {
				"path": "spec#seLinux#rule",
			},
			"Run as User:": {
				"path": "spec#runAsUser#rule",
			},
			"FS Group:": {
				"path": "spec#fsGroup#rule",
			},
			"Supplemental Groups:": {
				"path": "spec#supplementalGroups#rule",
			},
			"Volumes:": {
				"path": "spec#volumes",
				"extra_paths": "spec#volumes",
				"processor": field_processor_list,
				"fallback": [("strings", "none")],
			},
			"Runtime Class:": {
				"processor": None,
			},
			"  Allowed Runtime Classes:": {
				"path": "spec#allowedRuntimeClasses",
				"extra_paths": "spec#allowedRuntimeClasses",
				"processor": field_processor_list,
				"fallback": [("strings", "unset")],
			},
			"  Default Runtime Class:": {
				"path": "spec#defaultRuntimeClass",
				"fallback": [("strings", "unset")],
			},
		},
	},
	# scheduling.k8s.io
	("PriorityClass", "scheduling.k8s.io"): {
		"windowheader": "Priority Class Info",
		"fields": ["namespace", "name", "controller", "status", "node", "pod_ip", "age", "containers", "restarts"],
		"sortcolumn": "status",
		"activatedfun": resourceinfodispatch,
		"viewoverride": ("Pod", ""),
		"infopad": {
			"Value:": {
				# FIXME: This should be numerical
				"path": "value",
			},
			"Global Default:": {
				"path": "globalDefault",
			},
			"Preemption Policy:": {
				"path": "preemptionPolicy",
			},
			"Description:": {
				"path": "globalDefault",
				"processor": None,
			},
			"Description2:": {
				"fieldname": [(" ", curses.A_NORMAL)],
				"path": "description",
			},
		},
		"listpad": {
			"listgetter": "kh.get_list_by_kind_namespace(('Pod', ''), '')",
			"infogetter": get_pod_info,
			"infogetter_filters": [("\"spec#priorityClassName\"", "f\"{deep_get(obj, 'metadata#name')}\"")],
		},
	},
	("PriorityLevelConfiguration", "flowcontrol.apiserver.k8s.io"): {
		"windowheader": "Priority Level Configuration Info",
		"infopad": {
			"Priority Type:": {
				"path": "spec#type",
			},
			"Assured Concurrency Shares:": {
				# FIXME: This should be numerical
				"path": "spec#limited#assuredConcurrencyShares",
			},
			"Queues:": {
				# FIXME: This should be numerical
				"path": "spec#limited#limitResponse#queuing#queues",
			},
			"Hand Size:": {
				# FIXME: This should be numerical
				"path": "spec#limited#limitResponse#queuing#handSize",
			},
			"Queue Length Limit:": {
				# FIXME: This should be numerical
				"path": "spec#limited#limitResponse#queuing#queueLengthLimit",
			},
		},
	},
	("Prometheus", "monitoring.coreos.com"): {
		"infopad": {
			"Image:": {
				"processor": field_processor_images,
			},
			"Node Selector:": {
				"path": "spec#nodeSelector",
				"processor": field_processor_label_selector,
			},
			"Rule Selector:": {
				"path": "spec#ruleSelector#matchLabels",
				"processor": field_processor_label_selector,
			},
			"Replicas:": {
				"path": "spec#replicas",
			},
			"Service Account:": {
				"fieldname": [("S", ("main", "infoheader_shortcut")), ("ervice Account:", ("main", "infoheader"))],
				"path": "spec#serviceAccountName",
			}
		},
		"shortcuts": {
			"Alertmanager": {
				"shortcut": ord("A"),
				"helptext": ("[Shift] + A", "Show alertmanager information"),
				"widget": "windowwidget",
				"title": "Alertmanagers:",
				"headers": ["Name:", "Namespace:", "Port:"],
				"itemgetter": get_alertmanagers,
			},
			"Resources": {
				"shortcut": ord("r"),
				"helptext": ("R", "Show resource information"),
				"widget": "windowwidget",
				"title": "Resources:",
				"headers": ["Resource:", "Type:", "Value:"],
				"itemgetter": get_resources,
			},
			"Service Account": {
				"shortcut": ord("S"),
				"helptext": ("[Shift] + S", "Open info page for service account"),
				"call": resourceinfodispatch,
				"kind": ("ServiceAccount", ""),
				"name_path": "spec#serviceAccountName",
				"namespace_path": "metadata#namespace",
			},
		},
	},
	("PrometheusRule", "monitoring.coreos.com"): {
		"windowheader": "Prometheus Rule Info",
		"annotations": None,
		"fields": ["group", "rtype", "alertrecord", "duration"],
		"sortcolumn": "rtype",
		"activatedfun": resourceinfodispatch,
		"viewoverride": ("__PrometheusRuleData", "monitoring.coreos.com"),
		"infopad": {},
		"listpad": {
			"infogetter": get_promrules_info,
		},
	},
	("__PrometheusRuleData", "monitoring.coreos.com"): {
		"windowheader": "Prometheus Rule Data",
		"labels": None,
		"annotations": None,
		"name_path": None,
		"creation_timestamp_path": None,
		"timestamps": False,
		"infopad": {
			"Name:": {
				"extra_paths": ["alert", "record"],
				"processor": field_processor_list,
				"formatting": {
					"item_separators": [("", curses.A_NORMAL)],
					"field_separators": [("", curses.A_NORMAL)],
					"field_colors": [("types", "generic")],
					"conditionals": {
						"<<<none>>>": {
							"substitute": "",
						},
					},
				},
			},
			"Rule Type:": {
				"extra_paths": ["alert", "record"],
				"processor": field_processor_list,
				"formatting": {
					"item_separators": [("", curses.A_NORMAL)],
					"field_separators": [("", curses.A_NORMAL)],
					"field_colors": [("types", "generic")],
					"conditionals": {
						"<<<none>>>:<<<0>>>": {
							"substitute": "Record",
						},
						"<<<matchany>>>:<<<0>>>": {
							"substitute": "",
						},
						"<<<none>>>:<<<1>>>": {
							"substitute": "Alert",
						},
						"<<<matchany>>>:<<<1>>>": {
							"substitute": "",
						},
					}
				},
			},
			"Group:": {
				"objparam": 0,
				"processor": field_processor_empty,
			},
			"Duration:": {
				"path": "for",
				"fallback": [("strings", "unset")],
			},
		},
		"logpad": {
			"infogetter": get_promrule_expr,
		}
	},
	("ReplicaSet", "apps"): {
		"windowheader": "Replica Set Info",
		"fields": ["namespace", "name", "status", "node", "pod_ip", "age", "containers", "restarts"],
		"sortcolumn": "status",
		"activatedfun": resourceinfodispatch,
		"viewoverride": ("Pod", ""),
		"infopad": {
			"Controller:": {
				"fieldname": [("C", ("main", "infoheader_shortcut")), ("ontroller:", ("main", "infoheader"))],
				"processor": field_processor_controller,
			},
			"Label Selector:": {
				"path": "spec#selector#matchLabels",
				"processor": field_processor_label_selector,
			},
			"Set-based Selector:": {
				"path": "spec#selector#matchExpressions",
				"processor": field_processor_match_expressions,
			},
			"Priority Class:": {
				"fieldname": [("P", ("main", "infoheader_shortcut")), ("riority Class:", ("main", "infoheader"))],
				"path": "spec#template#spec#priorityClassName",
			},
			"Replicas:": {
				"processor": field_processor_replica_set_status,
			},
		},
		"listpad": {
			"listgetter": "kh.get_list_by_kind_namespace(('Pod', ''), '', label_selector = kh.make_selector(deep_get(obj, 'spec#selector#matchLabels')))",
			"infogetter": get_pod_info,
		},
		"shortcuts": {
			"Controller": {
				"shortcut": ord("C"),
				"helptext": ("[Shift] + C", "Open info page for controller"),
				"call": resourceinfodispatch,
				"namespace_path": "metadata#namespace",
				"owner_references_path": "metadata#ownerReferences",
			},
			"Priority Class": {
				"shortcut": ord("P"),
				"helptext": ("[Shift] + P", "Open info page for priority class"),
				"call": resourceinfodispatch,
				"kind": ("PriorityClass", "scheduling.k8s.io"),
				"name_path": "spec#template#spec#priorityClassName",
			},
		},
	},
	("ReplicationController", ""): {
		"windowheader": "Replication Controller Info",
		"fields": ["namespace", "name", "status", "node", "pod_ip", "age", "containers", "restarts"],
		"sortcolumn": "status",
		"activatedfun": resourceinfodispatch,
		"viewoverride": ("Pod", ""),
		"infopad": {
			"Label Selector:": {
				"path": "spec#selector",
				"processor": field_processor_label_selector,
			},
			"Replicas:": {
				"processor": field_processor_replication_controller_status,
			},
		},
		"listpad": {
			"listgetter": "kh.get_list_by_kind_namespace(('Pod', ''), '', label_selector = kh.make_selector(deep_get(obj, 'spec#selector')))",
			"infogetter": get_pod_info,
		},
	},
	("ResourceQuota", ""): {
		"windowheader": "Resource Quota Info",
		"fields": ["resource", "used", "hard"],
		"sortcolumn": "resource",
		"infopad": {
		},
		"listpad": {
			"infogetter": get_rq_item_info,
		},
	},
	("__ResourceView", ""): {
		"windowheader": "Resource Viewer",
		"timestamps": False,
		"logpad": {
			"infogetter": get_themearrays,
		},
		"shortcuts": {
			# The viewer cannot view...
			"Show Events": {},
			"Last Applied Configuration": {},
			"YAML": {},
		}
	},
	("Role", "rbac.authorization.k8s.io"): {
		"fields": ["api_group", "resource", "non_resource_urls", "resource_names", "verbs"],
		"sortcolumn": "resource",
		"infopad": {},
		"listpad": {
			"infogetter": get_policy_rule_info,
		},
	},
	("RoleBinding", "rbac.authorization.k8s.io"): {
		"windowheader": "Role Binding Info",
		"fields": ["namespace", "api_group", "name", "kind", "rtype"],
		"sortcolumn": "name",
		"activatedfun": resourceinfodispatch,
		"extraref": "kind",
		"infopad": {
			"Role:": {
				"processor": None,
			},
			"  API Group:": {
				"path": "roleRef#apiGroup",
			},
			"  Name:": {
				"path": "roleRef#name",
			},
			"  Kind:": {
				"path": "roleRef#kind",
			},
		},
		"listpad": {
			"infogetter": get_subject_info,
		},
	},
	("RuntimeClass", "node.k8s.io"): {
		"windowheader": "Runtime Class Info",
		"fields": ["resource", "overhead"],
		"sortcolumn": "resource",
		"infopad": {
			"Handler:": {
				"path": "handler",
			},
			"Node Selector:": {
				"path": "scheduling#nodeSelector",
				"processor": field_processor_label_selector,
			},
		},
		"listpad": {
			"infogetter": get_runtime_class_overhead_info,
		},
		"shortcuts": {
			"Show tolerations": {
				"shortcut": ord("t"),
				"helptext": ("T", "Show tolerations"),
				"widget": "windowwidget",
				"headers": ["Key:", "Operator:", "Value:", "Effect:", "Timeout:"],
				"itemgetter": get_pod_tolerations,
				# This isn't supported for now
				"sortcolumn": "key",
			},
		},
	},
	("Secret", ""): {
		"fields": ["key", "vtype", "vlen", "decoded_value"],
		"sortcolumn": "key",
		"infopad": {
			"Type:": {
				"path": "type",
				"processor": field_processor_secret_type,
			},
		},
		"listpad": {
			"listgetter": "deep_get(obj, 'data', {}).items()",
			"infogetter": get_key_value_info,
		},
		"shortcuts": {
			"Export": {
				"shortcut": ord("E"),
				"helptext": ("[Shift] + E", "Export selected value to a file"),
				"widget": "inputbox",
				"inputtitle": "Export to file:",
				"confirm": "os.path.exists(w_result)",
				"confirmtitle": "File already exists; overwrite?:",
				"call": export_data,
			},
		},
	},
	("Service", ""): {
		"fields": ["namespace", "name", "controller", "status", "node", "pod_ip", "age", "containers", "restarts"],
		"sortcolumn": "name",
		"activatedfun": resourceinfodispatch,
		"viewoverride": ("Pod", ""),
		"infopad": {
			"Selector:": {
				"path": "spec#selector",
				"processor": field_processor_label_selector,
			},
			"Endpoints:": {
				"fieldname": [("E", ("main", "infoheader_shortcut")), ("ndpoints:", ("main", "infoheader"))],
				"processor": field_processor_endpoints,
			},
			"Endpoint Slices:": {
				"fieldname": [("Endpoint ", ("main", "infoheader")), ("S", ("main", "infoheader_shortcut")), ("lices:", ("main", "infoheader"))],
				"processor": field_processor_endpoint_slices,
			},
			"Type:": {
				"path": "spec#type",
			},
			"Cluster-IP:": {
				"path": "spec#clusterIP",
			},
			"External-IP(s):": {
				"path": "spec#externalIPs",
				"extra_paths": "spec#externalIPs",
				"fallback": [("strings", "none")],
				"processor": field_processor_list,
			},
			"Port(s):": {
				"fieldname": [("P", ("main", "infoheader_shortcut")), ("orts:", ("main", "infoheader"))],
				"extra_paths": "spec#externalIPs",
				"processor": field_processor_svc_ports,
			},
			"Topology Keys:": {
				"path": "spec#topologyKeys",
				"extra_paths": "spec#topologyKeys",
				"fallback": [("strings", "none")],
				"processor": field_processor_list,
			},
		},
		"listpad": {
			"listgetter": "kh.get_list_by_kind_namespace(('Pod', ''), '', label_selector = kh.make_selector(deep_get(obj, 'spec#selector')))",
			"infogetter": get_pod_info_with_kind,
		},
		"shortcuts": {
			"Endpoints": {
				"shortcut": ord("E"),
				"helptext": ("[Shift] + E", "Open info page for endpoints"),
				"call": resourceinfodispatch,
				"kind": ("Endpoints", ""),
				"name_path": "metadata#name",
				"namespace_path": "metadata#namespace",
			},
			"Endpoint Slices": {
				"shortcut": ord("S"),
				"helptext": ("[Shift] + S", "List endpoint slices"),
				"widget": "windowwidget",
				"title": "Endpoint Slices:",
				"headers": ["Namespace:", "Name:"],
				"selectable": True,
				"callout": callout_obj,
				"kind": ("EndpointSlice", "discovery.k8s.io"),
				"itemgetter": get_endpoint_slices,
				# This isn't supported for now
				"sortcolumn": "namespace",
			},
			"Ports": {
				"shortcut": ord("P"),
				"helptext": ("[Shift] + P", "Show port mappings"),
				"widget": "windowwidget",
				"headers": ["Port:", "Node-Port:", "Target-Port:", "Endpoints:"],
				"itemgetter": get_svc_port_target_endpoints,
				# This isn't supported for now
				"sortcolumn": "port",
			},
		},
	},
	("ServiceAccount", ""): {
		"windowheader": "Service Account Info",
		"fields": ["stype", "namespace", "name"],
		"sortcolumn": "stype",
		"activatedfun": genericinfoloop,
		"viewoverride": ("Secret", ""),
		"infopad": {},
		"listpad": {
			"infogetter": get_sas_info,
		},
	},
	("ServiceEntry", "networking.istio.io"): {
		"windowheader": "Service Monitor Info",
		"infopad": {
			"Hosts:": {
				"extra_paths": "spec#hosts",
				"processor": field_processor_list,
			},
			"Ports:": {
				"processor": field_processor_svc_ports,
			},
			"Location:": {
				"path": "spec#location",
			},
			"Resolution:": {
				"path": "spec#resolution",
			},
		},
	},
	("ServiceMonitor", "monitoring.coreos.com"): {
		"windowheader": "Service Monitor Info",
		"fields": ["bearer_token_file", "port", "target_port", "interval", "scheme", "path", "honor_labels", "proxy_url"],
		"sortcolumn": "bearer_token_file",
		"infopad": {
			"Job Label:": {
				"path": "spec#jobLabel",
			},
			"Selector:": {
				"path": "spec#selector#matchLabels",
				"processor": field_processor_label_selector,
			},
			"Namespace Selector:": {
				"extra_paths": "spec#namespaceSelector#matchNames",
				"processor": field_processor_list,
			},
		},
		"listpad": {
			"infogetter": get_svcmon_endpoints_info,
		},
	},
	("Sidecar", "networking.istio.io"): {
		"fields": ["traffic_type", "port", "bind", "capture_mode", "default_endpoint", "hosts"],
		"sortcolumn": "traffic_type",
		"infopad": {
			"Workload Selector:": {
				"path": "spec#selector#labels",
				"processor": field_processor_label_selector,
			},
			"Outbound Traffic Policy:": {
				"path": "spec#outboundTrafficPolicy",
				"fallback": [("Default", curses.A_NORMAL)],
			},
		},
		"listpad": {
			"infogetter": get_sidecar_info,
		},
	},
	("StatefulSet", "apps"): {
		"windowheader": "Stateful Set Info",
		"fields": ["namespace", "name", "status", "node", "pod_ip", "age", "containers", "restarts"],
		"sortcolumn": "name",
		"activatedfun": resourceinfodispatch,
		"viewoverride": ("Pod", ""),
		"infopad": {
			"Controller:": {
				"fieldname": [("C", ("main", "infoheader_shortcut")), ("ontroller:", ("main", "infoheader"))],
				"processor": field_processor_controller,
			},
			"Label Selector:": {
				"path": "spec#selector#matchLabels",
				"processor": field_processor_label_selector,
			},
			"Set-based Selector:": {
				"path": "spec#selector#matchExpressions",
				"processor": field_processor_match_expressions,
			},
			"Pod Management Policy:": {
				"extra_paths": ["spec#podManagementPolicy"],
				"formatting": {
					"conditionals": {
						"<<<none>>>": {
							"substitute": "Ordered Ready",
						},
						"OrderedReady": {
							"substitute": "Ordered Ready",
						},
					},
				},
				"processor": field_processor_list,
			},
			"Replicas:": {
				# Desired
				"extra_paths": ["spec#replicas", "", "status#currentReplicas", "", "status#readyReplicas", "", "status#updatedReplicas", "" ],
				"formatting": {
					"field_colors": [("types", "numerical")],
					"item_separators": [
						(" Desired ", ("types", "generic")),
						("separators", "fraction_spaced"),
						(" Current", ("types", "generic")),
						("separators", "fraction_spaced"),
						(" Ready", ("types", "generic")),
						("separators", "fraction_spaced"),
						(" Updated", ("types", "generic")),
					],
					"conditionals": {
						"<<<none>>>": {
							"substitute": "0",
						},
					},
				},
				"processor": field_processor_list,
			},
			"Update Strategy:": {
				"extra_paths": ["spec#updateStrategy#type", "", "spec#strategy#rollingUpdate#partition", ""],
				"formatting": {
					"field_colors": [("types", "generic"), ("types", "numerical")],
					"item_separators": [
						(" (", ("types", "generic")),
						("Partition: ", ("types", "generic")),
						(")", ("types", "generic")),
					],
					"conditionals": {
						"RollingUpdate:<<<0>>>": {
							"substitute": "Rolling Update",
						},
						"<<<matchany>>>": {
							"stop": True,
						},
						"<<<none>>>:<<<0>>>": {
							"substitute": "Rolling Update",
						},
						"<<<none>>>:<<<2>>>": {
							"substitute": "1",
						},
					},
				},
				"processor": field_processor_list,
			},
		},
		"listpad": {
			"listgetter": "kh.get_list_by_kind_namespace(('Pod', ''), '', label_selector = kh.make_selector(deep_get(obj, 'spec#selector#matchLabels')))",
			"infogetter": get_pod_info,
		},
		"shortcuts": {
			"Controller": {
				"shortcut": ord("C"),
				"helptext": ("[Shift] + C", "Open info page for controller"),
				"call": resourceinfodispatch,
				"namespace_path": "metadata#namespace",
				"owner_references_path": "metadata#ownerReferences",
			},
		},
	},
	("StorageClass", "storage.k8s.io"): {
		"windowheader": "Storage Class Info",
		"infopad": {
			"Is Default Class:": {
				"processor": field_processor_default_storage_class,
			},
			"Provisioner:": {
				"path": "provisioner",
			},
			"Parameters:": {
				"path": "parameters",
				"processor": field_processor_key_value,
			},
			"Allow Volume Expansion:": {
				"path": "allowVolumeExpansion",
				"fallback": [("False", curses.A_NORMAL)],
			},
			"Mount Options:": {
				"path": "mountOptions",
				"extra_paths": "mountOptions",
				"processor": field_processor_list,
				"fallback": [("strings", "none")],
			},
			"Reclaim Policy:": {
				"path": "reclaimPolicy",
			},
			"Volume Binding Mode:": {
				"path": "volumeBindingMode",
				"fallback": [("Immediate", curses.A_NORMAL)],
			},
		},
		"shortcuts": {
			"Parameters": {
				"shortcut": ord("P"),
				"helptext": ("[Shift] + P", "Show storage class parameters"),
				"widget": "windowwidget",
				"title": "Parameters:",
				"headers": ["Parameter:", "Value:"],
				"itemgetter": get_key_value,
				"itemgetter_args": {
					"path": "parameters",
				},
				# This isn't supported for now
				"sortcolumn": "parameter",
			},
		}
	},
	("TASPolicy", "telemetry.intel.com"): {
		"windowheader": "Telemetry Aware Scheduling Policy Info",
		"fields": ["strategy", "name", "operator", "target"],
		"sortcolumn": "strategy",
		"reversible": False,
		"infopad": {},
		"listpad": {
			"infogetter": get_strategy_info,
		},
	},
	("User", "user.openshift.io"): {
		"infopad": {
			"Groups:": {
				"path": "groups",
				"extra_paths": "groups",
				"processor": field_processor_list,
				"fallback": [("strings", "none")],
			},
			"UID:": {
				"path": "metadata#uid",
			},
			"Identities:": {
				"path": "identities",
				"extra_paths": "identities",
				"processor": field_processor_list,
				"fallback": [("strings", "none")],
			},
		},
	},
	("ValidatingWebhookConfiguration", "admissionregistration.k8s.io"): {
		"windowheader": "Validating Webhook Configuration Info",
		"fields": ["name"],
		"sortcolumn": "name",
		"infopad": {},
		"listpad": {
			"infogetter": get_webhook_info,
		},
	},
	("VolumeSnapshot", "snapshot.storage.k8s.io"): {
		"windowheader": "Volume Snapshot Class Info",
		"infopad": {
			"Deletion Policy:": {
				"path": "deletionPolicy",
			},
			"Driver:": {
				"path": "driver",
			},
		},
	},
	("VirtualService", "networking.istio.io"): {
		"windowheader": "Virtual Service Info",
		"fields": ["rule_type", "destinations"],
		"sortcolumn": "rule_type",
		"infopad": {
			"Gateways:": {
				"extra_paths": "spec#gateways",
				"processor": field_processor_list,
			},
			"Hosts:": {
				"extra_paths": "spec#hosts",
				"processor": field_processor_list,
			},
			"Export To:": {
				"path": "spec#exportTo",
				"extra_paths": "spec#exportTo",
				"formatting": {
					"field_colors": [("types", "generic")],
					"conditionals": {
						"<<<none>>>": {
							"substitute": "All",
						},
						"*": {
							"substitute": "All",
						},
						".": {
							"substitute": "Same",
						},
					},
				},
				"fallback": [("All", ("types", "generic"))],
				"processor": field_processor_list,
			},
		},
		"listpad": {
			"infogetter": get_virtsvc_rule_info,
		},
	},
	("Workflow", "argoproj.io"): {
		"infopad": {
			"Entrypoint:": {
				"path": "spec#entrypoint",
			},
			"Service Account:": {
				"fieldname": [("S", ("main", "infoheader_shortcut")), ("ervice Account:", ("main", "infoheader"))],
				"path": "spec#serviceAccountName",
			},
			"Status:": {
				"path": "status#phase",
				"processor": field_processor_phase_status,
			},
		},
		"shortcuts": {
			"ServiceAccount": {
				"shortcut": ord("S"),
				"helptext": ("[Shift] + S", "Open info page for service account"),
				"call": resourceinfodispatch,
				"kind": ("ServiceAccount", ""),
				"namespace_path": "metadata#namespace",
				"name_path": "spec#serviceAccountName",
			},
		},
	},
}

views = {
	# special
	"Selector": {
		"windowheader": "",
		"commandline": ["selector"],
		"viewfunc": selectorloop,
		"fields": None,
		"skip": True,
		"is_taggable": False,
	},
	# special
	"Cluster Overview": {
		"commandline": ["clusteroverview", "clusterinfo", "overview", "co", "ci"],
		"group": "Administration",
		"viewfunc": clusteroverviewloop,
		"fields": None,
		"update_delay": 50,
		"sortcolumn": None,
		"helptext": helptexts.clusteroverview,
		"activatedfun": None,
		"listgetter": None,
		"infogetter": None,
		"is_taggable": False,
	},
	# special
	"Deprecations": {
		"commandline": ["deprecations"],
		"group": "Debugging",
		"fields": ["deprecated_api", "deprecated_removed_release"],
		"kind": ("__Deprecation", ""),
		"sortcolumn": "deprecated_api",
		"listgetter": "get_metrics_list",
		"extra_vars": {
			"listgetter": {
				"filter": "apiserver_requested_deprecated_apis",
			},
		},
		"is_taggable": False,
	},
	# special
	"Contexts": {
		"commandline": ["contexts", "context", "ctx"],
		"group": "Administration",
		"fields": ["current", "name", "cluster", "authinfo", "namespace"],
		"kind": None,
		"activatedfun": None,
		"listgetter": None,
		"infogetter": get_context_info,
		"shortcuts": {
			"Switch Context": {
				"shortcut": [ curses.KEY_ENTER, 10, 13 ],
				"helptext": ("[Enter]", "Switch cluster context"),
				"confirm": True,
				"title": "Switch cluster context",
				"call": set_cluster_context,
			},
		},
		"is_taggable": False,
	},
	# helm.cattle.io
	"Addons": {
		"commandline": ["addons", "addon"],
		"group": "API & Extendability",
		"fields": ["namespace", "name", "age"],
		"kind": ("Addon", "k3s.cattle.io"),
	},
	# monitoring.coreos.com
	"Alertmanagers": {
		"commandline": ["alertmanagers", "alertmanager", "alm"],
		"group": "Monitoring",
		"fields": ["namespace", "name", "version", "replicas", "age"],
		"kind": ("Alertmanager", "monitoring.coreos.com"),
	},
	# crd.antrea.io
	"Antrea Agent Infos": {
		"commandline": ["antreaagentinfos", "antreaagentinfo", "aai"],
		"group": "CNI",
		"fields": ["name", "healthy_antrea", "last_heartbeat_antrea"],
		"kind": ("AntreaAgentInfo", "crd.antrea.io"),
	},
	# crd.antrea.io
	"Antrea Controller Infos (Cluster)": {
		"commandline": ["antreacontrollerinfos", "antreacontrollerinfo", "aci"],
		"group": "CNI",
		"fields": ["name", "healthy_antrea", "last_heartbeat_antrea"],
		"kind": ("AntreaControllerInfo", "crd.antrea.io"),
	},
	# crd.antrea.io
	"Tiers": {
		"commandline": ["tiers", "tr"],
		"group": "CNI",
		"fields": ["name", "priority_tier", "age"],
		"kind": ("Tier", "crd.antrea.io"),
	},
	# system.antrea.io
	"Antrea Controller Infos (System)": {
		"commandline": ["antreasystemcontrollerinfos", "antreasystemcontrollerinfo"],
		"group": "CNI",
		"fields": ["name", "healthy_antrea", "last_heartbeat_antrea"],
		"kind": ("ControllerInfo", "system.antrea.io"),
	},
	# apiregistration.k8s.io
	"API Services": {
		"commandline": ["apiservices", "apiservice", "apisvcs", "apisvc", "as"],
		"group": "API & Extendability",
		"fields": ["name", "api_service", "available", "age", "api_service_status_message"],
		"kind": ("APIService", "apiregistration.k8s.io"),
	},
	# app.k8s.io
	"Applications": {
		"commandline": ["applications", "application", "apps", "app"],
		"group": "API & Extendability",
		"fields": ["namespace", "name", "age", "description"],
		"sortcolumn": "namespace",
		"kind": ("Application", "app.k8s.io"),
	},
	# kubeapps.com
	"App Repositories": {
		"commandline": ["apprepositories", "apprepository", "apprepos", "apprepo"],
		"group": "API & Extendability",
		"fields": ["namespace", "name", "repo_type", "repo_url"],
		"sortcolumn": "namespace",
		"kind": ("AppRepository", "kubeapps.com"),
	},
	# security.istio.io
	"Authorization Policies": {
		"commandline": ["authorizationpolicies", "authorizationpolicy", "authpols", "authpol"],
		"group": "Istio",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("AuthorizationPolicy", "security.istio.io"),
	},
	# crd.projectcalico.org
	"BGP Configurations": {
		"commandline": ["bgpconfigurations", "bgpconfiguration"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("BGPConfiguration", "crd.projectcalico.org"),
	},
	# crd.projectcalico.org
	"BGP Peers": {
		"commandline": ["bgppeers", "bgppeer"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("BGPPeer", "crd.projectcalico.org"),
	},
	# crd.projectcalico.org
	"Block Affinities": {
		"commandline": ["blockaffinities", "blockaffinity", "blkaffinities", "blkaffinity"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("BlockAffinity", "crd.projectcalico.org"),
	},
	# operators.coreos.com
	"Catalog Sources": {
		"commandline": ["catalogsources", "catalogsource", "catsrcs", "catsrc"],
		"group": "API & Extendability",
		"fields": ["namespace", "name", "display_name", "source_type", "publisher", "age"],
		"sortcolumn": "namespace",
		"kind":	("CatalogSource", "operators.coreos.com"),
	},
	# cert-manager.io <= rename from: certmanager.k8s.io
	"Certificates": {
		"commandline": ["certificates", "certificate", "certs", "cert"],
		"group": "Certificate Management",
		"fields": ["namespace", "name", "condition_ready", "secret", "issuer", "status_message", "age"],
		"kind": ("Certificate", "cert-manager.io"),
	},
	# cert-manager.io <= rename from: certmanager.k8s.io
	"Certificate Requests": {
		"commandline": ["certificaterequests", "certificaterequest", "certreqs", "certreq"],
		"group": "Certificate Management",
		"fields": ["namespace", "name", "condition_ready", "issuer", "status_message", "age"],
		"kind": ("CertificateRequest", "cert-manager.io"),
	},
	# certificates.k8s.io
	"Certificate Signing Requests": {
		"commandline": ["certificatesigningrequests", "certificatesigningrequest", "certsignreqs", "certsignreq", "csrs", "csr"],
		"group": "Certificate Management",
		"fields": ["name", "signername", "requestor", "csr_conditions", "age"],
		"kind": ("CertificateSigningRequest", "certificates.k8s.io"),
	},
	# cilium.io
	#"Cilium Clusterwide Network Policies": {
	#	"commandline": ["ciliumclusterwidenetworkpolicies", "ciliumclusterwidenetworkpolicy", "ccnps", "ccnp"],
	#	"group": "CNI",
	#	"fields": [FIXME],
	#	"kind": ("CiliumClusterwideNetworkPolicy", "cilium.io"),
	#},
	# cilium.io
	"Cilium Endpoints": {
		"commandline": ["ciliumendpoints", "ciliumendpoint", "ciliumeps", "ciliumep", "ceps", "cep"],
		"group": "CNI",
		"fields": ["namespace", "name", "endpoint_id", "identity_id", "ingress_enforcement", "egress_enforcement", "visibility_policy", "endpoint_state", "ipv4", "ipv6", "age"],
		"sortcolumn": "namespace",
		"kind": ("CiliumEndpoint", "cilium.io"),
		"infogetter": get_cilium_ep_info,
	},
	# cilium.io
	#"Cilium External Workloads": {
	#	"commandline": ["ciliumexternalworkloads", "ciliumexternalworkload", "cews", "cew"],
	#	"group": "CNI",
	#	"fields": [FIXME],
	#	"kind": ("CiliumExternalWorkload", "cilium.io"),
	#},
	# cilium.io
	"Cilium Identities": {
		"commandline": ["ciliumidentities", "ciliumidentity", "ciliumids", "ciliumid"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("CiliumIdentity", "cilium.io"),
	},
	# cilium.io
	#"Cilium Local Redirect Policies": {
	#	"commandline": ["ciliumlocalredirectpolicies", "ciliumlocalredirectpolicy", "clrps", "clrp"],
	#	"group": "CNI",
	#	"fields": [FIXME],
	#	"kind": ("CiliumLocalRedirectPolicy", "cilium.io"),
	#},
	# cilium.io
	#"Cilium Network Policies": {
	#	"commandline": ["ciliumnetworkpolicies", "ciliumnetworkpolicy", "ciliumnps", "ciliumnp", "cnps", "cnp"],
	#	"group": "CNI",
	#	"fields": [FIXME],
	#	"kind": ("CiliumNetworkPolicy", "cilium.io"),
	#},
	# cilium.io
	"Cilium Nodes": {
		"commandline": ["ciliumnodes", "ciliumnode", "ciliumns", "ciliumn", "cns", "cn"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("CiliumNode", "cilium.io"),
	},
	# aquasecurity.github.io
	"CIS Kube-Bench Reports": {
		"commandline": ["ciskubebenchreports", "ciskubebenchreport", "kubebench"],
		"group": "Security",
		"fields": ["name", "fail_count", "warn_count", "info_count", "pass_count", "age"],
		"kind": ("CISKubeBenchReport", "aquasecurity.github.io"),
	},
	# crd.projectcalico.org
	"Cluster Informations": {
		"commandline": ["clusterinformations", "clusterinformation", "clrinfos", "clrinfo"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("ClusterInformation", "crd.projectcalico.org"),
	},
	# cert-manager.io <= rename from: certmanager.k8s.io
	"Cluster Issuers": {
		"commandline": ["clusterissuers", "clusterissuer"],
		"group": "Certificate Management",
		"fields": ["name", "condition_ready", "age"],
		"kind": ("ClusterIssuer", "cert-manager.io"),
	},
	# nvidia.com
	"Cluster Policies": {
		"commandline": ["clusterpolicies", "clusterpolicy", "clrpols", "clrpol"],
		"group": "API & Extendability",
		"fields": ["name", "age"],
		"kind": ("ClusterPolicy", "nvidia.com"),
	},
	# rbac.authorization.k8s.io
	"Cluster Role Bindings": {
		"commandline": ["clusterrolebindings", "clusterrolebinding", "clrb"],
		"group": "Authorization & Access Control",
		"fields": ["name", "age", "role", "subject_users", "subject_groups", "subject_serviceaccounts"],
		"kind": ("ClusterRoleBinding", "rbac.authorization.k8s.io"),
	},
	# rbac.authorization.k8s.io
	"Cluster Roles": {
		"commandline": ["clusterroles", "clusterrole", "clr"],
		"group": "Authorization & Access Control",
		"fields": ["name", "age"],
		"kind": ("ClusterRole", "rbac.authorization.k8s.io"),
	},
	# operators.coreos.com
	"Cluster Service Versions": {
		"commandline": ["clusterserviceversions", "clusterserviceversion", "csvs", "csv"],
		"group": "API & Extendability",
		# Not sure where replaces is supposed to be taken from
		#"fields": ["namespace", "name", "display_name", "version", "replaces", "phase", "age"],
		"fields": ["namespace", "name", "display_name", "version", "phase", "age"],
		"sortcolumn": "namespace",
		"kind": ("ClusterServiceVersion", "operators.coreos.com"),
	},
	# metacontroller.k8s.io
	"Composite Controllers": {
		"commandline": ["compositecontrollers", "compositecontroller", "cctl", "cc"],
		"group": "API & Extendability",
		"fields": ["name", "age"],
		"kind": ("CompositeController", "metacontroller.k8s.io"),
	},
	# aquasecurity.github.io
	"Config Audit Reports": {
		"commandline": ["configauditreports", "configauditreport", "configaudits", "configaudit", "cfgaudits", "cfgaudit"],
		"group": "Security",
		"fields": ["namespace", "name", "danger_count", "warning_count", "scanner", "age"],
		"kind": ("ConfigAuditReport", "aquasecurity.github.io"),
	},
	# core API
	"Config Maps": {
		"commandline": ["configmaps", "configmap", "cm"],
		"group": "Core",
		"fields": ["namespace", "name", "data", "age"],
		"sortcolumn": "namespace",
		"helptext": helptexts.configmaplist,
		"kind": ("ConfigMap", ""),
	},
	# core API
	"Containers": {
		"commandline": ["containers", "container"],
		"group": "Core",
		"fields": ["name", "container_type", "image_version", "instances", "image_id", "pods"],
		"kind": ("__Container", ""),
		"listgetter": None,
		"infogetter": get_container_info,
		"is_taggable": False,
	},
	# apps
	"Controller Revisions": {
		"commandline": ["controllerrevisions", "controllerrevision", "cr"],
		"group": "API & Extendability",
		"fields": ["namespace", "name", "controller", "revision", "age"],
		"sortcolumn": "namespace",
		"helptext": helptexts.controllerrevisionlist,
		"kind": ("ControllerRevision", "apps"),
	},
	# batch
	"Cron Jobs": {
		"commandline": ["cronjobs", "cronjob", "cj"],
		"group": "Workloads",
		"fields": ["namespace", "name", "schedule", "suspend", "active", "concurrency_policy", "last_schedule", "age", "containers_jobtemplate"],
		"sortcolumn": "namespace",
		"kind": ("CronJob", "batch"),
	},
	# storage.k8s.io
	"CSI Drivers": {
		"commandline": ["csidrivers", "csidriver", "csidrvs", "csidrv"],
		"group": "Storage & Backups",
		"fields": ["name", "attach_required", "pod_info_on_mount", "volume_lifecycle_modes", "age"],
		"kind": ("CSIDriver", "storage.k8s.io"),
	},
	# storage.k8s.io
	"CSI Nodes": {
		"commandline": ["csinodes", "csinode"],
		"group": "Storage & Backups",
		"fields": ["name", "drivers", "age"],
		"kind": ("CSINode", "storage.k8s.io"),
	},
	# apiextensions.k8s.io
	"Custom Resource Definitions": {
		"commandline": ["customresourcedefinitions", "customresourcedefinition", "crds", "crd"],
		"group": "API & Extendability",
		"fields": ["name", "api_group_crd", "created_at"],
		"sortcolumn": "group",
		"activatedfun": resourcelistdispatch,
		"kind": ("CustomResourceDefinition", "apiextensions.k8s.io"),
	},
	# apps
	"Daemon Sets": {
		"commandline": ["daemonsets", "daemonset", "ds"],
		"group": "Workloads",
		"fields": ["namespace", "name", "desired_replicas_ds", "current_replicas_ds", "ready_replicas_ds", "uptodate_replicas_ds", "available_replicas_ds", "misscheduled_replicas", "age"],
		"sortcolumn": "namespace",
		"kind": ("DaemonSet", "apps"),
		"actions": {
			"title": "Perform action on tagged daemon sets",
			"actionlist": {
				"Restart daemon set (Rollout)": {
					"description": "Restart daemon set",
					"metadata": [("Strategy: Rollout",  ("windowwidget", "description"))],
					"confirm": True,
					"actionfunc": restart_resource_rollout,
				},
			},
		},
	},
	# apps.openshift.io
	"Deployment Configurations": {
		"commandline": ["deploymentconfigs", "deploymentconfig", "dc"],
		"group": "Workloads",
		# Not sure where to get "revision" or what it does; is it status#latestVersion?
		# Not sure where to get "triggered_by" or what it does; is it status#details#causes?
		#"fields": ["namespace", "name", "revision", "desired_replicas", "current_replicas", "triggered_by", "age"],
		"fields": ["namespace", "name", "desired_replicas", "current_replicas", "age"],
		"sortcolumn": "namespace",
		"kind": ("DeploymentConfig", "apps.openshift.io"),
	},
	# apps
	"Deployments": {
		"commandline": ["deployments", "deployment", "deploy", "dep"],
		"group": "Workloads",
		"fields": ["namespace", "name", "ready_replicas_tuple", "uptodate_replicas", "available_replicas", "age", "status_deployment", "status_message_deployment"],
		"sortcolumn": "namespace",
		"kind": ("Deployment", "apps"),
		"actions": {
			"title": "Perform action on tagged deployments",
			"actionlist": {
				#"Delete resource": {
				#	"description": "Delete resource",
				#	"category": "Resource Level",
				#	"confirm": True,
				#	"actionfunc": delete_resource,
				#},
				"Rescale deployment": {
					"description": "Rescale deployment",
					"category": "Resource Level",
					"confirm": True,
					"actionfunc": rescale_resource,
					"query": "New scale",
					"queryval": "scale",
					"queryfunc": "string",
				},
				"Pause deployment (Rollout)": {
					"description": "Pause deployment",
					"metadata": [("Strategy: Rollout",  ("windowwidget", "description"))],
					"category": "Resource Level",
					"confirm": True,
					"actionfunc": pause_resource_rollout,
				},
				"Resume deployment (Rollout)": {
					"description": "Resume deployment",
					"metadata": [("Strategy: Rollout",  ("windowwidget", "description"))],
					"category": "Resource Level",
					"confirm": True,
					"actionfunc": resume_resource_rollout,
				},
				"Restart deployment (Rollout)": {
					"description": "Restart deployment",
					"metadata": [("Strategy: Rollout",  ("windowwidget", "description"))],
					"category": "Resource Level",
					"confirm": True,
					"actionfunc": restart_resource_rollout,
				},
				"Restart deployment (Rescale)": {
					"description": "Restart deployment",
					"metadata": [("Strategy: Rescale", ("windowwidget", "description"))],
					"category": "Deprecated",
					"confirm": True,
					"actionfunc": restart_resource_rescale,
				},
				"Stop deployment (Rescale)": {
					"description": "Stop deployment",
					"metadata": [("Strategy: Rescale", ("windowwidget", "description"))],
					"category": "Deprecated",
					"confirm": True,
					"actionfunc": stop_resource_rescale,
				},
			},
		},
	},
	# networking.istio.io
	"Destination Rules": {
		"commandline": ["destinationrules", "destinationrule", "destrules", "destrule", "dstrules", "dstrule"],
		"group": "Istio",
		"fields": ["namespace", "name", "host", "age"],
		"sortcolumn": "namespace",
		"kind": ("DestinationRule", "networking.istio.io"),
	},
	# core API
	"Endpoints": {
		"commandline": ["endpoints", "endpoint", "ep"],
		"group": "Core",
		"fields": ["namespace", "name", "endpoints", "ports_ep", "age"],
		"sortcolumn": "namespace",
		"kind": ("Endpoints", ""),
	},
	# discovery.k8s.io
	"Endpoint Slices": {
		"commandline": ["endpointslices", "eps"],
		"group": "Core",
		"fields": ["namespace", "name", "addresstype", "ports_eps", "endpoints_eps", "age"],
		"sortcolumn": "namespace",
		"kind": ("EndpointSlice", "discovery.k8s.io"),
	},
	# networking.istio.io
	"Envoy Filters": {
		"commandline": ["envoyfilters", "envoyfilter"],
		"group": "Istio",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("EnvoyFilter", "networking.istio.io"),
	},
	# etcd.database.coreos.com
	#"Etcd Cluster": {
	#	"commandline": ["etcdclusters", "etcdcluster", "etcds", "etcd"],
	#	"group": "Etcd",
	#	"fields": [FIXME],
	#	"kind": ("EtcdCluster", "etcd.database.coreos.com"),
	#},
	# core API
	"Events": {
		"commandline": ["events", "event", "ev"],
		"group": "Core",
		"fields": ["namespace", "name", "seen", "event_status", "event_reason", "obj", "source", "first_seen", "count", "message"],
		"update_delay": 50,
		"sortcolumn": "seen",
		"helptext": helptexts.eventlist,
		"activatedfun": eventdispatch,
		"kind": ("Event", ""),
	},
	# crd.projectcalico.org
	"Felix Configurations": {
		"commandline": ["felixconfigurations", "felixconfiguration", "felixconfs", "felixconf"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("FelixConfiguration", "crd.projectcalico.org"),
	},
	# flowcontrol.apiserver.k8s.io
	"Flow Schemas": {
		"commandline": ["flowschemas", "flowschema"],
		"group": "Scheduling",
		"fields": ["name", "priority_level", "matching_precedence", "distinguisher_method", "age"],
		"sortcolumn": "matching_precedence",
		"kind": ("FlowSchema", "flowcontrol.apiserver.k8s.io"),
	},
	# networking.istio.io
	"Gateways": {
		"commandline": ["gateways", "gateway", "gws", "gw"],
		"group": "Istio",
		"fields": ["namespace", "name", "age"],
		"kind": ("Gateway", "networking.istio.io"),
	},
	# crd.projectcalico.org
	"Global Network Policies": {
		"commandline": ["globalnetworkpolicies", "globalnetworkpolicy", "globalnetpols", "globalnetpol"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("GlobalNetworkPolicy", "crd.projectcalico.org"),
	},
	# crd.projectcalico.org
	"Global Network Sets": {
		"commandline": ["globalnetworksets", "globalnetworkset", "globalnetsets", "globalnetset"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("GlobalNetworkSet", "crd.projectcalico.org"),
	},
	# deviceplugin.intel.com
	"GPU Device Plugins": {
		"commandline": ["gpudeviceplugins", "gpudeviceplugin", "gpudevplugins", "gpudevplugin"],
		"group": "Accelerators",
		"fields": ["name", "ready_scheduled_plugins", "node_selector", "age"],
		"kind": ("GpuDevicePlugin", "deviceplugin.intel.com"),
	},
	# integreatly.org
	"Grafanas": {
		"commandline": ["grafanas", "grafana"],
		"group": "Monitoring",
		"fields": ["namespace", "name", "age"],
		"kind": ("Grafana", "integreatly.org"),
	},
	# integreatly.org
	"Grafana Dashboards": {
		"commandline": ["grafanadashboards", "grafanadashboard", "grafanadbs", "grafanadb"],
		"group": "Monitoring",
		"fields": ["namespace", "name", "age"],
		"kind": ("GrafanaDashboard", "integreatly.org"),
	},
	# integreatly.org
	"Grafana Data Sources": {
		"commandline": ["grafanadatasources", "grafanadatasource", "grafanadatasrcs", "grafanadatasrc", "grafanadss", "grafanads"],
		"group": "Monitoring",
		"fields": ["namespace", "name", "age"],
		"kind": ("GrafanaDataSource", "integreatly.org"),
	},
	# user.openshift.io
	"Groups": {
		"commandline": ["groups", "group"],
		"group": "Users and Groups",
		"fields": ["name", "users", "age"],
		"kind": ("Group", "user.openshift.io"),
	},
	# helm.cattle.io
	"Helmcharts": {
		"commandline": ["helmcharts", "helmchart"],
		"group": "API & Extendability",
		"fields": ["namespace", "name", "age"],
		"kind": ("HelmChart", "helm.cattle.io"),
	},
	# helm.cattle.io
	"Helmchart Configs": {
		"commandline": ["helmchartconfigs", "helmchartconfig"],
		"group": "API & Extendability",
		"fields": ["namespace", "name", "age"],
		"kind": ("HelmChartConfig", "helm.cattle.io"),
	},
	# autoscaling
	"Horizontal Pod Autoscalers": {
		"commandline": ["horizontalpodautoscalers", "hpa"],
		"group": "Autoscaling",
		"fields": ["namespace", "name", "reference", "targets", "minpods", "maxpods", "current_replicas_tuple", "age"],
		"kind": ("HorizontalPodAutoscaler", "autoscaling"),
	},
	# crd.projectcalico.org
	"Host Endpoints": {
		"commandline": ["hostendpoints", "hostendpoint", "hostep"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("HostEndpoint", "crd.projectcalico.org"),
	},
	# user.openshift.io
	"Identities": {
		"commandline": ["identities", "identity"],
		"group": "Users and Groups",
		"fields": ["name", "provider", "providerusername", "user_uid_tuple", "age"],
		"kind": ("Identity", "user.openshift.io"),
	},
	# caching.internal.knative.dev
	"Images": {
		"commandline": ["images", "image"],
		"group": "Caching",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("Image", "caching.internal.knative.dev"),
	},
	# image.openshift.io
	"Images (OpenShift)": {
		"commandline": ["images.image.openshift.io", "image.image.openshift.io", "osimages", "osimage"],
		"group": "Caching",
		"fields": ["name", "docker_reference", "age"],
		"kind": ("Image", "image.openshift.io"),
	},
	# image.openshift.io
	"Image Streams": {
		"commandline": ["imagestreams", "imagestream", "is"],
		"group": "Caching",
		"fields": ["namespace", "name", "docker_repo", "tags", "age"],
		"sortcolumn": "namespace",
		"kind": ("ImageStream", "image.openshift.io"),
	},
	# image.openshift.io
	"Image Stream Tags": {
		"commandline": ["imagestreamtags", "imagestreamtag", "istags", "istag"],
		"group": "Caching",
		"fields": ["namespace", "name_version", "docker_reference_image", "age"],
		"sortcolumn": "namespace",
		"kind": ("ImageStreamTag", "image.openshift.io"),
	},
	# networking.k8s.io
	"Ingresses": {
		"commandline": ["ingresses", "ingress", "ing"],
		"group": "Network",
		"fields": ["namespace", "name", "ingress_hosts", "ingress_address", "ingress_ports", "age"],
		"sortcolumn": "namespace",
		"kind": ("Ingress", "networking.k8s.io"),
	},
	# traefik.containo.us
	"Ingress Routes": {
		"commandline": ["ingressroutes", "ingressroute", "ingroutes", "ingroute"],
		"group": "Network",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("IngressRoute", "traefik.containo.us"),
	},
	# traefik.containo.us
	"Ingress Route TCPs": {
		"commandline": ["ingressroutetcps", "ingressroutetcp", "ingroutetcps", "ingroutetcp"],
		"group": "Network",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("IngressRouteTCP", "traefik.containo.us"),
	},
	# traefik.containo.us
	"Ingress Route UDPs": {
		"commandline": ["ingressrouteudps", "ingressrouteudp", "ingrouteudps", "ingrouteudp"],
		"group": "Network",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("IngressRouteUDP", "traefik.containo.us"),
	},
	# special
	"Inventory": {
		"commandline": ["hosts", "machines", "mach", "inventory", "inv", "hosts", "baremetal"],
		"group": "Administration",
		"fields": ["name", "ips", "kubernetes_roles", "ansible_groups", "status"],
		"sortcolumn": "kubernetes_roles",
		"sortorder_reverse": True,
		"helptext": helptexts.inventorylist,
		"kind": ("__Inventory", ""),
		"labels": False,
		"listgetter": None,
		"infogetter": get_inventory_info,
		"actions": {
			"playbooklist": {
				"context": "inventory",
			},
			"actionlist": {
				"Edit resource": {},
				"View YAML dump": {},
			},
		},
		"shortcuts": {
			"SSH to Host": {
				"shortcut": ord("S"),
				"helptext": ("[Shift] + S", "SSH to host"),
				"call": ssh_to_host,
			},
		},
	},
	# crd.projectcalico.org
	"IPAM Blocks": {
		"commandline": ["ipamblocks", "ipamblock", "ipamblks", "ipamblk"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("IPAMBlock", "crd.projectcalico.org"),
	},
	# crd.projectcalico.org
	"IPAM Configs": {
		"commandline": ["ipamconfigs", "ipamconfig"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("IPAMConfig", "crd.projectcalico.org"),
	},
	# crd.projectcalico.org
	"IPAM Handles": {
		"commandline": ["ipamhandles", "ipamhandle"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("IPAMHandle", "crd.projectcalico.org"),
	},
	# crd.projectcalico.org
	"IP Pools": {
		"commandline": ["ippools", "ippool"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("IPPool", "crd.projectcalico.org"),
	},
	#"Install Plans": {
	#	"commandline": ["installplans", "installplan", "ip"],
	#	"group": "API & Extendability",
	#	"fields": FIXME,
	#	"sortcolumn": "namespace",
	#	"kind": ("InstallPlan", "operators.coreos.com"),
	#},
	# cert-manager.io <= rename from: certmanager.k8s.io
	"Issuers": {
		"commandline": ["issuers", "issuer"],
		"group": "Certificate Management",
		"fields": ["namespace", "name", "condition_ready", "age"],
		"sortcolumn": "namespace",
		"kind": ("Issuer", "cert-manager.io"),
	},
	# install.istio.io
	"Istio Operators": {
		"commandline": ["istiooperators", "istiooperator", "iop", "io"],
		"group": "Istio",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("IstioOperator", "install.istio.io"),
	},
	# jaegertracing.io
	"Jaegers": {
		"commandline": ["jaegers", "jaeger"],
		"group": "Monitoring",
		#"fields": ["namespace", "name", "status", "version", "strategy", "storage", "age"],
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("Jaeger", "jaegertracing.io"),
	},
	# batch
	"Jobs": {
		"commandline": ["jobs", "job"],
		"group": "Workloads",
		"fields": ["namespace", "name", "controller", "completions", "duration", "job_state", "age", "completion_time", "containers_template"],
		"sortcolumn": "state",
		"kind": ("Job", "batch"),
	},
	# crd.projectcalico.org
	"Kube Controllers Configurations": {
		"commandline": ["kubecontrollersconfigurations", "kubecontrollersconfiguration", "kubectlconfs", "kubectlconf"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("KubeControllersConfiguration", "crd.projectcalico.org"),
	},
	# aquasecurity.github.io
	"Kube-Hunter Reports": {
		"commandline": ["kubehunterreports", "kubehunterreport", "kubehunter"],
		"group": "Security",
		"fields": ["name", "scanner", "high_severity", "medium_severity", "low_severity", "unknown_severity", "age"],
		"kind": ("KubeHunterReport", "aquasecurity.github.io"),
	},
	# kubevirt.io
	"KubeVirts": {
		"commandline": ["kubevirts", "kubevirt", "kvs", "kv"],
		"group": "Virtualization",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("KubeVirt", "kubevirt.io"),
	},
	# coordination.k8s.io
	"Leases": {
		"commandline": ["leases", "lease"],
		"group": "Coordination",
		"fields": ["namespace", "name", "holder", "age"],
		"sortcolumn": "namespace",
		"kind": ("Lease", "coordination.k8s.io"),
	},
	# core API
	"Limit Ranges": {
		"commandline": ["limitranges", "limitrange", "limits", "lr"],
		"group": "Policy",
		"fields": ["namespace", "name", "created_at"],
		"kind": ("LimitRange", ""),
	},
	# special
	"Logs": {
		"commandline": ["logs", "log"],
		"group": "Administration",
		"fields": ["name", "action", "created_at", "log_type"],
		"sortcolumn": "created_at",
		"sortorder_reverse": True,
		"kind": ("__Log", ""),
		"labels": False,
		"listgetter": None,
		"infogetter": get_log_info,
		"actions": {
			"actionlist": {
				"Delete log": {
					"description": "Delete log",
					"confirm": True,
					"actionfunc": delete_logs,
				},
				# Override:
				"Edit resource": {},
				"View YAML dump": {},
			},
		},
	},
	# traefik.containo.us
	"Middlewares": {
		"commandline": ["middlewares", "middleware"],
		"group": "Network",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("Middleware", "traefik.containo.us"),
	},
	# traefik.containo.us
	"Middleware TCPs": {
		"commandline": ["middlewaretcps", "middlewaretcp"],
		"group": "Network",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("MiddlewareTCP", "traefik.containo.us"),
	},
	# kubeflow.org
	"MPI Jobs": {
		"commandline": ["mpijobs", "mpijob", "mpij", "mj"],
		# "group": "Workloads",
		"group": "Kubeflow",
		"fields": ["namespace", "name", "age", "status_latest_condition", ("status_message", "Message:")],
		"sortcolumn": "namespace",
		"kind": ("MPIJob", "kubeflow.org"),
	},
	# admissionregistration.k8s.io
	"Mutating Webhook Configurations": {
		"group": "API & Extendability",
		"commandline": ["mutatingwebhookconfigurations",  "mutatingwebhookconfigurations", "mwhcs", "mwhc"],
		"fields": ["name", "webhooks", "age"],
		"kind": ("MutatingWebhookConfiguration", "admissionregistration.k8s.io"),
	},
	# core API
	"Namespaces": {
		"commandline": ["namespaces", "namespace", "ns"],
		"group": "Core",
		"fields": ["name", ("phase", "Status:"), "age"],
		"kind": ("Namespace", ""),
		"shortcuts": {
			"Create namespace": {
				"shortcut": [ ord("C") ],
				"helptext": ("[Shift] + C", "Create a new namespace"),
				"confirm": True,
				"title": "Create namespace",
				"call": create_namespace,
				"query": "New namespace",
				"queryval": "namespace",
				"queryfunc": "string",
			},
		},
	},
	# special
	"Network Info": {
		"commandline": ["network", "net", "cni"],
		"group": "Administration",
		"viewfunc": networkinfoloop,
		"fields": None,
		"sortcolumn": None,
		"helptext": helptexts.networkinfo,
		"activatedfun": None,
		"listgetter": None,
		"infogetter": None,
	},
	# networking.k8s.io
	"Network Policies": {
		"commandline": ["networkpolicies", "networkpolicy", "netpols", "netpol"],
		"group": "Network",
		"fields": ["namespace", "name", "pod_selector", "age"],
		"sortcolumn": "namespace",
		"kind": ("NetworkPolicy", "networking.k8s.io"),
	},
	# core API
	"Nodes": {
		"commandline": ["nodes", "node", "no"],
		"group": "Core",
		"fields": ["name", "node_status", "kubernetes_roles", "age", "kubelet_version", "internal_ips", "os", "kernel", "container_runtime", "cpu", "mem", "taints"],
		"helptext": helptexts.nodelist,
		"activatedfun": genericinfoloop,
		"kind": ("Node", ""),
		"actions": {
			"playbooklist": {
				"context": "node",
			},
		},
		"shortcuts": {
			"SSH to Node": {
				"shortcut": ord("S"),
				"helptext": ("[Shift] + S", "SSH to node"),
				"call": ssh_to_host,
			},
		},
	},
	# metrics.k8s.io
	"Node Metrics": {
		"commandline": ["nodemetrics", "nodemetric"],
		"group": "Monitoring",
		"fields": ["name", "timestamp", "window"],
		"kind": ("NodeMetrics", "metrics.k8s.io"),
	},
	# opentelemetry.io
	"OpenTelemetry Collectors": {
		"commandline": ["opentelemetrycollectors", "opentelemetrycollectors", "otelcols", "otelcol"],
		"group": "Monitoring",
		#"fields": ["namespace", "name", "mode", "version", "age"],
		"fields": ["namespace", "name", "age"],
		"kind": ("OpenTelemetryCollector", "opentelemetry.io"),
	},
	# operators.coreos.com
	#"Operators": {
	#	"commandline": ["operators", "operator"],
	#	"group": "API & Extendability",
	#	"fields": FIXME,
	#	"kind": ("Operator", "operators.coreos.com"),
	#},
	# operators.coreos.com
	"Operator Groups": {
		"commandline": ["operatorgroups", "operatorgroup", "og"],
		"group": "API & Extendability",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("OperatorGroup", "operators.coreos.com"),
	},
	# packages.operators.coreos.com
	"Package Manifests": {
		"commandline": ["packagemanifests", "packagemanifest", "pkgmanifests", "pkgmanifest"],
		"group": "API & Extendability",
		"fields": ["namespace", "name", "catalogue", "age"],
		"kind": ("PackageManifest", "packages.operators.coreos.com"),
	},
	# kilo.squat.ai
	"Peers": {
		"commandline": ["peers", "peer"],
		"group": "CNI",
		"fields": ["name", "age"],
		"kind": ("Peer", "kilo.squat.ai"),
	},
	# core API
	"Persistent Volumes": {
		"commandline": ["persistentvolumes", "persistentvolume", "pv"],
		"group": "Storage & Backups",
		"fields": ["name", "pv_status", "capacity", "age", "access_modes", "storage_class", "claim", "reclaim_policy", "volume_mode", "reason"],
		"kind": ("PersistentVolume", ""),
	},
	# core API
	"Persistent Volume Claims": {
		"commandline": ["persistentvolumeclaims", "persistentvolumeclaim", "pvc"],
		"group": "Storage & Backups",
		"fields": ["namespace", "name", "pvc_status", "volume", "capacity", "access_modes", "storage_class", "age", "volume_mode"],
		"kind": ("PersistentVolumeClaim", ""),
	},
	# core API
	"Pods": {
		"commandline": ["pods", "pod", "po"],
		"group": "Workloads",
		"fields": ["namespace", "name", "controller", "status", "node", "pod_ip", "age", "containers", "restarts"],
		"sortcolumn": "status",
		"activatedfun": genericinfoloop,
		"kind": ("Pod", ""),
		"infogetter": get_pod_info,
		"extra_vars": {
			"infogetter": {
				"show_kind": "mixed",
				"show_evicted": False,
			},
		},
		"actions": {
			"title": "Perform action on tagged pods",
			"actionlist": {
				"Delete resource": {
					"description": "Delete resource",
					"confirm": True,
					"actionfunc": delete_resource,
				},
				"Force delete resource": {
					"description": "Delete resource (No grace period)",
					"confirm": True,
					"actionfunc": force_delete_resource,
				},
			},
		},
		"shortcuts": {
			"Show Evicted Pods": {
				"shortcut": ord("E"),
				"helptext": ("[Shift] + E", "Show / Hide evicted pods"),
				"eval": "_tmp = extra_vars['infogetter']['show_evicted']; extra_vars['infogetter']['show_evicted'] = not _tmp",
			},
		},
	},
	# policy
	"Pod Disruption Budgets": {
		"commandline": ["poddisruptionbudgets", "poddisruptionbudget", "pdb"],
		"group": "Policy",
		"fields": ["namespace", "name", "min_available", "max_unavailable", "allowed_disruptions", "age"],
		"kind": ("PodDisruptionBudget", "policy"),
	},
	# monitoring.coreos.com
	#"Pod Monitors": {
	#	"commandline": ["podmonitors", "podmonitor", "podmons", "podmon"],
	#	"group": "Monitoring",
	#	"fields": [FIXME],
	#	"kind": ("PodMonitor", "monitoring.coreos.com"),
	#},
	# metrics.k8s.io
	"Pod Metrics": {
		"commandline": ["podmetrics", "podmetric"],
		"group": "Monitoring",
		"fields": ["namespace", "name", "mem_pod_metrics_total", "cpu_pod_metrics_total", "container_count", "timestamp", "window"],
		"kind": ("PodMetrics", "metrics.k8s.io"),
	},
	# policy
	"Pod Security Policies": {
		"commandline": ["podsecuritypolicies", "podsecuritypolicy", "psp"],
		"group": "Policy",
		"fields": ["name", "privileged", "capabilities", "selinux", "runasuser", "fsgroup", "supgroups", "readonlyrootfs", "volumes", "age"],
		"kind": ("PodSecurityPolicy", "policy"),
	},
	# core API
	#"Pod Templates": {
	#	"commandline": ["podtemplates", "podtemplate"],
	#	"group": "Core",
	#	"fields": [FIXME],
	#	"sortcolumn": "namespace",
	#	"kind": ("PodTemplate", ""),
	#},
	# scheduling.k8s.io
	"Priority Classes": {
		"commandline": ["priorityclasses", "priorityclass", "pc"],
		"group": "Scheduling",
		"fields": ["name", "priority", "global_default", "age"],
		"kind": ("PriorityClass", "scheduling.k8s.io"),
	},
	# flowcontrol.apiserver.k8s.io
	"Priority Level Configurations": {
		"commandline": ["prioritylevelconfigurations", "prioritylevelconfiguration", "priolevelconfs", "priolevelconf"],
		"group": "Scheduling",
		"fields": ["name", "priority_type", "assured_concurrency_shares", "queues", "hand_size", "queue_length_limit", "age"],
		"kind": ("PriorityLevelConfiguration", "flowcontrol.apiserver.k8s.io"),
	},
	# kubeflow.org
	"Profiles": {
		"commandline": ["profiles", "profile"],
		"group": "Kubeflow",
		"fields": ["name", "age"],
		"kind": ("Profile", "kubeflow.org"),
	},
	# monitoring.coreos.com
	"Prometheuses": {
		"commandline": ["prometheuses", "prometheus", "proms", "prom"],
		"group": "Monitoring",
		"fields": ["namespace", "name", "version", "replicas", "age"],
		"kind": ("Prometheus", "monitoring.coreos.com"),
	},
	# monitoring.coreos.com
	"Prometheus Rules": {
		"commandline": ["prometheusrules", "prometheusrule", "promrules", "promrule"],
		"group": "Monitoring",
		"fields": ["namespace", "name", "age"],
		"kind": ("PrometheusRule", "monitoring.coreos.com"),
	},
	# reaper.cassandra-reaper.io
	"Cassandra Data Centers": {
		"commandline": ["cassandradatacenters", "cassandradatacenter", "cassdcs", "cassdc"],
		"group": "Storage & Backups",
		"fields": ["namespace", "name", "age"],
		"kind": ("CassandraDatacenter", "cassandra.datastax.com"),
	},
	# reaper.cassandra-reaper.io
	"Reapers": {
		"commandline": ["reapers", "reaper"],
		"group": "Storage & Backups",
		"fields": ["namespace", "name", "age"],
		"kind": ("Reaper", "reaper.cassandra-reaper.io"),
	},
	# apps
	"Replica Sets": {
		"commandline": ["replicasets", "replicaset", "rs"],
		"group": "Workloads",
		"fields": ["namespace", "name", "ready_replicas", "current_replicas", "desired_replicas", "max_replicas", "age"],
		"kind": ("ReplicaSet", "apps"),
		"actions": {
			"title": "Perform action on tagged replica sets",
			"actionlist": {
				"Rescale replica set": {
					"description": "Rescale replica set",
					"confirm": True,
					"actionfunc": rescale_resource,
					"query": "New scale",
					"queryval": "scale",
					"queryfunc": "string",
				},
			},
		},
	},
	# core API
	"Replication Controllers": {
		"commandline": ["replicationcontrollers", "replicationcontroller", "rcs", "rc"],
		"group": "Workloads",
		"fields": ["namespace", "name", "ready_replicas", "current_replicas", "desired_replicas", "age"],
		"kind": ("ReplicationController", ""),
		"actions": {
			"title": "Perform action on tagged replication controllers",
			"actionlist": {
				"Rescale replication controller": {
					"description": "Rescale replication controller",
					"confirm": True,
					"actionfunc": rescale_resource,
					"query": "New scale",
					"queryval": "scale",
					"queryfunc": "string",
				},
			},
		},
	},
	# core API
	"Resource Quotas": {
		"commandline": ["resourcequotas", "resourcequota", "quotas", "quota", "rq"],
		"group": "Policy",
		"fields": ["namespace", "name", "age", "request", "limit"],
		"kind": ("ResourceQuota", ""),
	},
	# rbac.authorization.k8s.io
	"Role Bindings": {
		"commandline": ["rolebindings", "rolebinding", "rb"],
		"group": "Authorization & Access Control",
		"fields": ["namespace", "name", "age"],
		"kind": ("RoleBinding", "rbac.authorization.k8s.io"),
	},
	# rbac.authorization.k8s.io
	"Roles": {
		"commandline": ["roles", "role"],
		"group": "Authorization & Access Control",
		"fields": ["namespace", "name", "age"],
		"kind": ("Role", "rbac.authorization.k8s.io"),
	},
	# node.k8s.io
	"Runtime Classes": {
		"commandline": ["runtimeclasses", "runtimeclass"],
		"group": "Policy",
		"fields": ["name", "handler", "age"],
		"kind": ("RuntimeClass", "node.k8s.io"),
	},
	# core API
	"Secrets": {
		"commandline": ["secrets", "secret", "sec"],
		"group": "Core",
		"fields": ["namespace", "name", "secret_type", "data", "age"],
		"kind": ("Secret", ""),
	},
	# policy.linkerd.io
	"Server Authorizations": {
		"commandline": ["serverauthorizations", "serverauthorization", "saz"],
		"group": "Network",
		"fields": ["namespace", "name", "server"],
		"sortcolumn": "namespace",
		"kind": ("ServerAuthorization", "policy.linkerd.io"),
	},
	# policy.linkerd.io
	"Servers": {
		"commandline": ["servers", "server", "srv"],
		"group": "Network",
		"fields": ["namespace", "name", "port_linkerd", "protocol"],
		"sortcolumn": "namespace",
		"kind": ("Server", "policy.linkerd.io"),
	},
	# traefik.containo.us
	"Servers Transports": {
		"commandline": ["serverstransports", "serverstransport"],
		"group": "Network",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("ServersTransport", "traefik.containo.us"),
	},
	# core API
	"Services": {
		"commandline": ["services", "service", "svc"],
		"group": "Core",
		"fields": ["namespace", "name", "svc_type", "cluster_ip", "external_ip", "ports_svc", "age"],
		"sortcolumn": "namespace",
		"kind": ("Service", ""),
	},
	# core API
	"Service Accounts": {
		"commandline": ["serviceaccounts", "serviceaccount", "sa"],
		"group": "Authorization & Access Control",
		"fields": ["namespace", "name", "nrofsecrets", "age"],
		"sortcolumn": "namespace",
		"kind": ("ServiceAccount", ""),
	},
	# servicecertsigner.config.openshift.io
	"Service Cert Signer Operator Configs": {
		"commandline": ["servicecertsigneroperatorconfigs", "servicecertsigneroperatorconfig", "svccertsigneropconfs", "svccertsigneropconf"],
		"group": "Authorization & Access Control",
		"fields": ["name", "age"],
		"kind": ("ServiceCertSignerOperatorConfig", "servicecertsigner.config.openshift.io"),
	},
	# networking.istio.io
	"Service Entries": {
		"commandline": ["serviceentries", "serviceentry", "svcentries", "svcentry"],
		"group": "Istio",
		"fields": ["namespace", "name", "age", "hosts", "ports_svc", "location", "resolution"],
		"kind": ("ServiceEntry", "networking.istio.io"),
	},
	# monitoring.coreos.com
	"Service Monitors": {
		"commandline": ["servicemonitors", "servicemonitor", "svcmons", "svcmon"],
		"group": "Monitoring",
		"fields": ["namespace", "name", "age"],
		"kind": ("ServiceMonitor", "monitoring.coreos.com"),
	},
	# linkerd.io
	"Service Profiles": {
		"commandline": ["serviceprofiles", "serviceprofile", "sp"],
		"group": "Network",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("ServiceProfile", "linkerd.io"),
	},
	# networking.istio.io
	"Sidecars": {
		"commandline": ["sidecars", "sidecar"],
		"group": "Istio",
		"fields": ["namespace", "name", "age"],
		"kind": ("Sidecar", "networking.istio.io"),
	},
	# dex.coreos.com
	"Signing Keys": {
		"commandline": ["signingkeies", "signingkeys", "signingkey", "sigkeys", "sigkey"],
		"group": "Dex",
		"fields": ["namespace", "name", "age"],
		"kind": ("SigningKey", "dex.coreos.com"),
	},
	# apps
	"Stateful Sets": {
		"commandline": ["statefulsets", "statefulset", "sts"],
		"group": "Workloads",
		"fields": ["namespace", "name", "ready_replicas_tuple", "age"],
		"sortcolumn": "namespace",
		"kind": ("StatefulSet", "apps"),
		"actions": {
			"title": "Perform action on tagged stateful sets",
			"actionlist": {
				"Rescale stateful set": {
					"description": "Rescale stateful set",
					"confirm": True,
					"actionfunc": rescale_resource,
					"query": "New scale",
					"queryval": "scale",
					"queryfunc": "string",
				},
				"Restart stateful set (Rollout)": {
					"description": "Restart stateful set",
					"metadata": [("Strategy: Rollout",  ("windowwidget", "description"))],
					"confirm": True,
					"actionfunc": restart_resource_rollout,
				},
			},
		},
	},
	# storage.k8s.io
	"Storage Classes": {
		"commandline": ["storageclasses", "storageclass", "scs", "sc"],
		"group": "Storage & Backups",
		"fields": ["name", "provisioner", "reclaim_policy", "volume_binding_mode", "allow_volume_expansion", "age"],
		"kind": ("StorageClass", "storage.k8s.io"),
	},
	# operators.coreos.com
	#"Subscriptions": {
	#	"commandline": ["subscriptions", "subscription", "subs", "sub"],
	#	"group": "API & Extendability",
	#	"fields": ["namespace", "name", "catalogsource"],
	#	"sortcolumn": "namespace",
	#	"kind": ("Subscription", "operators.coreos.com"),
	#},
	# telemetry.intel.com
	"Telemetry Aware Scheduling Policies": {
		"commandline": ["taspolicies", "taspolicy", "tas"],
		"group": "Scheduling",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("TASPolicy", "telemetry.intel.com"),
	},
	# monitoring.coreos.com
	#"Thanos Rulers": {
	#	"commandline": ["thanosrulers", "thanosruler", "thrules", "thrule"],
	#	"group": "Monitoring",
	#	"fields": [FIXME],
	#	"sortcolumn": "namespace",
	#	"kind": ("ThanosRuler", "monitoring.coreos.com"),
	#},
	# traefik.containo.us
	"TLS Options": {
		"commandline": ["tlsoptions", "tlsoption"],
		"group": "Network",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("TLSOption", "traefik.containo.us"),
	},
	# traefik.containo.us
	"TLS Stores": {
		"commandline": ["tlsstores", "tlsstore"],
		"group": "Network",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("TLSStore", "traefik.containo.us"),
	},
	# traefik.containo.us
	"Traefik Services": {
		"commandline": ["traefikservices", "traefikservice"],
		"group": "Network",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("TraefikService", "traefik.containo.us"),
	},
	# admissionregistration.k8s.io
	"Validating Webhook Configurations": {
		"commandline": ["validatingwebhookconfigurations",  "validatingwebhookconfigurations", "vwhcs", "vwhc"],
		"group": "API & Extendability",
		"fields": ["name", "webhooks", "age"],
		"kind": ("ValidatingWebhookConfiguration", "admissionregistration.k8s.io"),
	},
	# networking.istio.io
	"Virtual Services": {
		"commandline": ["virtualservices",  "virtualservice", "virtsvcs", "virtsvc"],
		"group": "Istio",
		"fields": ["namespace", "name", "gateways", "hosts", "age"],
		"kind": ("VirtualService", "networking.istio.io"),
	},
	# storage.k8s.io
	#"Volume Attachments": {
	#	"commandline": ["volumeattachments", "volumeattachment", "volattachments", "volattachment"],
	#	"group": "Storage & Backups",
	#	"fields": [FIXME],
	#	"kind": ("VolumeAttachment", "storage.k8s.io"),
	#},
	# snapshot.storage.k8s.io
	#"VolumeSnapshot": {
	#	"commandline": ["volumesnapshots", "volumesnapshot", "volsnaps", "volsnap"],
	#	"group": "Storage & Backups",
	#	"fields": [FIXME],
	#	"kind": ("VolumeSnapshot", "snapshot.storage.k8s.io"),
	#},
	# snapshot.storage.k8s.io
	"Volume Snapshot Classes": {
		"commandline": ["volumesnapshotclasses", "volumesnapshotclass", "volsnapclasses", "volsnapclass"],
		"group": "Storage & Backups",
		"fields": ["name", "age"],
		"kind": ("VolumeSnapshotClass", "snapshot.storage.k8s.io"),
	},
	# snapshot.storage.k8s.io
	#"Volume Snapshot Contents": {
	#	"commandline": ["volumesnapshotcontents", "volumesnapshotcontent", "volsnapcontents", "volsnapcontent"],
	#	"group": "Storage & Backups",
	#	"fields": [FIXME],
	#	"kind": ("VolumeSnapshotContent", "snapshot.storage.k8s.io"),
	#},
	# aquasecurity.github.io
	"Vulnerability Reports": {
		"commandline": ["vulnerabilityreports", "vulnerabilityreport", "vulns", "vuln"],
		"group": "Security",
		"fields": ["namespace", "name", "repository", "tag", "scanner", "age"],
		"kind": ("VulnerabilityReport", "aquasecurity.github.io"),
	},
	# user.openshift.io
	"Users": {
		"commandline": ["users", "user"],
		"group": "Users and Groups",
		"fields": ["name", "fullname", "uid", "identities", "age"],
		"kind": ("User", "user.openshift.io"),
	},
	# argoproj.io
	"Workflows": {
		"commandline": ["workflows", "workflow", "wf"],
		"group": "Workflows",
		"fields": ["namespace", "name", "age"],
		"sortcolumn": "namespace",
		"kind": ("Workflow", "argoproj.io"),
	},
}

def setupui(stdscr):
	# Hide the cursor
	curses.curs_set(False)
	# Disable CTRL+C, CTRL+Z, etc.
	curses.raw()
	# Enable mouse support
	curses_helper.set_mousemask(-1)
	curses_helper.init_curses()

	while True:
		if views.get(defaultview) is not None:
			if "viewfunc" in views[defaultview]:
				viewfunc = views[defaultview]["viewfunc"]
			else:
				viewfunc = genericlistloop
			viewfunc(stdscr, defaultview)
		else:
			iktprint([("Error: ", "error"), ("Unknown view “", "default"), (defaultview, "argument"), ("“; check “", "default"), (IKT_CONFIG_FILE, "path"), ("“ and all files in “", "default"), (IKT_CONFIG_FILE_DIR, "path"), ("“for typos.", "default")], stderr = True)
			sys.exit()

def list_namespaces():
	tmp = kh.get_list_by_kind_namespace(("Namespace", ""), "")
	namespaces = iktlib.join_tuple_list([deep_get(item, "metadata#name") for item in tmp], _tuple = "argument", separator = (", ", "separator"))
	iktprint([("Valid namespaces: ", "default")] + namespaces + [(".", "separator")])

def list_views():
	viewfields = []
	maxviewlen = 0
	maxkindlen = 0

	for view in views:
		viewref = views[view]
		if "skip" in viewref:
			continue
		if "kind" not in viewref or viewref["kind"] is None:
			continue
		if "fields" not in viewref:
			continue

		fields = []
		for field in viewref["fields"]:
			if type(field) == tuple:
				fieldname = field[0]
			else:
				fieldname = field
			if "sortcolumn" in viewref and viewref["sortcolumn"] == fieldname:
				fields.append((fieldname, "emphasis"))
			elif "sortcolumn" not in viewref and fieldname == "name":
				fields.append((fieldname, "emphasis"))
			else:
				fields.append((fieldname, "default"))
		kind = viewref["kind"][0]
		api_group = viewref["kind"][1]
		if api_group == "":
			viewfields.append((view, f"{kind}", fields))
			maxkindlen = max(maxkindlen, len(f"{kind}"))
		else:
			viewfields.append((view, f"{kind}.{api_group}", fields))
			maxkindlen = max(maxkindlen, len(f"{kind}.{api_group}"))
		maxviewlen = max(maxviewlen, len(view))

	iktprint([("View:", "header"), (f"{''.ljust(maxviewlen + 2 - len('View:'))}", "default"), ("Kind:", "header"), (f"{''.ljust(maxkindlen + 2 - len('Kind:'))}", "default"), ("Supported fields:", "header")])
	for view, kind, fields in viewfields:
		joined_fields = iktlib.join_tuple_list(fields, separator = (", ", "separator"))
		try:
			iktprint([(f"{view.ljust(maxviewlen + 2)}{kind.ljust(maxkindlen + 2)}", "default")] + joined_fields)
		except:
			iktprint([(f"view: {view}\nmaxviewlen + 2: {maxviewlen + 2}\nkind: {kind}\nmaxkindlen + 2: {maxkindlen + 2}\nfields: {fields}", "default")])
			sys.exit()

	iktprint([(f"\nList views can be customised to only show select fields by editing “", "default"), (IKT_CONFIG_FILE, "path"), ("“.\n", "default")])
	iktprint([(f"Simply add “", "default"), ("field:", "emphasis"), ("“ followed by a list of the fields you want the list to a section named like", "default")])
	iktprint([("the list view you want to customise.\n", "default")])

	iktprint([(f"Note that the fields “", "default"), ("name", "emphasis"), ("“ and, if applicable “", "default"), ("namespace", "emphasis"), ("“, will unconditionally", "default")])
	iktprint([("be included even if the list of fields doesn't include them; including them", "default")])
	iktprint([("in the list will only allow reordering the fields.\n", "default")])

	iktprint([(f"The fields in this list are those supported by ", "default"), (f"{about.ui_program_name}", "programname"), ("; it is NOT an exhaustive list", "default")])
	iktprint([("of fields available in the resource.\n", "default")])

	iktprint([("The highlighted field for each View is the default sortcolumn.", "default")])

def usage(progname):
	helptext = [(f"{about.ui_program_name} ", "programname")]
	helptext += helptexts.usage

	helptext += [("Valid views: ", "description")]

	first = True
	for view in sorted(views):
		if views[view].get("skip", False):
			continue
		if first:
			first = False
		else:
			helptext += [(", ", "separator")]
		helptext += [(views[view]["commandline"][0], "emphasis")]
	helptext += [(".\n", "separator")]

	iktprint(helptext)

def version(progname):
	iktprint([(f"{about.ui_program_name} ", "programname"), (f"{about.ui_program_version}", "version")])
	iktprint([(f"{about.program_suite_full_name} ({about.program_suite_name}) ", "programname"), (f"{about.program_suite_version}", "version")])
	print()
	print(about.copyright)
	print(about.license)
	print()
	print(PROGRAMAUTHORS)
	return 0

def set_default_view(view):
	global defaultview

	if defaultview == "":
		defaultview = view
	else:
		iktprint([(about.ui_program_name, "programname"), (": ", "default"), ("VIEW", "argument"), (" can only be specified once.", "default")], stderr = True)
		iktprint([("Try “", "default"), (f"{about.ui_program_name} ", "programname"), ("help", "command"), ("“ for more information.", "default")], stderr = True)
		sys.exit(2)

def checkforview(arg):
	for view in views:
		for cl in views[view]["commandline"]:
			if arg == cl:
				return view
	return None

def customise_listviews():
	for view in views:
		viewref = views[view]
		if "skip" in viewref:
			continue
		if "kind" not in viewref or viewref["kind"] is None:
			continue
		if "fields" not in viewref:
			continue

		# OK, we've now (hopefully) skipped all views that don't have a list view
		fields = deep_get(iktconfig, f"{view}#fields", [])
		if len(fields) == 0:
			continue

		custom_fields = {}

		if "namespace" in viewref["fields"] and "namespace" not in fields:
			custom_fields["namespace"] = None
		if "name" not in fields:
			custom_fields["name"] = None

		for field in fields:
			if field not in fields:
				iktprint([("Error: ", "error"), ("“", "default"), (field, "option"), ("“ is not a valid field for the view “", "default"), (view, "argument"), ("“; aborting.", "default")], stderr = True)
				sys.exit(2)
			elif field in custom_fields:
				iktprint([("Warning: ", "warning"), ("“", "default"), (field, "option"), ("“ is specified twice (or more) for the view “", "default"), (view, "argument"), ("“; ignoring.", "default")], stderr = True)
				continue
			custom_fields[field] = None
		viewref["custom_fields"] = list(custom_fields)

		# Next up it's time for the field blacklist
		if "blacklist" in deep_get(iktconfig, f"{view}", {}):
			blacklist = deep_get(iktconfig, f"{view}#fields", [])
			viewref["field_blacklist"] = blacklist

	# As a special case, the configuration option "Inventory#ping_hosts"
	# modifies the blacklist, but *only* if the field blacklist is unset
	ping_hosts = deep_get(iktconfig, "Inventory#ping_hosts", "Lazy")
	if ping_hosts == "Never" and "field_blacklist" not in views["Inventory"]:
		views["Inventory"]["field_blacklist"] = ["status"]

def main():
	defaultthemefile = f"{THEMEDIR}/{DEFAULT_THEME}"

	global iktconfig
	global initial_name
	global initial_namespace
	global initial_container
	global read_only_mode
	global namespace
	global defaultview

	iktconfig = iktlib.read_iktconfig()
	themefile = deep_get(iktconfig, "Global#theme")
	if themefile is None or len(themefile) == 0:
		themefile = defaultthemefile
	elif themefile.startswith("{HOME}/"):
		themefile = f"{HOMEDIR}/{themefile[len('{HOME}/'):]}"
	elif "/" not in themefile:
		themefile = f"{THEMEDIR}/{themefile}"
	read_theme(themefile, defaultthemefile)
	init_iktprint(themefile)

	tmpdefaultview = deep_get(iktconfig, "Global#defaultview", "")
	curses_configuration.abouttext = helptexts.about
	curses_configuration.mousescroll_enable = deep_get(iktconfig, "Mouse#enablescroll", False)
	# These values are ignored when scrolling is disabled, so the defaults don't matter
	curses_configuration.mousescroll_up = deep_get(iktconfig, "Mouse#scrollup", 0)
	curses_configuration.mousescroll_down = deep_get(iktconfig, "Mouse#scrolldown", 0)
	# Used by the ansible module
	ansible_configuration.ansible_forks = deep_get(iktconfig, "Ansible#forks", 5)
	ansible_user = deep_get(iktconfig, "Ansible#ansible_user")
	if ansible_user is None or len(ansible_user) == 0:
		ansible_user = getuser()
	ansible_configuration.ansible_user = ansible_user
	ansible_configuration.ansible_password = deep_get(iktconfig, "Ansible#ansible_password")
	ansible_configuration.disable_strict_host_key_checking = deep_get(iktconfig, "Nodes#disablestricthostkeychecking", False)
	ansible_configuration.save_logs = deep_get(iktconfig, "Ansible#save_logs", True)
	# Customises the list views
	customise_listviews()

	y = 1

	while y < len(sys.argv):
		arg = sys.argv[y]

		tmp = checkforview(arg)

		if tmp is not None and defaultview == "":
			defaultview = tmp
			y += 1
			continue

		elif arg == "list-views":
			list_views()
			sys.exit(0)
		elif arg == "--namespace":
			namespace = sys.argv[y + 1]
			y += 1
		elif arg == "--read-only":
			read_only_mode = True
		elif arg == "list-namespaces":
			init_kubernetes_client()
			list_namespaces()
			sys.exit(0)
		elif arg in ["help", "--help"]:
			usage(os.path.basename(sys.argv[0]))
			sys.exit(0)
		elif arg in ["version", "--version"]:
			version(os.path.basename(sys.argv[0]))
			sys.exit(0)
		elif arg.startswith("--"):
			print("%s: unrecognised option “%s”" % (os.path.basename(sys.argv[0]), arg), file = sys.stderr)
			iktprint([("Try “", "default"), (f"{about.ui_program_name} ", "programname"), ("help", "command"), ("“ for more information.", "default")], stderr = True)
			sys.exit(2)
		else:
			# After we've received a view we assume that non-flag arguments
			# that follow are used as shortcuts to a specific item
			if initial_name is None:
				tmp = re.match(r"^(.+)/(.+)$", arg)
				if tmp is not None:
					if "/" in tmp[1]:
						print("%s: invalid syntax “%s %s”" % (os.path.basename(sys.argv[0]), os.path.basename(sys.argv[0]), " ".join(sys.argv[1:])), file = sys.stderr)
						print("Format is either NAMESPACE/NAME or NAME NAMESPACE", file = sys.stderr)
						iktprint([("Try “", "default"), (f"{about.ui_program_name} ", "programname"), ("help", "command"), ("“ for more information.", "default")], stderr = True)
						sys.exit(2)
					else:
						initial_namespace = tmp[1]
						initial_name = tmp[2]
				else:
					initial_name = arg
			elif initial_namespace is None:
				if "/" in arg:
					print("%s: invalid syntax “%s %s”" % (os.path.basename(sys.argv[0]), os.path.basename(sys.argv[0]), " ".join(sys.argv[1:])), file = sys.stderr)
					print("Format is either NAMESPACE/NAME or NAME NAMESPACE", file = sys.stderr)
					iktprint([("Try “", "default"), (f"{about.ui_program_name} ", "programname"), ("help", "command"), ("“ for more information.", "default")], stderr = True)
					sys.exit(2)
				else:
					initial_namespace = arg
			elif initial_container is None:
				initial_container = arg
			else:
				print("%s: unrecognised option “%s”" % (os.path.basename(sys.argv[0]), arg), file = sys.stderr)
				iktprint([("Try “", "default"), (f"{about.ui_program_name} ", "programname"), ("help", "command"), ("“ for more information.", "default")], stderr = True)
				sys.exit(2)
		y += 1

	if defaultview == "":
		if tmpdefaultview is not None and tmpdefaultview != "":
			defaultview = tmpdefaultview
		else:
			set_default_view("Selector")

	if initial_name is not None:
		# Role, RoleBinding, ClusterRole, and ClusterRoleBinding can contain ":"
		# Most resources do not allow ":" at all, and for pods we use it to indicate a container name

		colon_whitelist = { "Roles", "Role Bindings", "Cluster Roles", "Cluster Role Bindings" }

		if defaultview not in colon_whitelist:
			tmp = re.match(r"^(.+):(.*)$", initial_name)
			if tmp is not None:
				if ":" in tmp[1]:
					iktprint([(f"{about.ui_program_name}", "programname"), (": invalid syntax “", "default"), (f"{about.ui_program_name} ", "programname"), (f"{' '.join(sys.argv[1:])}", "argument"), ("”", "default")], stderr = True)
					iktprint([("Format is either ", "default"), ("NAMESPACE/NAME:", "argument"), ("{", "separator"), ("CONTAINER", "argument"), (",", "separator"), ("CONFIGMAP", "argument"), ("}", "separator"), (" or ", "default"), ("NAME NAMESPACE ", "argument"), ("{", "separator"), ("CONTAINER", "argument"), (",", "separator"), ("CONFIGMAP", "argument"), ("}", "separator")], stderr = True)
					iktprint([("Try “", "default"), (f"{about.ui_program_name} ", "programname"), ("help", "command"), ("“ for more information.", "default")], stderr = True)
					sys.exit(2)
				else:
					initial_name = tmp[1]
					initial_container = tmp[2]

	# Containers and Config Maps can have a third level (container/data)
	if initial_container is not None and not (defaultview in ["Pods", "Config Maps"]):
		iktprint([(f"{about.ui_program_name}", "programname"), (": invalid syntax “", "default"), (f"{about.ui_program_name} ", "programname"), (f"{' '.join(sys.argv[1:])}", "argument"), ("”", "default")], stderr = True)
		iktprint([(f"CONTAINER", "argument"), ("/", "separator"), ("CONFIGMAP", "argument"), (" only makes sense in ", "default"), ("pod", "command"), ("/", "separator"), ("configmap", "command"), (" view.", "default")], stderr = True)
		iktprint([("Try “", "default"), (f"{about.ui_program_name} ", "programname"), ("help", "command"), ("“ for more information.", "default")], stderr = True)
		sys.exit(2)

	# We don't need escape sequences, so cut down on the delay to 25ms
	os.environ.setdefault('ESCDELAY', '25')
	init_kubernetes_client()

	wrapper(setupui)

if __name__ == "__main__":
	main()
