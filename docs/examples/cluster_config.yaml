apiVersion: "v1alpha4"
kind: "ClusterDeployment"

# Note: templated installs are only supported for kubeadm and rke2 at the moment
# This configuration uses:
#   ClusterConfiguration kubeadm.k8s.io/v1beta3 [Kubernetes 1.23+]
#   InitConfiguration kubeadm.k8s.io/v1beta3 [Kubernetes 1.23+]
#   KubeletConfiguration kubelet.config.k8s.io/v1beta1 [Kubernetes 1.10+]
#   KubeProxyConfiguration kubeproxy.config.k8s.io/v1alpha1 [Kubernetes 1.12+]
options:
  # Kubernetes distro; valid options:
  # kubeadm
  # rke2
  # [optional; default: kubeadm]
  # distro: "kubeadm"

  # Kubernetes version; valid options:
  # 1.<minor>
  # latest
  # [optional; default: latest]
  # version: "latest"

  # Pod network CIDR:
  # [optional; default: 10.244.0.0/16]
  # podNetworkCIDR: "10.244.0.0/16"

  # Verbose output; valid options:
  # true
  # false
  # [optional; default: false]
  # verbose: false

  # What CNI to use; valid options:
  # antrea
  # calico
  # canal
  # cilium
  # flannel
  # kube-router
  # weave
  # none [no cni will be installed]
  # [optional; default: cilium]
  # cni: "cilium"

  # Hardcoded version of CNI;
  # useful (for instance) when behind a corporate firewall that ratelimits github API requests.
  # [optional; default: latest]
  # cni_version: "v0.16.16"

  # Hardcoded version of CNI executable;
  # useful (for instance) when behind a corporate firewall that ratelimits github API requests.
  # [optional; default: latest]
  # cni_executable_version: "v0.16.16"

  # Should the program prompt for an ansible/host password; valid options:
  # true
  # false
  # [optional; default: true]
  # promptForPassword: true

  # Should ansible logs be saved for later viewing via cmu logs; valid options:
  # true
  # false
  # [optional; default: false]
  # saveAnsibleLogs: false

# Cluster-wide configuration and configuration for the control plane(s)
cluster:
  # Control plane(s) for the cluster:
  # Note: Currently only one control plane is supported
  # [mandatory]
  controlPlanes:
  - "mycontrolplane"

  # CRI to use for the control plane(s); valid options:
  # containerd
  # docker-shim [Kubernetes < 1.24]
  # cri-o
  # [optional; default: containerd]
  cri: "containerd"

  # Hardcoded version of CRI;
  # useful (for instance) when behind a corporate firewall that ratelimits github API requests.
  # Currently only supported for cri-o.
  # [optional; default: auto]
  # cri_version: "1.30"

  # Name of the cluster:
  # [mandatory]
  clusterName: "mycluster"

  # Should the control plane(s) accept normal workloads?
  # [optional; default: false]
  # untaintControlPlanes: false

  # Cluster configuration templates:
  # The configuration templates can either be embedded in the data
  # section, or provided via a path; Only one of data/path is valid for each type
  # of configuration; providing both is NOT supported.
  # The post-processed templates will be concatenated together before passing them
  # to kubeadm.
  #
  # Note: you can add any valid cluster configuration here; the configuration here
  # is just the equivalent of the default configuration used by cmtadm/cmt,
  # as well as the extra options provided by cmtadm
  configurationTemplates:
    rke2Configuration:
      # Embedded cluster configuration template:
      data: |
        cni:
        - none
        # This will be auto-populated from the cluster settings
        cluster-cidr: "{{ pod_network_cidr }}"

        # This will be auto-populated from the cluster settings
        container-runtime-endpoint: "{{ cri_socket }}"

        # disable is a comma-separated list of built-in features to disable;
        # unless you want to use rke2-canal as your CNI you should keep it here
        # if you make changes
        # [optional; default: "rke2-canal"]
        # disable: "rke2-canal"

        # disable-cloud-controller: true
        #kube-apiserver-arg:
        # feature-gates is a comma-separated string
        # To Enable Dynamic Resource Allocations use the following:
        # Note: also requires alpha APIs and the matching feature gates
        # to be enabled for controllerManager, kubeProxy and scheduler in cluster_configuration,
        # kube_proxy_configuration, and kubelet_configuration
        # [optional; default: null]
        # - feature-gates=DynamicResourceAllocation=true
        # runtime-config is a comma-separated string;
        # [optional; default: null]
        # - runtime-config=api/alpha=true
        #kube-controller-manager-arg:
        # feature-gates is a comma-separated string
        # To Enable Dynamic Resource Allocations use the following:
        # Note: also requires alpha APIs and the matching feature gates
        # to be enabled for apiServer and scheduler in cluster_configuration,
        # as well as in kube_proxy_configuration, and kubelet_configuration
        # [optional; default: null]
        # - feature-gates=DynamicResourceAllocation=true
        kubelet-arg:
        # feature-gates is a comma-separated string
        # To Enable Dynamic Resource Allocations use the following:
        # Note: also requires alpha APIs and the matching feature gates
        # to be enabled for apiServer and scheduler in cluster_configuration,
        # as well as in kube_proxy_configuration, and kubelet_configuration
        # [optional; default: null]
        # - feature-gates=DynamicResourceAllocation=true
        # The cgroup driver to use
        - cgroup-driver=systemd
        #kube-proxy-arg:
        # feature-gates is a comma-separated string
        # To Enable Dynamic Resource Allocations use the following:
        # Note: also requires alpha APIs and the matching feature gates
        # to be enabled for apiServer and scheduler in cluster_configuration,
        # as well as in kube_proxy_configuration, and kubelet_configuration
        # [optional; default: ""]
        # - feature-gates=DynamicResourceAllocation=true
        #kube-scheduler-arg:
        # feature-gates is a comma-separated string
        # To Enable Dynamic Resource Allocations use the following:
        # Note: also requires alpha APIs and the matching feature gates
        # to be enabled for apiServer and scheduler in cluster_configuration,
        # as well as in kube_proxy_configuration, and kubelet_configuration
        # [optional; default: ""]
        # - feature-gates=DynamicResourceAllocation=true

      # Path to cluster configuration template, relative to the playbook directory:
      # path: "templates/config/rke2_configuration.yaml.j2"
    clusterConfiguration:
      # Embedded cluster configuration template:
      data: |
        apiVersion: kubeadm.k8s.io/v1beta3
        kind: ClusterConfiguration
        apiServer:
          extraArgs:
            # feature-gates is a comma-separated string
            # To Enable Dynamic Resource Allocations use the following:
            # Note: also requires alpha APIs and the matching feature gates
            # to be enabled for controllerManager, kubeProxy and scheduler in cluster_configuration,
            # kube_proxy_configuration, and kubelet_configuration
            # [optional; default: false]
            # feature-gates: "DynamicResourceAllocation=true"
            # runtime-config is a comma-separated string
            # runtime-config: "api/alpha=true"
        # This will be auto-populated from the cluster settings
        clusterName: "{{ cluster_name }}"
        controllerManager:
          extraArgs:
            # feature-gates is a comma-separated string
            # To Enable Dynamic Resource Allocations use the following:
            # Note: also requires alpha APIs and the matching feature gates
            # to be enabled for apiServer and scheduler in cluster_configuration,
            # as well as in kube_proxy_configuration, and kubelet_configuration
            # [optional; default: false]
            # feature-gates: "DynamicResourceAllocation=true"
        networking:
          # This will be auto-populated from the cluster settings
          podSubnet: "{{ pod_network_cidr }}"
        scheduler:
          extraArgs:
            # feature-gates is a comma-separated string
            # To Enable Dynamic Resource Allocations use the following:
            # Note: also requires alpha APIs and the matching feature gates
            # to be enabled for apiServer and controllerManager in cluster_configuration,
            # as well as in kube_proxy_configuration, and kubelet_configuration
            # [optional; default: false]
            # feature-gates: "DynamicResourceAllocation=true"

      # Path to cluster configuration template, relative to the playbook directory:
      # path: "templates/config/cluster_configuration.yaml.j2"

    initConfiguration:
      # Embedded cluster configuration template:
      data: |
        apiVersion: kubeadm.k8s.io/v1beta3
        kind: InitConfiguration
        nodeRegistration:
          # This will be auto-populated from the cluster settings
          criSocket: "{{ cri_socket }}"
          # Use this to add labels to the control planes
          # kubeletExtraArgs:
          #   node-labels: "foo=bar"
          # This will be auto-populated
        {% if taint_control_plane %}
          taints:
          - key: node-role.kubernetes.io/control-plane
            effect: NoSchedule
        {% else %}
          taints: []
        {% endif %}

      # Path to cluster configuration template, relative to the playbook directory:
      # path: "templates/config/init_configuration.yaml.j2"

    kubeletConfiguration:
      # Embedded cluster configuration template:
      data: |
        apiVersion: kubelet.config.k8s.io/v1beta1
        kind: KubeletConfiguration
        # Enable Dynamic Resource Allocations?
        # Note: also requires alpha APIs and the matching feature gates
        # to be enabled for apiServer, controllerManager, kubeProxy # and scheduler in cluster_configuration
        # [optional; default: false]
        featureGates:
          # DynamicResourceAllocation: true
        # This will be auto-populated from the cluster settings
        containerRuntimeEndpoint: "{{ cri_socket }}"

      # Path to cluster configuration template, relative to the playbook directory:
      # path: "templates/config/kubelet_configuration.yaml.j2"

    kubeProxyConfiguration:
      # Embedded cluster configuration template:
      data: |
        apiVersion: kubeproxy.config.k8s.io/v1alpha1
        kind: KubeProxyConfiguration
        # Enable Dynamic Resource Allocations?
        # Note: also requires alpha APIs and the matching feature gates
        # to be enabled for apiServer, controllerManager and scheduler in cluster_configuration
        # as well as for kubelet in kubelet_configuration
        # [optional; default: false]
        featureGates:
          # DynamicResourceAllocation: true

      # Path to cluster configuration template, relative to the playbook directory:
      # path: "templates/config/kube_proxy_configuration.yaml.j2"

# Configuration for nodes
nodes:
  # Global node options
  options:
    # CRI to use for the nodes; valid options:
    # containerd
    # docker-shim [Kubernetes < 1.24]
    # cri-o
    # [optional; if not provided the cluster-wide default will be used]
    # cri: "containerd"

    # Hardcoded version of CRI;
    # useful (for instance) when behind a corporate firewall that ratelimits github API requests.
    # Currently only supported for cri-o.
    # [optional; default: auto]
    # cri_version: "1.30"

    # Ansible parallelism:
    # [optional; default: 5]
    # forks: 5

    # Cordon nodes on join:
    # [optional; default: false]
    # cordonOnJoin: false

    # Node configuration templates:
    # The configuration templates can either be embedded in the data
    # section, or provided via a path; Only one of data/path is valid for each type
    # of configuration; providing both is NOT supported.
    # The post-processed templates will be concatenated together before passing them
    # to kubeadm. Currently only JoinConfiguration is supported for worker nodes.
    # The templates will be processed by jinja; the following variables are available:
    # cri_socket
    # join_token
    # ca_cert_hash
    # control_plane_ip
    # control_plane_port
    # control_plane_path
    # pod_network_cidr
    configurationTemplates:
      rke2Configuration:
        # Embedded cluster configuration template:
        data: |
          # This will be auto-populated from the cluster settings
          server: "https://{{ control_plane_ip }}:9345"
          # This will be auto-populated from the cluster settings
          container-runtime-endpoint: "{{ cri_socket }}"
          # This will be auto-populated from the newly generated cluster
          token: "{{ node_token }}"
          # Arguments to pass to kubelet
          kubelet-arg:
          # feature-gates is a comma-separated string
          # To Enable Dynamic Resource Allocations use the following:
          # Note: also requires alpha APIs and the matching feature gates
          # to be enabled for apiServer and scheduler in cluster_configuration,
          # as well as in kube_proxy_configuration, and kubelet_configuration
          # [optional; default: ""]
          # - feature-gates=DynamicResourceAllocation=true
          # The cgroup driver to use
          - cgroup-driver=systemd
          # Arguments to pass to kube-proxy
          #kube-proxy-arg:
          # feature-gates is a comma-separated string
          # To Enable Dynamic Resource Allocations use the following:
          # Note: also requires alpha APIs and the matching feature gates
          # to be enabled for apiServer and scheduler in cluster_configuration,
          # as well as in kube_proxy_configuration, and kubelet_configuration
          # [optional; default: ""]
          # - feature-gates=DynamicResourceAllocation=true

        # Path to node configuration template, relative to the playbook directory:
        # path: "templates/config/rke2_agent_configuration.yaml.j2"

      joinConfiguration:
        # Embedded node configuration template:
        data: |
          apiVersion: kubeadm.k8s.io/v1beta3
          kind: JoinConfiguration
          nodeRegistration:
            # This will be auto-populated from the node settings; only change if you know what you're doing
            criSocket: "{{ cri_socket }}"
            # Use this to add labels to the nodes
            # kubeletExtraArgs:
            #   node-labels: "foo=bar"
            # A list of taints; an empty list means no taints
            # This will be auto-populated; add extra taints below if wanted
          {% if cordon_nodes_on_join %}
            taints:
            - key: node.kubernetes.io/unschedulable
              effect: NoSchedule
          {% else %}
            taints: []
          {% endif %}
          discovery:
            bootstrapToken:
              # This will be auto-populated; only change if you know what you're doing
              token: "{{ join_token }}"
              caCertHashes:
              # This will be auto-populated; only change if you know what you're doing
              - "sha256:{{ ca_cert_hash }}"
              # This will be auto-populated; only change if you know what you're doing
              apiServerEndpoint: "{{ control_plane_ip }}:{{ control_plane_port }}{{ control_plane_path }}"

        # Path to node configuration template, relative to the playbook directory:
        # path: "templates/config/join_configuration.yaml.j2"

  # Every group of nodes has their own group name;
  # they will be added to that group in the ansible inventory
  # (as well as the nodes group and the group for the cluster);
  # Currently different Ansible groups are *not* processed in parallel.
  groups:
    # <group name1>:
    #   # A list of all nodes that belong to this group; leave empty
    #   # if only configuring a control plane at this time,
    #   # or using virtualised nodes.
    #   nodes:
    #   - "<node 1>"
    #
    #   # Set this to true if the nodes are virtualised;
    #   # if so the "nodes" should be empty or unset.
    #   # [optional; default: false]
    #   # virtualise: false
    #
    #   # If the nodes in this group are virtual machines this is the host system for those VMs.
    #   # [mandatory if virtualised nodes are used]
    #   # vmHost: "vmhost"
    #
    #   # Virtualised nodes names are templated;
    #   # item.0 is the high-byte of the range, item.1 is the low-byte.
    #   # [optional; default: "vmnode-<<<item.0>>>-<<<item.1>>>"]
    #   # vmNameTemplate: "vmnode-<<<item.0>>>-<<<item.1>>>"
    #
    #   # Virtualised nodes IP-addresses are templated;
    #   # item.0 is the high-byte of the range, item.1 is the low-byte.
    #   # [optional; default: "192.168.<<<item.0>>>.<<<item.1>>>"]
    #   ipTemplate: "192.168.<<<item.0>>>.<<<item.1>>>"
    #
    #   # Virtualised nodes MAC-addresses are templated;
    #   # item.0 is the high-byte of the range, item.1 is the low-byte.
    #   # (Note: for MAC-addresses the low- and high-byte are converted to hex).
    #   # [optional; default: "52:54:00:42:<<<item.0>>>:<<<item.1>>>"]
    #   macTemplate: "52:54:00:42:<<<item.0>>>:<<<item.1>>>"
    #
    #   # One or several ranges of hostnames/IP-addresses/MAC-addresses for the VM instances.
    #   # cmt creates its own network called cmt, which by default reserves 192.168.124.0/22,
    #   # with 192.168.124.2-39 being dynamic DHCP-addresses. Thus the available ranges
    #   # here are:
    #   # - [124, [40, 254]]
    #   # - [125, [1, 254]]
    #   # - [126, [1, 254]]
    #   # - [127, [1, 254]]
    #   # In total this allows for 973 VMs.  If this isn't enough, and if CMT seems to scale
    #   # past that, feel free to file a feature request and we'll be happy to
    #   # make the maximum range larger.  The example below just creates 2 VM nodes.
    #   # [mandatory]
    #   # ranges:
    #   # - [124, [40, 42]]
    #
    #   # The Operating System image to use for the VMs in this group.
    #   # osImage: "{HOME}/iso/noble-server-cloudimg-amd64.img"
    #
    #   # The OS variant for this OS image. See the virt-install manual page for more info.
    #   # If you don't mind suboptimal performance you can use detect=on.
    #   # osVariant: "ubuntu24.04"
    #
    #   # Each VM will be expanded beyond the size of the OS image
    #   # to allow for updates, containers, etc.
    #   # [optional; default: 5G]
    #   # templateBalloonSize: "5G"
    #
    #   # How many vCPU cores should each VM be assigned?
    #   # [optional; default: 4]
    #   # vCPUs: 4
    #
    #   # How many MB of RAM should each VM be assigned?
    #   # [optional; default: 4096]
    #   # vmRAM: 4096
    #
    #   # The name of the template image used as the base when instantiating new VMs.
    #   # You only need to change this value if you have multiple different VM groups.
    #   # [optional; default: "template"]
    #   # vmTemplateImageName: "template"

    #   # Group specific options
    #   options:
    #     # CRI to use for the nodes in this group; valid options:
    #     # containerd
    #     # docker-shim [Kubernetes < 1.24]
    #     # cri-o
    #     # [optional; if not provided the cluster-wide default will be used]
    #     # cri: "containerd"

    #     # Hardcoded version of CRI;
    #     # useful (for instance) when behind a corporate firewall that ratelimits github API requests.
    #     # [optional; default: auto]
    #     # Currently only supported for cri-o.
    #     # cri_version: "1.30"


    #     # Ansible parallelism:
    #     # [optional; default: 5]
    #     # forks: 5

    #     # Cordon nodes on join:
    #     # [optional; default: false]
    #     # cordonOnJoin: false

    #     # Node configuration templates:
    #     # The configuration templates can either be embedded in the data
    #     # section, or provided via a path; Only one of data/path is valid for each type
    #     # of configuration; providing both is NOT supported.
    #     # The post-processed templates will be concatenated together before passing them
    #     # to kubeadm. Currently only JoinConfiguration is supported for worker nodes.
    #     # The templates will be processed by jinja; the following variables are available:
    #     # cri_socket
    #     # join_token
    #     # ca_cert_hash
    #     # control_plane_ip
    #     # control_plane_port
    #     # control_plane_path
    #     # pod_network_cidr
    #     configurationTemplates:
    #       rke2Configuration:
    #         # Embedded cluster configuration template:
    #         data: |
    #           # This will be auto-populated from the cluster settings
    #           server: "https://{{ control_plane_ip }}:9345"
    #           # This will be auto-populated from the cluster settings
    #           container-runtime-endpoint: "{{ cri_socket }}"
    #           # This will be auto-populated from the newly generated cluster
    #           token: "{{ node_token }}"
    #           kubelet-arg:
    #           # feature-gates is a comma-separated string
    #           # To Enable Dynamic Resource Allocations use the following:
    #           # Note: also requires alpha APIs and the matching feature gates
    #           # to be enabled for apiServer and scheduler in cluster_configuration,
    #           # as well as in kube_proxy_configuration, and kubelet_configuration
    #           # [optional; default: ""]
    #           # - feature-gates=DynamicResourceAllocation=true
    #           # The cgroup driver to use
    #           - cgroup-driver=systemd
    #           kube-proxy-arg:
    #           # feature-gates is a comma-separated string
    #           # To Enable Dynamic Resource Allocations use the following:
    #           # Note: also requires alpha APIs and the matching feature gates
    #           # to be enabled for apiServer and scheduler in cluster_configuration,
    #           # as well as in kube_proxy_configuration, and kubelet_configuration
    #           # [optional; default: ""]
    #           # - feature-gates=DynamicResourceAllocation=true

    #         # Path to node configuration template, relative to the playbook directory:
    #         # path: "templates/config/rke2_agent_configuration.yaml.j2"

    #       joinConfiguration:
    #         # Embedded node configuration template:
    #         data: |
    #           apiVersion: kubeadm.k8s.io/v1beta3
    #           kind: JoinConfiguration
    #           nodeRegistration:
    #             # This will be auto-populated from the node group settings; only change if you know what you're doing
    #             criSocket: "{{ cri_socket }}"
    #             # Use this to add labels to the nodes
    #             # kubeletExtraArgs:
    #             #   node-labels: "foo=bar"
    #             # A list of taints; an empty list means no taints
    #             # This will be auto-populated from the node group settings; add extra taints below if wanted
    #           {% if cordon_nodes_on_join %}
    #             taints:
    #             - key: node.kubernetes.io/unschedulable
    #               effect: NoSchedule
    #           {% else %}
    #             taints: []
    #           {% endif %}
    #           discovery:
    #             bootstrapToken:
    #               # This will be auto-populated; only change if you know what you're doing
    #               token: "{{ join_token }}"
    #               caCertHashes:
    #               # This will be auto-populated; only change if you know what you're doing
    #               - "sha256:{{ ca_cert_hash }}"
    #               # This will be auto-populated; only change if you know what you're doing
    #               apiServerEndpoint: "{{ control_plane_ip }}:{{ control_plane_port }}{{ control_plane_path }}"

    #         # Path to node configuration template, relative to the playbook directory:
    #         # path: "templates/config/join_configuration.yaml.j2"
workloads:
  # The name of the workload
  # [mandatory]
- name: "node-feature-discovery"
  # Workloads can be disabled by setting skip to true
  # [optional; default: false]
  skip: false
  # A more elaborate description of the workload;
  # only displayed when verbose is set
  # [optional; default: ""]
  description: |
    node-feature-discovery helps detect hardware features and system configuration.
  # Error policy:
  # ignore [ignore errors]
  # abort [abort installation on errors]
  # [optional; default: ignore]
  errorPolicy: "ignore"
  # The deployments that this workload consists of
  # [mandatory]
  deployments:
    # Each deployment can be deployed either using kubectl or helm;
    # repositories can be checked out using git.
    # Hence kubectl, helm, and git are valid targets
  - kubectl:
      # The deployment method; valid options:
      # apply [apply --server-side]
      # create [create]
      # [optional; default: apply]
      method: "apply"
      # The type of data; valid options:
      # file
      # kustomization
      # [mandatory]
      type: "kustomization"
      # When applying multiple URIs it can be convenient to centralise the version#;
      # if <<<version>>> is encountered in a URI it will be substituted for the value
      # of the version field.
      # [optional; default: ""]
      version: "v0.16.5"
      # The URI[s] to the data;
      # this field can be either a string (a single URI) or a list (a single or multiple URIs)
      # All URIs need to be prefixed with their type
      # Valid prefixes:
      # http:// [highly discouraged due to security reasons]
      # https://
      # file://
      # {HOME} is substituted for the path to the home directory of the user
      # [mandatory]
      uri: "https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/default?ref=<<<version>>>"
- name: "cert-manager"
  description: |
    cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters,
    and simplifies the process of obtaining, renewing and using those certificates.
  deployments:
  - kubectl:
      method: "apply"
      type: "file"
      version: "v1.16.1"
      uri: "https://github.com/cert-manager/cert-manager/releases/download/<<<version>>>/cert-manager.crds.yaml"
  - helm:
      # The name of the helm repo to add
      repoName: "jetstack"
      # The URL to the helm repo
      repoURL: "https://charts.jetstack.io"
      # The namespace to use
      namespace: "cert-manager"
      # The name of the component to install
      name: "cert-manager"
      # The name of the chart to install
      chart: "jetstack/cert-manager"
      # Create the namespace if it doesn't exist?
      createNamespace: true
      # The version to install
      version: "v1.16.1"
- name: "intel-resource-drivers-for-kubernetes"
  description: |
    Intel GPU resource driver is a better alternative for Intel GPU device plugin,
    facilitating workload offloading by providing GPU access on Kubernetes cluster worker nodes.
  deployments:
  - git:
      # The repository to clone (if not already available locally)
      # [optional; the program will exit if no local checkout exists]
      repo: "https://github.com/intel/intel-resource-drivers-for-kubernetes.git"
      # The directory to store the repository in
      # [mandatory]
      destination: "{HOME}/git/intel-resource-drivers-for-kubernetes"
      # Should git fetch be called each time the ClusterDeployment is applied
      # [optional; default: false]
      fetch: true
  - kubectl:
      method: "apply"
      type: "file"
      # Multiple applies can be defined at once as long as they are the same type
      uri:
      - "file://{HOME}/git/intel-resource-drivers-for-kubernetes/deployments/gpu/static/crds/"
      - "file://{HOME}/git/intel-resource-drivers-for-kubernetes/deployments/gpu/resource-class.yaml"
      - "file://{HOME}/git/intel-resource-drivers-for-kubernetes/deployments/gpu/resource-driver-namespace.yaml"
      - "file://{HOME}/git/intel-resource-drivers-for-kubernetes/deployments/gpu/resource-defaults.yaml"
      - "file://{HOME}/git/intel-resource-drivers-for-kubernetes/deployments/gpu/resource-driver.yaml"
